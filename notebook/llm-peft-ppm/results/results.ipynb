{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b979f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results\n",
      "project_root in sys.path: True\n",
      "ENTITY: privajet-university-of-mannheim\n",
      "WANDB_MODE: offline\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import wandb\n",
    "\n",
    "project_root = os.path.abspath(\"..\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from ppm.wandb_utils import fetch_experiments\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ.setdefault(\"ENTITY\", \"privajet-university-of-mannheim\")\n",
    "entity = os.environ[\"ENTITY\"]\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"project_root in sys.path:\", project_root in sys.path)\n",
    "print(\"ENTITY:\", entity)\n",
    "print(\"WANDB_MODE:\", os.environ.get(\"WANDB_MODE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d407aeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_csv = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv\"\n",
    "output_dir_plots = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots\"\n",
    "os.makedirs(output_dir_csv, exist_ok=True)\n",
    "os.makedirs(output_dir_plots, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6adf8acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all lines pandas\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "mpl.rcParams.update({\n",
    "    \"figure.figsize\": (6, 4),          \n",
    "    \"font.size\": 10,                   \n",
    "    \"axes.labelsize\": 10,              \n",
    "    \"axes.titlesize\": 10,              \n",
    "    \"legend.fontsize\": 9,              \n",
    "    \"xtick.labelsize\": 9,              \n",
    "    \"ytick.labelsize\": 9,\n",
    "    \"lines.linewidth\": 1.5,            \n",
    "    \"lines.markersize\": 5,             \n",
    "    \"axes.grid\": True,                 \n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"grid.linewidth\": 0.5,\n",
    "    \"legend.frameon\": False,           \n",
    "    \"pdf.fonttype\": 42,                \n",
    "    \"ps.fonttype\": 42,\n",
    "    \"savefig.bbox\": \"tight\",           \n",
    "    \"savefig.dpi\": 300,                \n",
    "})\n",
    "\n",
    "colors = [\n",
    "    \"#9467bd\",\n",
    "    \"#2ca02c\",\n",
    "    \"#bcbd22\",\n",
    "    \"#7f7f7f\",\n",
    "    \"#e377c2\",\n",
    "    \"#8c564b\",\n",
    "    \"#d62728\",\n",
    "    \"#17becf\",\n",
    "    \"#1f77b4\",\n",
    "    \"#ff7f0e\",\n",
    "]\n",
    "\n",
    "plt.rcParams[\"axes.prop_cycle\"] = plt.cycler(color=colors)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45623d55",
   "metadata": {
    "title": "Experimental setup"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Log</th>\n",
       "      <th># cases</th>\n",
       "      <th># evt.</th>\n",
       "      <th># act.</th>\n",
       "      <th>Trace length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BPI20PrepaidTravelCosts</td>\n",
       "      <td>2099</td>\n",
       "      <td>18246</td>\n",
       "      <td>29</td>\n",
       "      <td>8.6927±2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BPI20RequestForPayment</td>\n",
       "      <td>6886</td>\n",
       "      <td>36796</td>\n",
       "      <td>19</td>\n",
       "      <td>5.3436±1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BPI20TravelPermitData</td>\n",
       "      <td>7065</td>\n",
       "      <td>86581</td>\n",
       "      <td>51</td>\n",
       "      <td>12.2549±5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>13087</td>\n",
       "      <td>262200</td>\n",
       "      <td>24</td>\n",
       "      <td>20.0351±19.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BPI17</td>\n",
       "      <td>31509</td>\n",
       "      <td>1202267</td>\n",
       "      <td>26</td>\n",
       "      <td>38.1563±16.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Log  # cases   # evt.  # act.  Trace length\n",
       "0  BPI20PrepaidTravelCosts     2099    18246      29    8.6927±2.3\n",
       "1   BPI20RequestForPayment     6886    36796      19    5.3436±1.5\n",
       "2    BPI20TravelPermitData     7065    86581      51   12.2549±5.6\n",
       "3                    BPI12    13087   262200      24  20.0351±19.9\n",
       "4                    BPI17    31509  1202267      26  38.1563±16.7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties = pd.read_csv(\n",
    "    \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/log_properties.csv\"\n",
    ")\n",
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4440f2ca",
   "metadata": {
    "title": "Architecture illustration"
   },
   "outputs": [],
   "source": [
    "# from ppm.models import NextEventPredictor\n",
    "# import torch \n",
    "\n",
    "# use_cuda = torch.cuda.is_available()\n",
    "# device = \"cuda\" if use_cuda else \"cpu\"\n",
    "# print(\"Using device:\", device)\n",
    "\n",
    "# rnn_example = NextEventPredictor(\n",
    "#     embedding_size=32,\n",
    "#     categorical_cols=[\"activity\"],\n",
    "#     numerical_cols=[\"accumulated_time\"],\n",
    "#     categorical_sizes={\"activity\": 20},\n",
    "#     categorical_targets=[\"activity\"],\n",
    "#     numerical_targets=[\"remaining_time\"],\n",
    "#     backbone_name=\"rnn\",\n",
    "#     backbone_hidden_size=64,\n",
    "#     backbone_n_layers=2,\n",
    "#     padding_idx=0,\n",
    "#     strategy=\"sum\",\n",
    "#     backbone_pretrained=False,\n",
    "#     backbone_finetuning=None,\n",
    "#     backbone_type=\"lstm\",\n",
    "#     device=device,\n",
    "# )\n",
    "# pprint(rnn_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "689e01c4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _to_int_or_none(x):\n",
    "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "        return None\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def map_setting(row):\n",
    "    ft = row.get(\"fine_tuning\")\n",
    "    k_raw = row.get(\"few_shot_k\", None)\n",
    "    fl    = row.get(\"freeze_layers\", None)\n",
    "    ep_raw = row.get(\"epochs\", None)\n",
    "\n",
    "    k  = _to_int_or_none(k_raw)\n",
    "    ep = _to_int_or_none(ep_raw)\n",
    "\n",
    "    # LoRA Few-Shot\n",
    "    if ft == \"lora\" and k == 8:\n",
    "        return \"FewShot-LoRA\"\n",
    "\n",
    "    # LoRA Full\n",
    "    if ft == \"lora\" and k is None:\n",
    "        return \"LoRA\"\n",
    "\n",
    "    # Zero-Shot (epochs = 0)\n",
    "    if ft == \"freeze\" and ep == 0:\n",
    "        return \"ZeroShot\"\n",
    "\n",
    "    # Freezing Few-Shot\n",
    "    if ft == \"freeze\" and k == 8:\n",
    "        return \"FewShot-Freezing\"\n",
    "\n",
    "    # Freezing standard (keine freeze_layers angegeben)\n",
    "    if ft == \"freeze\" and fl in (None, \"\", [], ()):\n",
    "        return \"Freezing\"\n",
    "\n",
    "    # Freezing layer configs (z.B. -1, -2 / 0, 1)\n",
    "    if ft == \"freeze\" and fl is not None:\n",
    "        if isinstance(fl, (list, tuple)):\n",
    "            fl_clean = [_to_int_or_none(x) for x in fl]\n",
    "        else:\n",
    "            tokens = str(fl).replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \" \").split()\n",
    "            fl_clean = [_to_int_or_none(x) for x in tokens]\n",
    "        fl_clean = [x for x in fl_clean if x is not None]\n",
    "        return f\"Freezing-{fl_clean}\"\n",
    "\n",
    "    return \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95ea8e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_path = os.path.join(output_dir_csv, \"global_results.pkl\")\n",
    "\n",
    "BACKBONE_PROJECTS = {\n",
    "    \"majority\":         \"llm-peft-ppm_majority_baseline\",\n",
    "    \"rnn\":              \"llm-peft-ppm_rnn\",\n",
    "    \"transformer\":      \"llm-peft-ppm_transformer_baseline\",\n",
    "    \"tabpfn\":           \"llm-peft-ppm_tabpfn_baseline\",\n",
    "    \"saprpt\":           \"llm-peft-ppm_saprpt_baseline\",\n",
    "    \"gpt2\":             \"llm-peft-ppm_gpt2\",\n",
    "    \"gptneo-1b3\":       \"llm-peft-ppm_gpt-neo-1.3B\",\n",
    "    \"qwen25-05b\":       \"llm-peft-ppm_qwen25-05b\",\n",
    "    \"llama32-1b\":       \"llm-peft-ppm_llama32-1b\",\n",
    "    \"gemma-2-2b\":       \"llm-peft-ppm_gemma-2-2b\",\n",
    "}\n",
    "\n",
    "def build_global_results():\n",
    "    all_results = []\n",
    "    for backbone, project_name in BACKBONE_PROJECTS.items():\n",
    "        df_tmp = fetch_experiments(project=project_name, entity=entity, include_metrics=True)\n",
    "        df_tmp[\"backbone\"] = backbone\n",
    "        df_tmp[\"project\"] = project_name\n",
    "        all_results.append(df_tmp)\n",
    "\n",
    "    gr = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "    safe_cols = [\n",
    "        \"id\", \"log\", \"backbone\", \"project\", \"fine_tuning\",\n",
    "        \"total_params\", \"trainable_params\", \"seed\", \"_runtime\", \"_timestamp\",\n",
    "        \"categorical_features\", \"categorical_targets\",\n",
    "        \"continuous_features\", \"continuous_targets\", \"device\", \"model\", \"name\",\n",
    "\n",
    "        \"test_next_activity_acc\",\n",
    "        \"test_next_activity_loss\",\n",
    "        \"test_next_remaining_time_loss\",\n",
    "        \"test_next_time_to_next_event_loss\",\n",
    "        \"best_test_next_activity_acc\",\n",
    "        \"best_test_next_activity_loss\",\n",
    "        \"best_test_next_remaining_time_loss\",\n",
    "        \"best_test_next_time_to_next_event_loss\",\n",
    "\n",
    "        \"batch_size\",\n",
    "        \"embedding_size\",\n",
    "        \"epochs\",\n",
    "        \"freeze_layers\",\n",
    "        \"grad_clip\",\n",
    "        \"hidden_size\",\n",
    "        \"lr\",\n",
    "        \"n_layers\",\n",
    "        \"rnn_type\",\n",
    "        \"strategy\",\n",
    "        \"weight_decay\",\n",
    "        \"lora_alpha\",\n",
    "        \"r\",\n",
    "        \"few_shot_k\",\n",
    "    ]\n",
    "\n",
    "    safe_cols = [c for c in safe_cols if c in gr.columns]\n",
    "    gr = gr[safe_cols]\n",
    "    return gr\n",
    "\n",
    "\n",
    "if os.path.exists(pkl_path):\n",
    "    try:\n",
    "        global_results = pd.read_pickle(pkl_path)\n",
    "    except Exception as e:\n",
    "        print(\"Fehler beim Laden von global_results.pkl, baue neu:\", repr(e))\n",
    "        global_results = build_global_results()\n",
    "        global_results.to_pickle(pkl_path)\n",
    "else:\n",
    "    global_results = build_global_results()\n",
    "    global_results.to_pickle(pkl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "723b0b89",
   "metadata": {
    "title": "Checking best models"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>log</th>\n",
       "      <th>backbone</th>\n",
       "      <th>project</th>\n",
       "      <th>fine_tuning</th>\n",
       "      <th>total_params</th>\n",
       "      <th>trainable_params</th>\n",
       "      <th>seed</th>\n",
       "      <th>_runtime</th>\n",
       "      <th>_timestamp</th>\n",
       "      <th>...</th>\n",
       "      <th>rnn_type</th>\n",
       "      <th>strategy</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>lora_alpha</th>\n",
       "      <th>r</th>\n",
       "      <th>few_shot_k</th>\n",
       "      <th>na_norm</th>\n",
       "      <th>rt_norm</th>\n",
       "      <th>nt_norm</th>\n",
       "      <th>mt_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pox3cg0n</td>\n",
       "      <td>BPI20PrepaidTravelCosts</td>\n",
       "      <td>majority</td>\n",
       "      <td>llm-peft-ppm_majority_baseline</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.127169</td>\n",
       "      <td>1.762725e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>lstm</td>\n",
       "      <td>concat</td>\n",
       "      <td>0.10</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.129842</td>\n",
       "      <td>0.944634</td>\n",
       "      <td>0.940268</td>\n",
       "      <td>2.014744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>whfyo8uu</td>\n",
       "      <td>BPI12</td>\n",
       "      <td>majority</td>\n",
       "      <td>llm-peft-ppm_majority_baseline</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.600980</td>\n",
       "      <td>1.762849e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>lstm</td>\n",
       "      <td>concat</td>\n",
       "      <td>0.10</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.252059</td>\n",
       "      <td>0.955857</td>\n",
       "      <td>0.943034</td>\n",
       "      <td>2.150950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oy378knj</td>\n",
       "      <td>BPI20TravelPermitData</td>\n",
       "      <td>majority</td>\n",
       "      <td>llm-peft-ppm_majority_baseline</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.808587</td>\n",
       "      <td>1.762850e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>lstm</td>\n",
       "      <td>concat</td>\n",
       "      <td>0.10</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.087414</td>\n",
       "      <td>0.951915</td>\n",
       "      <td>0.961259</td>\n",
       "      <td>2.000588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3gchqw9a</td>\n",
       "      <td>BPI20RequestForPayment</td>\n",
       "      <td>majority</td>\n",
       "      <td>llm-peft-ppm_majority_baseline</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.075565</td>\n",
       "      <td>1.762850e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>lstm</td>\n",
       "      <td>concat</td>\n",
       "      <td>0.10</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.188470</td>\n",
       "      <td>0.968881</td>\n",
       "      <td>0.975401</td>\n",
       "      <td>2.132752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cl197f3r</td>\n",
       "      <td>BPI17</td>\n",
       "      <td>majority</td>\n",
       "      <td>llm-peft-ppm_majority_baseline</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>107.474805</td>\n",
       "      <td>1.762850e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>lstm</td>\n",
       "      <td>concat</td>\n",
       "      <td>0.10</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.169815</td>\n",
       "      <td>0.972109</td>\n",
       "      <td>0.963413</td>\n",
       "      <td>2.105338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vl6y1axq</td>\n",
       "      <td>BPI12</td>\n",
       "      <td>rnn</td>\n",
       "      <td>llm-peft-ppm_rnn</td>\n",
       "      <td>None</td>\n",
       "      <td>88733.0</td>\n",
       "      <td>88733.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>64.434233</td>\n",
       "      <td>1.764079e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>lstm</td>\n",
       "      <td>sum</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.896924</td>\n",
       "      <td>0.890896</td>\n",
       "      <td>0.959974</td>\n",
       "      <td>2.747794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wwv4s6ta</td>\n",
       "      <td>BPI12</td>\n",
       "      <td>rnn</td>\n",
       "      <td>llm-peft-ppm_rnn</td>\n",
       "      <td>None</td>\n",
       "      <td>88733.0</td>\n",
       "      <td>88733.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>65.573150</td>\n",
       "      <td>1.764080e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>lstm</td>\n",
       "      <td>sum</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.865574</td>\n",
       "      <td>0.885115</td>\n",
       "      <td>0.957324</td>\n",
       "      <td>2.708013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xjcuy92g</td>\n",
       "      <td>BPI12</td>\n",
       "      <td>rnn</td>\n",
       "      <td>llm-peft-ppm_rnn</td>\n",
       "      <td>None</td>\n",
       "      <td>88733.0</td>\n",
       "      <td>88733.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>66.748843</td>\n",
       "      <td>1.764080e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>lstm</td>\n",
       "      <td>sum</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.882514</td>\n",
       "      <td>0.882064</td>\n",
       "      <td>0.956911</td>\n",
       "      <td>2.721489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>r5ax60vx</td>\n",
       "      <td>BPI12</td>\n",
       "      <td>rnn</td>\n",
       "      <td>llm-peft-ppm_rnn</td>\n",
       "      <td>None</td>\n",
       "      <td>88733.0</td>\n",
       "      <td>88733.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>60.193704</td>\n",
       "      <td>1.764080e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>lstm</td>\n",
       "      <td>sum</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.875060</td>\n",
       "      <td>0.886308</td>\n",
       "      <td>0.959447</td>\n",
       "      <td>2.720816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>s5xrzrsn</td>\n",
       "      <td>BPI12</td>\n",
       "      <td>rnn</td>\n",
       "      <td>llm-peft-ppm_rnn</td>\n",
       "      <td>None</td>\n",
       "      <td>88733.0</td>\n",
       "      <td>88733.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>68.910497</td>\n",
       "      <td>1.764080e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>lstm</td>\n",
       "      <td>sum</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.902644</td>\n",
       "      <td>0.889049</td>\n",
       "      <td>0.956773</td>\n",
       "      <td>2.748466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                      log  backbone  \\\n",
       "0  pox3cg0n  BPI20PrepaidTravelCosts  majority   \n",
       "1  whfyo8uu                    BPI12  majority   \n",
       "2  oy378knj    BPI20TravelPermitData  majority   \n",
       "3  3gchqw9a   BPI20RequestForPayment  majority   \n",
       "4  cl197f3r                    BPI17  majority   \n",
       "5  vl6y1axq                    BPI12       rnn   \n",
       "6  wwv4s6ta                    BPI12       rnn   \n",
       "7  xjcuy92g                    BPI12       rnn   \n",
       "8  r5ax60vx                    BPI12       rnn   \n",
       "9  s5xrzrsn                    BPI12       rnn   \n",
       "\n",
       "                          project fine_tuning  total_params  trainable_params  \\\n",
       "0  llm-peft-ppm_majority_baseline        None           1.0               1.0   \n",
       "1  llm-peft-ppm_majority_baseline        None           1.0               1.0   \n",
       "2  llm-peft-ppm_majority_baseline        None           1.0               1.0   \n",
       "3  llm-peft-ppm_majority_baseline        None           1.0               1.0   \n",
       "4  llm-peft-ppm_majority_baseline        None           1.0               1.0   \n",
       "5                llm-peft-ppm_rnn        None       88733.0           88733.0   \n",
       "6                llm-peft-ppm_rnn        None       88733.0           88733.0   \n",
       "7                llm-peft-ppm_rnn        None       88733.0           88733.0   \n",
       "8                llm-peft-ppm_rnn        None       88733.0           88733.0   \n",
       "9                llm-peft-ppm_rnn        None       88733.0           88733.0   \n",
       "\n",
       "   seed    _runtime    _timestamp  ... rnn_type strategy weight_decay  \\\n",
       "0   NaN    1.127169  1.762725e+09  ...     lstm   concat         0.10   \n",
       "1   NaN   35.600980  1.762849e+09  ...     lstm   concat         0.10   \n",
       "2   NaN   23.808587  1.762850e+09  ...     lstm   concat         0.10   \n",
       "3   NaN   21.075565  1.762850e+09  ...     lstm   concat         0.10   \n",
       "4   NaN  107.474805  1.762850e+09  ...     lstm   concat         0.10   \n",
       "5  41.0   64.434233  1.764079e+09  ...     lstm      sum         0.01   \n",
       "6  42.0   65.573150  1.764080e+09  ...     lstm      sum         0.01   \n",
       "7  43.0   66.748843  1.764080e+09  ...     lstm      sum         0.01   \n",
       "8  44.0   60.193704  1.764080e+09  ...     lstm      sum         0.01   \n",
       "9  45.0   68.910497  1.764080e+09  ...     lstm      sum         0.01   \n",
       "\n",
       "  lora_alpha     r few_shot_k   na_norm   rt_norm   nt_norm  mt_score  \n",
       "0       None  None        NaN  0.129842  0.944634  0.940268  2.014744  \n",
       "1       None  None        NaN  0.252059  0.955857  0.943034  2.150950  \n",
       "2       None  None        NaN  0.087414  0.951915  0.961259  2.000588  \n",
       "3       None  None        NaN  0.188470  0.968881  0.975401  2.132752  \n",
       "4       None  None        NaN  0.169815  0.972109  0.963413  2.105338  \n",
       "5       None  None        NaN  0.896924  0.890896  0.959974  2.747794  \n",
       "6       None  None        NaN  0.865574  0.885115  0.957324  2.708013  \n",
       "7       None  None        NaN  0.882514  0.882064  0.956911  2.721489  \n",
       "8       None  None        NaN  0.875060  0.886308  0.959447  2.720816  \n",
       "9       None  None        NaN  0.902644  0.889049  0.956773  2.748466  \n",
       "\n",
       "[10 rows x 43 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\n",
    "    \"id\",\n",
    "    \"log\",\n",
    "    \"backbone\",\n",
    "    \"project\",\n",
    "    \"fine_tuning\",\n",
    "    \"total_params\",\n",
    "    \"trainable_params\",\n",
    "    \"test_next_activity_acc\",\n",
    "    \"test_next_activity_loss\",\n",
    "    \"test_next_remaining_time_loss\",\n",
    "    \"test_next_time_to_next_event_loss\",\n",
    "    \"best_test_next_activity_acc\",\n",
    "    \"best_test_next_activity_loss\",\n",
    "    \"best_test_next_remaining_time_loss\",\n",
    "    \"best_test_next_time_to_next_event_loss\",\n",
    "    \"_runtime\",\n",
    "    \"mt_score\",\n",
    "]\n",
    "\n",
    "df = global_results.copy()\n",
    "df = df[\n",
    "    df[\"test_next_activity_acc\"].notna()\n",
    "    & df[\"test_next_remaining_time_loss\"].notna()\n",
    "    & df[\"test_next_time_to_next_event_loss\"].notna()\n",
    "].copy()\n",
    "\n",
    "sc_acc = MinMaxScaler()\n",
    "sc_rt  = MinMaxScaler()\n",
    "sc_nt  = MinMaxScaler()\n",
    "\n",
    "df[\"na_norm\"] = sc_acc.fit_transform(df[[\"test_next_activity_acc\"]])\n",
    "df[\"rt_norm\"] = sc_rt.fit_transform(-df[[\"test_next_remaining_time_loss\"]])\n",
    "df[\"nt_norm\"] = sc_nt.fit_transform(-df[[\"test_next_time_to_next_event_loss\"]])\n",
    "df[\"mt_score\"] = df[\"na_norm\"] + df[\"rt_norm\"] + df[\"nt_norm\"]\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1682d10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "    \"test_next_activity_acc\",\n",
    "    \"test_next_activity_loss\",\n",
    "    \"test_next_remaining_time_loss\",\n",
    "    \"test_next_time_to_next_event_loss\",\n",
    "    \"best_test_next_activity_acc\",\n",
    "    \"best_test_next_activity_loss\",\n",
    "    \"best_test_next_remaining_time_loss\",\n",
    "    \"best_test_next_time_to_next_event_loss\",\n",
    "]\n",
    "\n",
    "def agg_over_seeds(group: pd.DataFrame) -> pd.Series:\n",
    "    out = {\"n_runs\": len(group)}\n",
    "    for c in [\"total_params\", \"trainable_params\"]:\n",
    "        if c in group.columns:\n",
    "            out[c] = group[c].iloc[0]\n",
    "    if \"mt_score\" in group.columns:\n",
    "        out[\"mt_score_mean\"] = group[\"mt_score\"].mean()\n",
    "        out[\"mt_score_std\"] = group[\"mt_score\"].std()\n",
    "    if \"_runtime\" in group.columns:\n",
    "        out[\"_runtime_mean\"] = group[\"_runtime\"].mean()\n",
    "        out[\"_runtime_std\"]  = group[\"_runtime\"].std()\n",
    "    for m in METRICS:\n",
    "        if m in group.columns:\n",
    "            vals = group[m].dropna()\n",
    "            out[m + \"_mean\"] = vals.mean()\n",
    "            out[m + \"_std\"] = vals.std()\n",
    "    return pd.Series(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "889f1a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter columns: ['batch_size', 'embedding_size', 'epochs', 'freeze_layers', 'grad_clip', 'hidden_size', 'lr', 'n_layers', 'rnn_type', 'strategy', 'weight_decay']\n"
     ]
    }
   ],
   "source": [
    "majority = df[df[\"backbone\"] == \"majority\"].copy()\n",
    "majority_grouped = (\n",
    "    majority\n",
    "    .groupby([\"log\", \"backbone\"], dropna=False)\n",
    "    .apply(agg_over_seeds)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "BASELINE_BACKBONES = [\"rnn\", \"transformer\", \"tabpfn\", \"saprpt\"]\n",
    "baseline = df[df[\"backbone\"].isin(BASELINE_BACKBONES)].copy()\n",
    "\n",
    "NON_HP_COLS = set(\n",
    "    [\n",
    "        \"id\",\"log\",\"backbone\",\"categorical_features\",\"categorical_targets\",\n",
    "        \"continuous_features\",\"continuous_targets\",\"device\",\"project\",\"model\",\n",
    "        \"name\",\"fine_tuning\",\"lora_alpha\", \"r\", \"few_shot_k\", \"seed\",\"_runtime\",\"_timestamp\",\n",
    "        \"na_norm\",\"rt_norm\",\"nt_norm\",\"mt_score\",\"majority_stat\",\n",
    "        \"total_params\",\"trainable_params\",\"best_train_next_remaining_time_loss\",\n",
    "        \"_step\",\"best_train_next_activity_loss\",\"train_next_time_to_next_event_loss\",\n",
    "        \"best_train_next_time_to_next_event_loss\",\"train_next_activity_acc\",\n",
    "        \"train_next_activity_loss\",\"_wandb.runtime\",\"best_train_next_activity_acc\",\n",
    "        \"train_next_remaining_time_loss\",\"persist_model\",\"project_name\",\"wandb\",\n",
    "    ]\n",
    "    + METRICS\n",
    ")\n",
    "\n",
    "HP_COLS = [c for c in baseline.columns if c not in NON_HP_COLS]\n",
    "print(\"Hyperparameter columns:\", HP_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8c1e4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved baseline summary to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/baseline_best_settings_mean_std.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log</th>\n",
       "      <th>backbone</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>embedding_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>freeze_layers</th>\n",
       "      <th>grad_clip</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>...</th>\n",
       "      <th>Backbone_pretty</th>\n",
       "      <th>test_next_activity_acc_mean_std</th>\n",
       "      <th>test_next_activity_loss_mean_std</th>\n",
       "      <th>test_next_remaining_time_loss_mean_std</th>\n",
       "      <th>test_next_time_to_next_event_loss_mean_std</th>\n",
       "      <th>best_test_next_activity_acc_mean_std</th>\n",
       "      <th>best_test_next_activity_loss_mean_std</th>\n",
       "      <th>best_test_next_remaining_time_loss_mean_std</th>\n",
       "      <th>best_test_next_time_to_next_event_loss_mean_std</th>\n",
       "      <th>Runtime (h)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>rnn</td>\n",
       "      <td>32.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>RNN</td>\n",
       "      <td>0.7757 ± 0.0142</td>\n",
       "      <td>0.7689 ± 0.042</td>\n",
       "      <td>1.7738 ± 0.0889</td>\n",
       "      <td>1.3352 ± 0.0302</td>\n",
       "      <td>0.7799 ± 0.0095</td>\n",
       "      <td>0.758 ± 0.0285</td>\n",
       "      <td>1.7033 ± 0.1138</td>\n",
       "      <td>1.3205 ± 0.014</td>\n",
       "      <td>0.01943 ± 0.00025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>saprpt</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>SAP-RPT</td>\n",
       "      <td>0.6294 ± 0.0121</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>2.1601 ± 0.1146</td>\n",
       "      <td>1.6706 ± 0.372</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>0.00500 ± 0.00093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>tabpfn</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>TabPFN</td>\n",
       "      <td>0.6364 ± 0.0153</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>2.2338 ± 0.0937</td>\n",
       "      <td>1.6877 ± 0.4063</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>0.12074 ± 0.00085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>transformer</td>\n",
       "      <td>32.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>0.7562 ± 0.0246</td>\n",
       "      <td>0.7139 ± 0.1077</td>\n",
       "      <td>2.8388 ± 0.1209</td>\n",
       "      <td>1.3671 ± 0.0495</td>\n",
       "      <td>0.7687 ± 0.016</td>\n",
       "      <td>0.6635 ± 0.0383</td>\n",
       "      <td>2.6204 ± 0.1508</td>\n",
       "      <td>1.3364 ± 0.016</td>\n",
       "      <td>0.00758 ± 0.00008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BPI17</td>\n",
       "      <td>rnn</td>\n",
       "      <td>256.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>RNN</td>\n",
       "      <td>0.8535 ± 0.0013</td>\n",
       "      <td>0.4193 ± 0.0068</td>\n",
       "      <td>0.6725 ± 0.0325</td>\n",
       "      <td>0.7629 ± 0.0159</td>\n",
       "      <td>0.8535 ± 0.0013</td>\n",
       "      <td>0.4193 ± 0.0068</td>\n",
       "      <td>0.6097 ± 0.0138</td>\n",
       "      <td>0.7629 ± 0.0159</td>\n",
       "      <td>0.04166 ± 0.00022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BPI17</td>\n",
       "      <td>saprpt</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>SAP-RPT</td>\n",
       "      <td>0.6472 ± 0.0268</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>1.3056 ± 0.2773</td>\n",
       "      <td>1.1287 ± 0.1969</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>0.00440 ± 0.00005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BPI17</td>\n",
       "      <td>tabpfn</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>TabPFN</td>\n",
       "      <td>0.669 ± 0.0194</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>1.6011 ± 0.0589</td>\n",
       "      <td>1.1679 ± 0.2366</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>0.12188 ± 0.00312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BPI17</td>\n",
       "      <td>transformer</td>\n",
       "      <td>32.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>0.8556 ± 0.0058</td>\n",
       "      <td>0.4141 ± 0.0136</td>\n",
       "      <td>1.156 ± 0.0803</td>\n",
       "      <td>0.8836 ± 0.0296</td>\n",
       "      <td>0.8596 ± 0.0024</td>\n",
       "      <td>0.407 ± 0.0142</td>\n",
       "      <td>1.1343 ± 0.0847</td>\n",
       "      <td>0.8433 ± 0.0401</td>\n",
       "      <td>0.01953 ± 0.00030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BPI20PrepaidTravelCosts</td>\n",
       "      <td>rnn</td>\n",
       "      <td>32.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>RNN</td>\n",
       "      <td>0.7841 ± 0.0201</td>\n",
       "      <td>0.6578 ± 0.0553</td>\n",
       "      <td>1.0744 ± 0.0308</td>\n",
       "      <td>1.1906 ± 0.0485</td>\n",
       "      <td>0.7897 ± 0.0134</td>\n",
       "      <td>0.6549 ± 0.0492</td>\n",
       "      <td>0.9536 ± 0.027</td>\n",
       "      <td>1.1239 ± 0.0216</td>\n",
       "      <td>0.00316 ± 0.00005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BPI20PrepaidTravelCosts</td>\n",
       "      <td>saprpt</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>SAP-RPT</td>\n",
       "      <td>0.7646 ± 0.0054</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>1.041 ± 0.0605</td>\n",
       "      <td>1.0863 ± 0.2173</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>0.00448 ± 0.00007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BPI20PrepaidTravelCosts</td>\n",
       "      <td>tabpfn</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>TabPFN</td>\n",
       "      <td>0.6637 ± 0.0121</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>1.1278 ± 0.0716</td>\n",
       "      <td>1.1919 ± 0.2369</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>0.01156 ± 0.00005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BPI20PrepaidTravelCosts</td>\n",
       "      <td>transformer</td>\n",
       "      <td>32.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>0.7821 ± 0.0061</td>\n",
       "      <td>0.6683 ± 0.0118</td>\n",
       "      <td>1.1187 ± 0.0865</td>\n",
       "      <td>1.237 ± 0.0537</td>\n",
       "      <td>0.7994 ± 0.0105</td>\n",
       "      <td>0.6527 ± 0.0174</td>\n",
       "      <td>0.9541 ± 0.0334</td>\n",
       "      <td>1.0688 ± 0.0115</td>\n",
       "      <td>0.00162 ± 0.00003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BPI20RequestForPayment</td>\n",
       "      <td>rnn</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>RNN</td>\n",
       "      <td>0.8777 ± 0.0234</td>\n",
       "      <td>0.3678 ± 0.0387</td>\n",
       "      <td>0.6826 ± 0.0213</td>\n",
       "      <td>0.7504 ± 0.0088</td>\n",
       "      <td>0.886 ± 0.0114</td>\n",
       "      <td>0.3647 ± 0.0376</td>\n",
       "      <td>0.6795 ± 0.0219</td>\n",
       "      <td>0.7444 ± 0.0072</td>\n",
       "      <td>0.00952 ± 0.00008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BPI20RequestForPayment</td>\n",
       "      <td>saprpt</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>SAP-RPT</td>\n",
       "      <td>0.7434 ± 0.0096</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>0.6383 ± 0.0255</td>\n",
       "      <td>0.7122 ± 0.0769</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>0.00452 ± 0.00017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BPI20RequestForPayment</td>\n",
       "      <td>tabpfn</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>TabPFN</td>\n",
       "      <td>0.7439 ± 0.0088</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>0.7148 ± 0.034</td>\n",
       "      <td>0.7176 ± 0.0719</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>0.03380 ± 0.00020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BPI20RequestForPayment</td>\n",
       "      <td>transformer</td>\n",
       "      <td>32.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>0.8937 ± 0.0013</td>\n",
       "      <td>0.2719 ± 0.0012</td>\n",
       "      <td>0.6571 ± 0.009</td>\n",
       "      <td>0.7407 ± 0.0091</td>\n",
       "      <td>0.8954 ± 0.001</td>\n",
       "      <td>0.2639 ± 0.0012</td>\n",
       "      <td>0.6438 ± 0.0042</td>\n",
       "      <td>0.7187 ± 0.0037</td>\n",
       "      <td>0.00471 ± 0.00003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BPI20TravelPermitData</td>\n",
       "      <td>rnn</td>\n",
       "      <td>32.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>RNN</td>\n",
       "      <td>0.7302 ± 0.0031</td>\n",
       "      <td>0.9502 ± 0.014</td>\n",
       "      <td>1.0098 ± 0.0383</td>\n",
       "      <td>1.052 ± 0.0308</td>\n",
       "      <td>0.7314 ± 0.0026</td>\n",
       "      <td>0.9381 ± 0.0158</td>\n",
       "      <td>0.7632 ± 0.0399</td>\n",
       "      <td>0.8585 ± 0.0104</td>\n",
       "      <td>0.01171 ± 0.00011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BPI20TravelPermitData</td>\n",
       "      <td>saprpt</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>SAP-RPT</td>\n",
       "      <td>0.6869 ± 0.013</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>0.9545 ± 0.0682</td>\n",
       "      <td>1.0428 ± 0.0515</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>0.00449 ± 0.00005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BPI20TravelPermitData</td>\n",
       "      <td>tabpfn</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>TabPFN</td>\n",
       "      <td>0.6229 ± 0.012</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>1.091 ± 0.0495</td>\n",
       "      <td>1.1187 ± 0.0619</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>0.12027 ± 0.00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BPI20TravelPermitData</td>\n",
       "      <td>transformer</td>\n",
       "      <td>16.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>0.7183 ± 0.0019</td>\n",
       "      <td>0.8407 ± 0.0143</td>\n",
       "      <td>0.8088 ± 0.0386</td>\n",
       "      <td>0.8717 ± 0.0301</td>\n",
       "      <td>0.7191 ± 0.0022</td>\n",
       "      <td>0.8369 ± 0.0147</td>\n",
       "      <td>0.7829 ± 0.0103</td>\n",
       "      <td>0.8473 ± 0.0178</td>\n",
       "      <td>0.00808 ± 0.00005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>majority</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Majority</td>\n",
       "      <td>0.226 ± nan</td>\n",
       "      <td>6.2004 ± nan</td>\n",
       "      <td>1.3884 ± nan</td>\n",
       "      <td>1.5434 ± nan</td>\n",
       "      <td>0.226 ± nan</td>\n",
       "      <td>6.2004 ± nan</td>\n",
       "      <td>1.3884 ± nan</td>\n",
       "      <td>1.5434 ± nan</td>\n",
       "      <td>0.00989 ± nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>BPI17</td>\n",
       "      <td>majority</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Majority</td>\n",
       "      <td>0.1523 ± nan</td>\n",
       "      <td>6.7908 ± nan</td>\n",
       "      <td>1.0963 ± nan</td>\n",
       "      <td>1.2125 ± nan</td>\n",
       "      <td>0.1523 ± nan</td>\n",
       "      <td>6.7908 ± nan</td>\n",
       "      <td>1.0963 ± nan</td>\n",
       "      <td>1.2125 ± nan</td>\n",
       "      <td>0.02985 ± nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>BPI20PrepaidTravelCosts</td>\n",
       "      <td>majority</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Majority</td>\n",
       "      <td>0.1164 ± nan</td>\n",
       "      <td>7.0789 ± nan</td>\n",
       "      <td>1.5902 ± nan</td>\n",
       "      <td>1.5883 ± nan</td>\n",
       "      <td>0.1164 ± nan</td>\n",
       "      <td>7.0789 ± nan</td>\n",
       "      <td>1.5902 ± nan</td>\n",
       "      <td>1.5883 ± nan</td>\n",
       "      <td>0.00031 ± nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>BPI20RequestForPayment</td>\n",
       "      <td>majority</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Majority</td>\n",
       "      <td>0.169 ± nan</td>\n",
       "      <td>6.6539 ± nan</td>\n",
       "      <td>1.1543 ± nan</td>\n",
       "      <td>1.0179 ± nan</td>\n",
       "      <td>0.169 ± nan</td>\n",
       "      <td>6.6539 ± nan</td>\n",
       "      <td>1.1543 ± nan</td>\n",
       "      <td>1.0179 ± nan</td>\n",
       "      <td>0.00585 ± nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>BPI20TravelPermitData</td>\n",
       "      <td>majority</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Majority</td>\n",
       "      <td>0.0784 ± nan</td>\n",
       "      <td>7.3902 ± nan</td>\n",
       "      <td>1.4593 ± nan</td>\n",
       "      <td>1.2475 ± nan</td>\n",
       "      <td>0.0784 ± nan</td>\n",
       "      <td>7.3902 ± nan</td>\n",
       "      <td>1.4593 ± nan</td>\n",
       "      <td>1.2475 ± nan</td>\n",
       "      <td>0.00661 ± nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        log     backbone  batch_size  embedding_size  epochs  \\\n",
       "0                     BPI12          rnn        32.0           128.0    25.0   \n",
       "1                     BPI12       saprpt        16.0            16.0    25.0   \n",
       "2                     BPI12       tabpfn        16.0            16.0    25.0   \n",
       "3                     BPI12  transformer        32.0           128.0    10.0   \n",
       "4                     BPI17          rnn       256.0            32.0    25.0   \n",
       "5                     BPI17       saprpt        16.0            16.0    25.0   \n",
       "6                     BPI17       tabpfn        16.0            16.0    25.0   \n",
       "7                     BPI17  transformer        32.0           128.0    10.0   \n",
       "8   BPI20PrepaidTravelCosts          rnn        32.0           128.0    25.0   \n",
       "9   BPI20PrepaidTravelCosts       saprpt        16.0            16.0    25.0   \n",
       "10  BPI20PrepaidTravelCosts       tabpfn        16.0            16.0    25.0   \n",
       "11  BPI20PrepaidTravelCosts  transformer        32.0           128.0    10.0   \n",
       "12   BPI20RequestForPayment          rnn        32.0            32.0    25.0   \n",
       "13   BPI20RequestForPayment       saprpt        16.0            16.0    25.0   \n",
       "14   BPI20RequestForPayment       tabpfn        16.0            16.0    25.0   \n",
       "15   BPI20RequestForPayment  transformer        32.0           128.0    10.0   \n",
       "16    BPI20TravelPermitData          rnn        32.0           256.0    25.0   \n",
       "17    BPI20TravelPermitData       saprpt        16.0            16.0    25.0   \n",
       "18    BPI20TravelPermitData       tabpfn        16.0            16.0    25.0   \n",
       "19    BPI20TravelPermitData  transformer        16.0           256.0    10.0   \n",
       "20                    BPI12     majority         NaN             NaN     NaN   \n",
       "21                    BPI17     majority         NaN             NaN     NaN   \n",
       "22  BPI20PrepaidTravelCosts     majority         NaN             NaN     NaN   \n",
       "23   BPI20RequestForPayment     majority         NaN             NaN     NaN   \n",
       "24    BPI20TravelPermitData     majority         NaN             NaN     NaN   \n",
       "\n",
       "    freeze_layers  grad_clip  hidden_size       lr  n_layers  ...  \\\n",
       "0             NaN        5.0        512.0  0.00005       1.0  ...   \n",
       "1             NaN        5.0         32.0  0.00010       1.0  ...   \n",
       "2             NaN        5.0         32.0  0.00010       1.0  ...   \n",
       "3             NaN        5.0        128.0  0.00100       2.0  ...   \n",
       "4             NaN        5.0        512.0  0.00010       1.0  ...   \n",
       "5             NaN        5.0         32.0  0.00010       1.0  ...   \n",
       "6             NaN        5.0         32.0  0.00010       1.0  ...   \n",
       "7             NaN        5.0        128.0  0.00100       1.0  ...   \n",
       "8             NaN        5.0        128.0  0.00050       1.0  ...   \n",
       "9             NaN        5.0         32.0  0.00010       1.0  ...   \n",
       "10            NaN        5.0         32.0  0.00010       1.0  ...   \n",
       "11            NaN        5.0        128.0  0.00050       2.0  ...   \n",
       "12            NaN        5.0        512.0  0.00010       1.0  ...   \n",
       "13            NaN        5.0         32.0  0.00010       1.0  ...   \n",
       "14            NaN        5.0         32.0  0.00010       1.0  ...   \n",
       "15            NaN        5.0        128.0  0.00100       2.0  ...   \n",
       "16            NaN        5.0        128.0  0.00050       1.0  ...   \n",
       "17            NaN        5.0         32.0  0.00010       1.0  ...   \n",
       "18            NaN        5.0         32.0  0.00010       1.0  ...   \n",
       "19            NaN        5.0        256.0  0.00050       2.0  ...   \n",
       "20            NaN        NaN          NaN      NaN       NaN  ...   \n",
       "21            NaN        NaN          NaN      NaN       NaN  ...   \n",
       "22            NaN        NaN          NaN      NaN       NaN  ...   \n",
       "23            NaN        NaN          NaN      NaN       NaN  ...   \n",
       "24            NaN        NaN          NaN      NaN       NaN  ...   \n",
       "\n",
       "   Backbone_pretty test_next_activity_acc_mean_std  \\\n",
       "0              RNN                 0.7757 ± 0.0142   \n",
       "1          SAP-RPT                 0.6294 ± 0.0121   \n",
       "2           TabPFN                 0.6364 ± 0.0153   \n",
       "3      Transformer                 0.7562 ± 0.0246   \n",
       "4              RNN                 0.8535 ± 0.0013   \n",
       "5          SAP-RPT                 0.6472 ± 0.0268   \n",
       "6           TabPFN                  0.669 ± 0.0194   \n",
       "7      Transformer                 0.8556 ± 0.0058   \n",
       "8              RNN                 0.7841 ± 0.0201   \n",
       "9          SAP-RPT                 0.7646 ± 0.0054   \n",
       "10          TabPFN                 0.6637 ± 0.0121   \n",
       "11     Transformer                 0.7821 ± 0.0061   \n",
       "12             RNN                 0.8777 ± 0.0234   \n",
       "13         SAP-RPT                 0.7434 ± 0.0096   \n",
       "14          TabPFN                 0.7439 ± 0.0088   \n",
       "15     Transformer                 0.8937 ± 0.0013   \n",
       "16             RNN                 0.7302 ± 0.0031   \n",
       "17         SAP-RPT                  0.6869 ± 0.013   \n",
       "18          TabPFN                  0.6229 ± 0.012   \n",
       "19     Transformer                 0.7183 ± 0.0019   \n",
       "20        Majority                     0.226 ± nan   \n",
       "21        Majority                    0.1523 ± nan   \n",
       "22        Majority                    0.1164 ± nan   \n",
       "23        Majority                     0.169 ± nan   \n",
       "24        Majority                    0.0784 ± nan   \n",
       "\n",
       "    test_next_activity_loss_mean_std  test_next_remaining_time_loss_mean_std  \\\n",
       "0                     0.7689 ± 0.042                         1.7738 ± 0.0889   \n",
       "1                          nan ± nan                         2.1601 ± 0.1146   \n",
       "2                          nan ± nan                         2.2338 ± 0.0937   \n",
       "3                    0.7139 ± 0.1077                         2.8388 ± 0.1209   \n",
       "4                    0.4193 ± 0.0068                         0.6725 ± 0.0325   \n",
       "5                          nan ± nan                         1.3056 ± 0.2773   \n",
       "6                          nan ± nan                         1.6011 ± 0.0589   \n",
       "7                    0.4141 ± 0.0136                          1.156 ± 0.0803   \n",
       "8                    0.6578 ± 0.0553                         1.0744 ± 0.0308   \n",
       "9                          nan ± nan                          1.041 ± 0.0605   \n",
       "10                         nan ± nan                         1.1278 ± 0.0716   \n",
       "11                   0.6683 ± 0.0118                         1.1187 ± 0.0865   \n",
       "12                   0.3678 ± 0.0387                         0.6826 ± 0.0213   \n",
       "13                         nan ± nan                         0.6383 ± 0.0255   \n",
       "14                         nan ± nan                          0.7148 ± 0.034   \n",
       "15                   0.2719 ± 0.0012                          0.6571 ± 0.009   \n",
       "16                    0.9502 ± 0.014                         1.0098 ± 0.0383   \n",
       "17                         nan ± nan                         0.9545 ± 0.0682   \n",
       "18                         nan ± nan                          1.091 ± 0.0495   \n",
       "19                   0.8407 ± 0.0143                         0.8088 ± 0.0386   \n",
       "20                      6.2004 ± nan                            1.3884 ± nan   \n",
       "21                      6.7908 ± nan                            1.0963 ± nan   \n",
       "22                      7.0789 ± nan                            1.5902 ± nan   \n",
       "23                      6.6539 ± nan                            1.1543 ± nan   \n",
       "24                      7.3902 ± nan                            1.4593 ± nan   \n",
       "\n",
       "    test_next_time_to_next_event_loss_mean_std  \\\n",
       "0                              1.3352 ± 0.0302   \n",
       "1                               1.6706 ± 0.372   \n",
       "2                              1.6877 ± 0.4063   \n",
       "3                              1.3671 ± 0.0495   \n",
       "4                              0.7629 ± 0.0159   \n",
       "5                              1.1287 ± 0.1969   \n",
       "6                              1.1679 ± 0.2366   \n",
       "7                              0.8836 ± 0.0296   \n",
       "8                              1.1906 ± 0.0485   \n",
       "9                              1.0863 ± 0.2173   \n",
       "10                             1.1919 ± 0.2369   \n",
       "11                              1.237 ± 0.0537   \n",
       "12                             0.7504 ± 0.0088   \n",
       "13                             0.7122 ± 0.0769   \n",
       "14                             0.7176 ± 0.0719   \n",
       "15                             0.7407 ± 0.0091   \n",
       "16                              1.052 ± 0.0308   \n",
       "17                             1.0428 ± 0.0515   \n",
       "18                             1.1187 ± 0.0619   \n",
       "19                             0.8717 ± 0.0301   \n",
       "20                                1.5434 ± nan   \n",
       "21                                1.2125 ± nan   \n",
       "22                                1.5883 ± nan   \n",
       "23                                1.0179 ± nan   \n",
       "24                                1.2475 ± nan   \n",
       "\n",
       "    best_test_next_activity_acc_mean_std  \\\n",
       "0                        0.7799 ± 0.0095   \n",
       "1                              nan ± nan   \n",
       "2                              nan ± nan   \n",
       "3                         0.7687 ± 0.016   \n",
       "4                        0.8535 ± 0.0013   \n",
       "5                              nan ± nan   \n",
       "6                              nan ± nan   \n",
       "7                        0.8596 ± 0.0024   \n",
       "8                        0.7897 ± 0.0134   \n",
       "9                              nan ± nan   \n",
       "10                             nan ± nan   \n",
       "11                       0.7994 ± 0.0105   \n",
       "12                        0.886 ± 0.0114   \n",
       "13                             nan ± nan   \n",
       "14                             nan ± nan   \n",
       "15                        0.8954 ± 0.001   \n",
       "16                       0.7314 ± 0.0026   \n",
       "17                             nan ± nan   \n",
       "18                             nan ± nan   \n",
       "19                       0.7191 ± 0.0022   \n",
       "20                           0.226 ± nan   \n",
       "21                          0.1523 ± nan   \n",
       "22                          0.1164 ± nan   \n",
       "23                           0.169 ± nan   \n",
       "24                          0.0784 ± nan   \n",
       "\n",
       "    best_test_next_activity_loss_mean_std  \\\n",
       "0                          0.758 ± 0.0285   \n",
       "1                               nan ± nan   \n",
       "2                               nan ± nan   \n",
       "3                         0.6635 ± 0.0383   \n",
       "4                         0.4193 ± 0.0068   \n",
       "5                               nan ± nan   \n",
       "6                               nan ± nan   \n",
       "7                          0.407 ± 0.0142   \n",
       "8                         0.6549 ± 0.0492   \n",
       "9                               nan ± nan   \n",
       "10                              nan ± nan   \n",
       "11                        0.6527 ± 0.0174   \n",
       "12                        0.3647 ± 0.0376   \n",
       "13                              nan ± nan   \n",
       "14                              nan ± nan   \n",
       "15                        0.2639 ± 0.0012   \n",
       "16                        0.9381 ± 0.0158   \n",
       "17                              nan ± nan   \n",
       "18                              nan ± nan   \n",
       "19                        0.8369 ± 0.0147   \n",
       "20                           6.2004 ± nan   \n",
       "21                           6.7908 ± nan   \n",
       "22                           7.0789 ± nan   \n",
       "23                           6.6539 ± nan   \n",
       "24                           7.3902 ± nan   \n",
       "\n",
       "    best_test_next_remaining_time_loss_mean_std  \\\n",
       "0                               1.7033 ± 0.1138   \n",
       "1                                     nan ± nan   \n",
       "2                                     nan ± nan   \n",
       "3                               2.6204 ± 0.1508   \n",
       "4                               0.6097 ± 0.0138   \n",
       "5                                     nan ± nan   \n",
       "6                                     nan ± nan   \n",
       "7                               1.1343 ± 0.0847   \n",
       "8                                0.9536 ± 0.027   \n",
       "9                                     nan ± nan   \n",
       "10                                    nan ± nan   \n",
       "11                              0.9541 ± 0.0334   \n",
       "12                              0.6795 ± 0.0219   \n",
       "13                                    nan ± nan   \n",
       "14                                    nan ± nan   \n",
       "15                              0.6438 ± 0.0042   \n",
       "16                              0.7632 ± 0.0399   \n",
       "17                                    nan ± nan   \n",
       "18                                    nan ± nan   \n",
       "19                              0.7829 ± 0.0103   \n",
       "20                                 1.3884 ± nan   \n",
       "21                                 1.0963 ± nan   \n",
       "22                                 1.5902 ± nan   \n",
       "23                                 1.1543 ± nan   \n",
       "24                                 1.4593 ± nan   \n",
       "\n",
       "    best_test_next_time_to_next_event_loss_mean_std        Runtime (h)  \n",
       "0                                    1.3205 ± 0.014  0.01943 ± 0.00025  \n",
       "1                                         nan ± nan  0.00500 ± 0.00093  \n",
       "2                                         nan ± nan  0.12074 ± 0.00085  \n",
       "3                                    1.3364 ± 0.016  0.00758 ± 0.00008  \n",
       "4                                   0.7629 ± 0.0159  0.04166 ± 0.00022  \n",
       "5                                         nan ± nan  0.00440 ± 0.00005  \n",
       "6                                         nan ± nan  0.12188 ± 0.00312  \n",
       "7                                   0.8433 ± 0.0401  0.01953 ± 0.00030  \n",
       "8                                   1.1239 ± 0.0216  0.00316 ± 0.00005  \n",
       "9                                         nan ± nan  0.00448 ± 0.00007  \n",
       "10                                        nan ± nan  0.01156 ± 0.00005  \n",
       "11                                  1.0688 ± 0.0115  0.00162 ± 0.00003  \n",
       "12                                  0.7444 ± 0.0072  0.00952 ± 0.00008  \n",
       "13                                        nan ± nan  0.00452 ± 0.00017  \n",
       "14                                        nan ± nan  0.03380 ± 0.00020  \n",
       "15                                  0.7187 ± 0.0037  0.00471 ± 0.00003  \n",
       "16                                  0.8585 ± 0.0104  0.01171 ± 0.00011  \n",
       "17                                        nan ± nan  0.00449 ± 0.00005  \n",
       "18                                        nan ± nan  0.12027 ± 0.00014  \n",
       "19                                  0.8473 ± 0.0178  0.00808 ± 0.00005  \n",
       "20                                     1.5434 ± nan      0.00989 ± nan  \n",
       "21                                     1.2125 ± nan      0.02985 ± nan  \n",
       "22                                     1.5883 ± nan      0.00031 ± nan  \n",
       "23                                     1.0179 ± nan      0.00585 ± nan  \n",
       "24                                     1.2475 ± nan      0.00661 ± nan  \n",
       "\n",
       "[25 rows x 45 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_cols = [\"log\", \"backbone\"] + HP_COLS\n",
    "\n",
    "baseline_grouped = (\n",
    "    baseline\n",
    "    .groupby(group_cols, dropna=False)\n",
    "    .apply(agg_over_seeds)   # deine Funktion von oben\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "score_col = \"mt_score_mean\"\n",
    "if score_col not in baseline_grouped.columns:\n",
    "    score_col = \"test_next_activity_acc_mean\"\n",
    "\n",
    "idx_best = (\n",
    "    baseline_grouped\n",
    "    .groupby([\"log\", \"backbone\"])[score_col]\n",
    "    .idxmax()\n",
    ")\n",
    "baseline_best = baseline_grouped.loc[idx_best].reset_index(drop=True)\n",
    "\n",
    "baseline_all = pd.concat([baseline_best, majority_grouped], ignore_index=True)\n",
    "\n",
    "DATASET_MAP = {\n",
    "    \"BPI12\": \"BPI12\",\n",
    "    \"BPI17\": \"BPI17\",\n",
    "    \"BPI20PrepaidTravelCosts\": \"BPI20PTC\",\n",
    "    \"BPI20RequestForPayment\": \"BPI20RfP\",\n",
    "    \"BPI20TravelPermitData\": \"BPI20TPD\",\n",
    "}\n",
    "BACKBONE_MAP = {\n",
    "    \"majority\": \"Majority\",\n",
    "    \"rnn\": \"RNN\",\n",
    "    \"transformer\": \"Transformer\",\n",
    "    \"tabpfn\": \"TabPFN\",\n",
    "    \"saprpt\": \"SAP-RPT\",\n",
    "}\n",
    "\n",
    "baseline_all[\"Dataset\"] = baseline_all[\"log\"].map(DATASET_MAP).fillna(baseline_all[\"log\"])\n",
    "baseline_all[\"Backbone_pretty\"] = baseline_all[\"backbone\"].map(BACKBONE_MAP).fillna(baseline_all[\"backbone\"])\n",
    "\n",
    "for m in METRICS:\n",
    "    mean_col = m + \"_mean\"\n",
    "    std_col  = m + \"_std\"\n",
    "    if mean_col in baseline_all.columns and std_col in baseline_all.columns:\n",
    "        baseline_all[m + \"_mean_std\"] = (\n",
    "            baseline_all[mean_col].round(4).astype(str)\n",
    "            + \" ± \"\n",
    "            + baseline_all[std_col].round(4).astype(str)\n",
    "        )\n",
    "        \n",
    "if \"_runtime_mean\" in baseline_all.columns:\n",
    "    baseline_all[\"runtime_mean_h\"] = baseline_all[\"_runtime_mean\"] / 3600.0\n",
    "if \"_runtime_std\" in baseline_all.columns:\n",
    "    baseline_all[\"runtime_std_h\"]  = baseline_all[\"_runtime_std\"]  / 3600.0\n",
    "\n",
    "if {\"runtime_mean_h\", \"runtime_std_h\"}.issubset(baseline_all.columns):\n",
    "    mean_str = baseline_all[\"runtime_mean_h\"].map(lambda x: f\"{x:.5f}\")\n",
    "    std_str  = baseline_all[\"runtime_std_h\"].map(lambda x: f\"{x:.5f}\")\n",
    "\n",
    "    baseline_all[\"Runtime (h)\"] = mean_str + \" ± \" + std_str\n",
    "    \n",
    "cols_to_drop = [\"_runtime_mean\", \"_runtime_std\", \"runtime_mean_h\", \"runtime_std_h\"]\n",
    "cols_to_drop = [c for c in cols_to_drop if c in baseline_all.columns]\n",
    "\n",
    "baseline_all = baseline_all.drop(columns=cols_to_drop)\n",
    "\n",
    "csv_path = os.path.join(output_dir_csv, \"baseline_best_settings_mean_std.csv\")\n",
    "baseline_all.to_csv(csv_path, index=False)\n",
    "print(\"Saved baseline summary to:\", csv_path)\n",
    "\n",
    "baseline_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "872e9b00",
   "metadata": {
    "title": "Multi-task models"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Hyperparameter columns: ['batch_size', 'embedding_size', 'epochs', 'freeze_layers', 'grad_clip', 'hidden_size', 'lr', 'n_layers', 'rnn_type', 'strategy', 'weight_decay', 'lora_alpha', 'r', 'few_shot_k']\n",
      "Saved LLM summary to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/llm_all_settings_by_method_mean_std.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log</th>\n",
       "      <th>backbone</th>\n",
       "      <th>Setting</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>embedding_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>freeze_layers</th>\n",
       "      <th>grad_clip</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>...</th>\n",
       "      <th>Backbone_pretty</th>\n",
       "      <th>test_next_activity_acc_mean_std</th>\n",
       "      <th>test_next_activity_loss_mean_std</th>\n",
       "      <th>test_next_remaining_time_loss_mean_std</th>\n",
       "      <th>test_next_time_to_next_event_loss_mean_std</th>\n",
       "      <th>best_test_next_activity_acc_mean_std</th>\n",
       "      <th>best_test_next_activity_loss_mean_std</th>\n",
       "      <th>best_test_next_remaining_time_loss_mean_std</th>\n",
       "      <th>best_test_next_time_to_next_event_loss_mean_std</th>\n",
       "      <th>Runtime (h)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>FewShot-Freezing</td>\n",
       "      <td>8</td>\n",
       "      <td>2304</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2304</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>...</td>\n",
       "      <td>Gemma-2-2B</td>\n",
       "      <td>0.0799 ± 0.0341</td>\n",
       "      <td>3.6757 ± 0.2217</td>\n",
       "      <td>1.712 ± 0.236</td>\n",
       "      <td>1.609 ± 0.0568</td>\n",
       "      <td>0.0876 ± 0.0281</td>\n",
       "      <td>3.4803 ± 0.1926</td>\n",
       "      <td>1.3363 ± 0.123</td>\n",
       "      <td>1.5519 ± 0.0226</td>\n",
       "      <td>0.11437 ± 0.00098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>FewShot-LoRA</td>\n",
       "      <td>8</td>\n",
       "      <td>2304</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2304</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>...</td>\n",
       "      <td>Gemma-2-2B</td>\n",
       "      <td>0.0751 ± 0.0496</td>\n",
       "      <td>4.1549 ± 0.8314</td>\n",
       "      <td>3.5247 ± 2.3271</td>\n",
       "      <td>2.502 ± 1.3303</td>\n",
       "      <td>0.0953 ± 0.0496</td>\n",
       "      <td>3.1654 ± 0.2169</td>\n",
       "      <td>1.4307 ± 0.2371</td>\n",
       "      <td>1.6082 ± 0.0677</td>\n",
       "      <td>0.14829 ± 0.00058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>Freezing</td>\n",
       "      <td>8</td>\n",
       "      <td>2304</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2304</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>...</td>\n",
       "      <td>Gemma-2-2B</td>\n",
       "      <td>0.3912 ± 0.0314</td>\n",
       "      <td>1.4898 ± 0.0318</td>\n",
       "      <td>2.0048 ± 0.0841</td>\n",
       "      <td>1.5121 ± 0.0317</td>\n",
       "      <td>0.4028 ± 0.0211</td>\n",
       "      <td>1.4816 ± 0.0364</td>\n",
       "      <td>1.9842 ± 0.0928</td>\n",
       "      <td>1.4755 ± 0.0157</td>\n",
       "      <td>0.63840 ± 0.00268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>Freezing-[-1, -2]</td>\n",
       "      <td>8</td>\n",
       "      <td>2304</td>\n",
       "      <td>10</td>\n",
       "      <td>-1,-2</td>\n",
       "      <td>5</td>\n",
       "      <td>2304</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>...</td>\n",
       "      <td>Gemma-2-2B</td>\n",
       "      <td>0.7409 ± 0.0644</td>\n",
       "      <td>0.7587 ± 0.1567</td>\n",
       "      <td>2.9607 ± 0.0668</td>\n",
       "      <td>1.407 ± 0.0574</td>\n",
       "      <td>0.7921 ± 0.0109</td>\n",
       "      <td>0.6666 ± 0.0472</td>\n",
       "      <td>2.1902 ± 0.1723</td>\n",
       "      <td>1.3602 ± 0.0079</td>\n",
       "      <td>0.70089 ± 0.00147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>Freezing-[-1]</td>\n",
       "      <td>8</td>\n",
       "      <td>2304</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>2304</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>...</td>\n",
       "      <td>Gemma-2-2B</td>\n",
       "      <td>0.7481 ± 0.0678</td>\n",
       "      <td>0.8252 ± 0.3164</td>\n",
       "      <td>2.6484 ± 0.1867</td>\n",
       "      <td>1.4281 ± 0.0205</td>\n",
       "      <td>0.8007 ± 0.0061</td>\n",
       "      <td>0.6464 ± 0.0429</td>\n",
       "      <td>2.2036 ± 0.1755</td>\n",
       "      <td>1.3601 ± 0.0117</td>\n",
       "      <td>0.66969 ± 0.00339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>Freezing-[0, 1]</td>\n",
       "      <td>8</td>\n",
       "      <td>2304</td>\n",
       "      <td>10</td>\n",
       "      <td>0,1</td>\n",
       "      <td>5</td>\n",
       "      <td>2304</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>...</td>\n",
       "      <td>Gemma-2-2B</td>\n",
       "      <td>0.7328 ± 0.0603</td>\n",
       "      <td>0.7753 ± 0.0927</td>\n",
       "      <td>2.413 ± 0.1309</td>\n",
       "      <td>1.3021 ± 0.0234</td>\n",
       "      <td>0.797 ± 0.0121</td>\n",
       "      <td>0.6983 ± 0.0244</td>\n",
       "      <td>2.3113 ± 0.1783</td>\n",
       "      <td>1.2989 ± 0.019</td>\n",
       "      <td>0.70158 ± 0.00118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>Freezing-[0]</td>\n",
       "      <td>8</td>\n",
       "      <td>2304</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2304</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>...</td>\n",
       "      <td>Gemma-2-2B</td>\n",
       "      <td>0.7857 ± 0.0103</td>\n",
       "      <td>0.729 ± 0.0074</td>\n",
       "      <td>2.4203 ± 0.1474</td>\n",
       "      <td>1.3369 ± 0.0233</td>\n",
       "      <td>0.7933 ± 0.0051</td>\n",
       "      <td>0.7206 ± 0.0217</td>\n",
       "      <td>2.2122 ± 0.0645</td>\n",
       "      <td>1.3219 ± 0.0238</td>\n",
       "      <td>0.67204 ± 0.00218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>LoRA</td>\n",
       "      <td>8</td>\n",
       "      <td>2304</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2304</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>...</td>\n",
       "      <td>Gemma-2-2B</td>\n",
       "      <td>0.7726 ± 0.0182</td>\n",
       "      <td>0.6433 ± 0.0493</td>\n",
       "      <td>2.6577 ± 0.2777</td>\n",
       "      <td>1.4247 ± 0.0643</td>\n",
       "      <td>0.8033 ± 0.0094</td>\n",
       "      <td>0.5982 ± 0.0168</td>\n",
       "      <td>1.912 ± 0.1038</td>\n",
       "      <td>1.3099 ± 0.0157</td>\n",
       "      <td>0.72725 ± 0.00116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>ZeroShot</td>\n",
       "      <td>8</td>\n",
       "      <td>2304</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2304</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>...</td>\n",
       "      <td>Gemma-2-2B</td>\n",
       "      <td>0.0298 ± 0.0184</td>\n",
       "      <td>4.4444 ± 0.6145</td>\n",
       "      <td>4.3306 ± 3.2487</td>\n",
       "      <td>3.6357 ± 2.514</td>\n",
       "      <td>0.0298 ± 0.0184</td>\n",
       "      <td>4.4444 ± 0.6145</td>\n",
       "      <td>4.3306 ± 3.2487</td>\n",
       "      <td>3.6357 ± 2.514</td>\n",
       "      <td>0.01072 ± 0.00068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>FewShot-Freezing</td>\n",
       "      <td>8</td>\n",
       "      <td>768</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>768</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>...</td>\n",
       "      <td>GPT2</td>\n",
       "      <td>0.0372 ± 0.0315</td>\n",
       "      <td>3.5797 ± 0.3214</td>\n",
       "      <td>1.9015 ± 0.8402</td>\n",
       "      <td>1.6738 ± 0.0887</td>\n",
       "      <td>0.0387 ± 0.0323</td>\n",
       "      <td>3.5797 ± 0.3214</td>\n",
       "      <td>1.8622 ± 0.8651</td>\n",
       "      <td>1.5788 ± 0.0816</td>\n",
       "      <td>0.00966 ± 0.00005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>FewShot-LoRA</td>\n",
       "      <td>8</td>\n",
       "      <td>768</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>768</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>...</td>\n",
       "      <td>GPT2</td>\n",
       "      <td>0.066 ± 0.0435</td>\n",
       "      <td>3.7202 ± 0.3176</td>\n",
       "      <td>2.3615 ± 0.6155</td>\n",
       "      <td>1.8859 ± 0.3613</td>\n",
       "      <td>0.1164 ± 0.0438</td>\n",
       "      <td>3.2111 ± 0.0963</td>\n",
       "      <td>1.4217 ± 0.2947</td>\n",
       "      <td>1.5557 ± 0.0218</td>\n",
       "      <td>0.01476 ± 0.00008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>Freezing</td>\n",
       "      <td>8</td>\n",
       "      <td>768</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>768</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>...</td>\n",
       "      <td>GPT2</td>\n",
       "      <td>0.4803 ± 0.0131</td>\n",
       "      <td>1.5799 ± 0.0667</td>\n",
       "      <td>2.4276 ± 0.1278</td>\n",
       "      <td>1.4812 ± 0.0203</td>\n",
       "      <td>0.4837 ± 0.0199</td>\n",
       "      <td>1.5797 ± 0.0667</td>\n",
       "      <td>2.2677 ± 0.0831</td>\n",
       "      <td>1.4791 ± 0.0194</td>\n",
       "      <td>0.05725 ± 0.00027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>Freezing-[-1, -2]</td>\n",
       "      <td>8</td>\n",
       "      <td>768</td>\n",
       "      <td>10</td>\n",
       "      <td>-1,-2</td>\n",
       "      <td>5</td>\n",
       "      <td>768</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>...</td>\n",
       "      <td>GPT2</td>\n",
       "      <td>0.6947 ± 0.0256</td>\n",
       "      <td>0.9061 ± 0.057</td>\n",
       "      <td>2.6129 ± 0.1035</td>\n",
       "      <td>1.462 ± 0.0251</td>\n",
       "      <td>0.6954 ± 0.0242</td>\n",
       "      <td>0.9034 ± 0.0582</td>\n",
       "      <td>2.026 ± 0.1606</td>\n",
       "      <td>1.4404 ± 0.0239</td>\n",
       "      <td>0.06356 ± 0.00019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>Freezing-[-1]</td>\n",
       "      <td>8</td>\n",
       "      <td>768</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>768</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>...</td>\n",
       "      <td>GPT2</td>\n",
       "      <td>0.6911 ± 0.0075</td>\n",
       "      <td>0.9258 ± 0.039</td>\n",
       "      <td>2.4571 ± 0.0836</td>\n",
       "      <td>1.4513 ± 0.0207</td>\n",
       "      <td>0.6911 ± 0.0075</td>\n",
       "      <td>0.9258 ± 0.039</td>\n",
       "      <td>2.0822 ± 0.1661</td>\n",
       "      <td>1.4288 ± 0.0158</td>\n",
       "      <td>0.06041 ± 0.00019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>Freezing-[0, 1]</td>\n",
       "      <td>8</td>\n",
       "      <td>768</td>\n",
       "      <td>10</td>\n",
       "      <td>0,1</td>\n",
       "      <td>5</td>\n",
       "      <td>768</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>...</td>\n",
       "      <td>GPT2</td>\n",
       "      <td>0.7404 ± 0.0289</td>\n",
       "      <td>0.7458 ± 0.0581</td>\n",
       "      <td>2.4466 ± 0.0754</td>\n",
       "      <td>1.4039 ± 0.0141</td>\n",
       "      <td>0.7568 ± 0.0197</td>\n",
       "      <td>0.7256 ± 0.0445</td>\n",
       "      <td>2.1075 ± 0.1452</td>\n",
       "      <td>1.3891 ± 0.0152</td>\n",
       "      <td>0.06336 ± 0.00018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      log    backbone            Setting  batch_size  embedding_size  epochs  \\\n",
       "0   BPI12  gemma-2-2b   FewShot-Freezing           8            2304      10   \n",
       "1   BPI12  gemma-2-2b       FewShot-LoRA           8            2304      10   \n",
       "2   BPI12  gemma-2-2b           Freezing           8            2304      10   \n",
       "3   BPI12  gemma-2-2b  Freezing-[-1, -2]           8            2304      10   \n",
       "4   BPI12  gemma-2-2b      Freezing-[-1]           8            2304      10   \n",
       "5   BPI12  gemma-2-2b    Freezing-[0, 1]           8            2304      10   \n",
       "6   BPI12  gemma-2-2b       Freezing-[0]           8            2304      10   \n",
       "7   BPI12  gemma-2-2b               LoRA           8            2304      10   \n",
       "8   BPI12  gemma-2-2b           ZeroShot           8            2304       0   \n",
       "9   BPI12        gpt2   FewShot-Freezing           8             768      10   \n",
       "10  BPI12        gpt2       FewShot-LoRA           8             768      10   \n",
       "11  BPI12        gpt2           Freezing           8             768      10   \n",
       "12  BPI12        gpt2  Freezing-[-1, -2]           8             768      10   \n",
       "13  BPI12        gpt2      Freezing-[-1]           8             768      10   \n",
       "14  BPI12        gpt2    Freezing-[0, 1]           8             768      10   \n",
       "\n",
       "   freeze_layers  grad_clip  hidden_size       lr  ...  Backbone_pretty  \\\n",
       "0            NaN          5         2304  0.00005  ...       Gemma-2-2B   \n",
       "1            NaN          5         2304  0.00005  ...       Gemma-2-2B   \n",
       "2            NaN          5         2304  0.00005  ...       Gemma-2-2B   \n",
       "3          -1,-2          5         2304  0.00005  ...       Gemma-2-2B   \n",
       "4             -1          5         2304  0.00005  ...       Gemma-2-2B   \n",
       "5            0,1          5         2304  0.00005  ...       Gemma-2-2B   \n",
       "6              0          5         2304  0.00005  ...       Gemma-2-2B   \n",
       "7            NaN          5         2304  0.00005  ...       Gemma-2-2B   \n",
       "8            NaN          5         2304  0.00005  ...       Gemma-2-2B   \n",
       "9            NaN          5          768  0.00005  ...             GPT2   \n",
       "10           NaN          5          768  0.00005  ...             GPT2   \n",
       "11           NaN          5          768  0.00005  ...             GPT2   \n",
       "12         -1,-2          5          768  0.00005  ...             GPT2   \n",
       "13            -1          5          768  0.00005  ...             GPT2   \n",
       "14           0,1          5          768  0.00005  ...             GPT2   \n",
       "\n",
       "   test_next_activity_acc_mean_std test_next_activity_loss_mean_std  \\\n",
       "0                  0.0799 ± 0.0341                  3.6757 ± 0.2217   \n",
       "1                  0.0751 ± 0.0496                  4.1549 ± 0.8314   \n",
       "2                  0.3912 ± 0.0314                  1.4898 ± 0.0318   \n",
       "3                  0.7409 ± 0.0644                  0.7587 ± 0.1567   \n",
       "4                  0.7481 ± 0.0678                  0.8252 ± 0.3164   \n",
       "5                  0.7328 ± 0.0603                  0.7753 ± 0.0927   \n",
       "6                  0.7857 ± 0.0103                   0.729 ± 0.0074   \n",
       "7                  0.7726 ± 0.0182                  0.6433 ± 0.0493   \n",
       "8                  0.0298 ± 0.0184                  4.4444 ± 0.6145   \n",
       "9                  0.0372 ± 0.0315                  3.5797 ± 0.3214   \n",
       "10                  0.066 ± 0.0435                  3.7202 ± 0.3176   \n",
       "11                 0.4803 ± 0.0131                  1.5799 ± 0.0667   \n",
       "12                 0.6947 ± 0.0256                   0.9061 ± 0.057   \n",
       "13                 0.6911 ± 0.0075                   0.9258 ± 0.039   \n",
       "14                 0.7404 ± 0.0289                  0.7458 ± 0.0581   \n",
       "\n",
       "    test_next_remaining_time_loss_mean_std  \\\n",
       "0                            1.712 ± 0.236   \n",
       "1                          3.5247 ± 2.3271   \n",
       "2                          2.0048 ± 0.0841   \n",
       "3                          2.9607 ± 0.0668   \n",
       "4                          2.6484 ± 0.1867   \n",
       "5                           2.413 ± 0.1309   \n",
       "6                          2.4203 ± 0.1474   \n",
       "7                          2.6577 ± 0.2777   \n",
       "8                          4.3306 ± 3.2487   \n",
       "9                          1.9015 ± 0.8402   \n",
       "10                         2.3615 ± 0.6155   \n",
       "11                         2.4276 ± 0.1278   \n",
       "12                         2.6129 ± 0.1035   \n",
       "13                         2.4571 ± 0.0836   \n",
       "14                         2.4466 ± 0.0754   \n",
       "\n",
       "    test_next_time_to_next_event_loss_mean_std  \\\n",
       "0                               1.609 ± 0.0568   \n",
       "1                               2.502 ± 1.3303   \n",
       "2                              1.5121 ± 0.0317   \n",
       "3                               1.407 ± 0.0574   \n",
       "4                              1.4281 ± 0.0205   \n",
       "5                              1.3021 ± 0.0234   \n",
       "6                              1.3369 ± 0.0233   \n",
       "7                              1.4247 ± 0.0643   \n",
       "8                               3.6357 ± 2.514   \n",
       "9                              1.6738 ± 0.0887   \n",
       "10                             1.8859 ± 0.3613   \n",
       "11                             1.4812 ± 0.0203   \n",
       "12                              1.462 ± 0.0251   \n",
       "13                             1.4513 ± 0.0207   \n",
       "14                             1.4039 ± 0.0141   \n",
       "\n",
       "    best_test_next_activity_acc_mean_std  \\\n",
       "0                        0.0876 ± 0.0281   \n",
       "1                        0.0953 ± 0.0496   \n",
       "2                        0.4028 ± 0.0211   \n",
       "3                        0.7921 ± 0.0109   \n",
       "4                        0.8007 ± 0.0061   \n",
       "5                         0.797 ± 0.0121   \n",
       "6                        0.7933 ± 0.0051   \n",
       "7                        0.8033 ± 0.0094   \n",
       "8                        0.0298 ± 0.0184   \n",
       "9                        0.0387 ± 0.0323   \n",
       "10                       0.1164 ± 0.0438   \n",
       "11                       0.4837 ± 0.0199   \n",
       "12                       0.6954 ± 0.0242   \n",
       "13                       0.6911 ± 0.0075   \n",
       "14                       0.7568 ± 0.0197   \n",
       "\n",
       "    best_test_next_activity_loss_mean_std  \\\n",
       "0                         3.4803 ± 0.1926   \n",
       "1                         3.1654 ± 0.2169   \n",
       "2                         1.4816 ± 0.0364   \n",
       "3                         0.6666 ± 0.0472   \n",
       "4                         0.6464 ± 0.0429   \n",
       "5                         0.6983 ± 0.0244   \n",
       "6                         0.7206 ± 0.0217   \n",
       "7                         0.5982 ± 0.0168   \n",
       "8                         4.4444 ± 0.6145   \n",
       "9                         3.5797 ± 0.3214   \n",
       "10                        3.2111 ± 0.0963   \n",
       "11                        1.5797 ± 0.0667   \n",
       "12                        0.9034 ± 0.0582   \n",
       "13                         0.9258 ± 0.039   \n",
       "14                        0.7256 ± 0.0445   \n",
       "\n",
       "    best_test_next_remaining_time_loss_mean_std  \\\n",
       "0                                1.3363 ± 0.123   \n",
       "1                               1.4307 ± 0.2371   \n",
       "2                               1.9842 ± 0.0928   \n",
       "3                               2.1902 ± 0.1723   \n",
       "4                               2.2036 ± 0.1755   \n",
       "5                               2.3113 ± 0.1783   \n",
       "6                               2.2122 ± 0.0645   \n",
       "7                                1.912 ± 0.1038   \n",
       "8                               4.3306 ± 3.2487   \n",
       "9                               1.8622 ± 0.8651   \n",
       "10                              1.4217 ± 0.2947   \n",
       "11                              2.2677 ± 0.0831   \n",
       "12                               2.026 ± 0.1606   \n",
       "13                              2.0822 ± 0.1661   \n",
       "14                              2.1075 ± 0.1452   \n",
       "\n",
       "    best_test_next_time_to_next_event_loss_mean_std        Runtime (h)  \n",
       "0                                   1.5519 ± 0.0226  0.11437 ± 0.00098  \n",
       "1                                   1.6082 ± 0.0677  0.14829 ± 0.00058  \n",
       "2                                   1.4755 ± 0.0157  0.63840 ± 0.00268  \n",
       "3                                   1.3602 ± 0.0079  0.70089 ± 0.00147  \n",
       "4                                   1.3601 ± 0.0117  0.66969 ± 0.00339  \n",
       "5                                    1.2989 ± 0.019  0.70158 ± 0.00118  \n",
       "6                                   1.3219 ± 0.0238  0.67204 ± 0.00218  \n",
       "7                                   1.3099 ± 0.0157  0.72725 ± 0.00116  \n",
       "8                                    3.6357 ± 2.514  0.01072 ± 0.00068  \n",
       "9                                   1.5788 ± 0.0816  0.00966 ± 0.00005  \n",
       "10                                  1.5557 ± 0.0218  0.01476 ± 0.00008  \n",
       "11                                  1.4791 ± 0.0194  0.05725 ± 0.00027  \n",
       "12                                  1.4404 ± 0.0239  0.06356 ± 0.00019  \n",
       "13                                  1.4288 ± 0.0158  0.06041 ± 0.00019  \n",
       "14                                  1.3891 ± 0.0152  0.06336 ± 0.00018  \n",
       "\n",
       "[15 rows x 49 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "\n",
    "llm = df[df[\"backbone\"].isin(LLM_BACKBONES)].copy()\n",
    "\n",
    "llm[\"Setting\"] = llm.apply(map_setting, axis=1)\n",
    "\n",
    "NON_HP_COLS_LLM = set(NON_HP_COLS)\n",
    "for col in [\"lora_alpha\", \"r\", \"few_shot_k\"]:\n",
    "    NON_HP_COLS_LLM.discard(col)\n",
    "NON_HP_COLS_LLM.add(\"Setting\")\n",
    "\n",
    "HP_COLS_LLM = [c for c in llm.columns if c not in NON_HP_COLS_LLM]\n",
    "print(\"LLM Hyperparameter columns:\", HP_COLS_LLM)\n",
    "\n",
    "group_cols_llm = [\"log\", \"backbone\", \"Setting\"] + HP_COLS_LLM\n",
    "\n",
    "llm_grouped = (\n",
    "    llm\n",
    "    .groupby(group_cols_llm, dropna=False)\n",
    "    .apply(agg_over_seeds)   # gleiche Funktion wie bei Baselines\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "score_col = \"mt_score_mean\"\n",
    "if score_col not in llm_grouped.columns:\n",
    "    score_col = \"test_next_activity_acc_mean\"\n",
    "\n",
    "idx_best_llm = (\n",
    "    llm_grouped\n",
    "    .groupby([\"log\", \"backbone\", \"Setting\"])[score_col]\n",
    "    .idxmax()\n",
    ")\n",
    "\n",
    "llm_all = llm_grouped.loc[idx_best_llm].reset_index(drop=True)\n",
    "\n",
    "BACKBONE_MAP_LLM = {\n",
    "    \"gpt2\":         \"GPT2\",\n",
    "    \"gptneo-1b3\":   \"GPT-Neo-1.3B\",\n",
    "    \"qwen25-05b\":   \"Qwen2.5-0.5B\",\n",
    "    \"llama32-1b\":   \"Llama3.2-1B\",\n",
    "    \"gemma-2-2b\":   \"Gemma-2-2B\",\n",
    "}\n",
    "\n",
    "llm_all[\"Dataset\"] = llm_all[\"log\"].map(DATASET_MAP).fillna(llm_all[\"log\"])\n",
    "llm_all[\"Backbone_pretty\"] = llm_all[\"backbone\"].map(BACKBONE_MAP_LLM).fillna(llm_all[\"backbone\"])\n",
    "\n",
    "for m in METRICS:\n",
    "    mean_col = m + \"_mean\"\n",
    "    std_col  = m + \"_std\"\n",
    "    if mean_col in llm_all.columns and std_col in llm_all.columns:\n",
    "        llm_all[m + \"_mean_std\"] = (\n",
    "            llm_all[mean_col].round(4).astype(str)\n",
    "            + \" ± \"\n",
    "            + llm_all[std_col].round(4).astype(str)\n",
    "        )\n",
    "        \n",
    "if \"_runtime_mean\" in llm_all.columns:\n",
    "    llm_all[\"runtime_mean_h\"] = llm_all[\"_runtime_mean\"] / 3600.0\n",
    "if \"_runtime_std\" in llm_all.columns:\n",
    "    llm_all[\"runtime_std_h\"]  = llm_all[\"_runtime_std\"]  / 3600.0\n",
    "\n",
    "if {\"runtime_mean_h\", \"runtime_std_h\"}.issubset(llm_all.columns):\n",
    "    mean_str = llm_all[\"runtime_mean_h\"].map(lambda x: f\"{x:.5f}\")\n",
    "    std_str  = llm_all[\"runtime_std_h\"].map(lambda x: f\"{x:.5f}\")\n",
    "    llm_all[\"Runtime (h)\"] = mean_str + \" ± \" + std_str\n",
    "\n",
    "cols_to_drop_llm = [\"_runtime_mean\", \"_runtime_std\", \"runtime_mean_h\", \"runtime_std_h\"]\n",
    "cols_to_drop_llm = [c for c in cols_to_drop_llm if c in llm_all.columns]\n",
    "llm_all = llm_all.drop(columns=cols_to_drop_llm)\n",
    "\n",
    "csv_path = os.path.join(output_dir_csv, \"llm_all_settings_by_method_mean_std.csv\")\n",
    "llm_all.to_csv(csv_path, index=False)\n",
    "print(\"Saved LLM summary to:\", csv_path)\n",
    "\n",
    "llm_all.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6952eb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting\n",
      "LoRA                 729\n",
      "Freezing-[-1, -2]    125\n",
      "Freezing-[0, 1]      125\n",
      "Freezing-[0]         125\n",
      "Freezing-[-1]        125\n",
      "Freezing             125\n",
      "ZeroShot             125\n",
      "FewShot-Freezing     125\n",
      "FewShot-LoRA         125\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "llm[\"Setting\"] = llm.apply(map_setting, axis=1)\n",
    "print(llm[\"Setting\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83d95d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined multi-task table to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/multi_task_benchmark_results.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log</th>\n",
       "      <th>backbone</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>embedding_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>freeze_layers</th>\n",
       "      <th>grad_clip</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>...</th>\n",
       "      <th>test_next_time_to_next_event_loss_mean_std</th>\n",
       "      <th>best_test_next_activity_acc_mean_std</th>\n",
       "      <th>best_test_next_activity_loss_mean_std</th>\n",
       "      <th>best_test_next_remaining_time_loss_mean_std</th>\n",
       "      <th>best_test_next_time_to_next_event_loss_mean_std</th>\n",
       "      <th>Runtime (h)</th>\n",
       "      <th>Setting</th>\n",
       "      <th>lora_alpha</th>\n",
       "      <th>r</th>\n",
       "      <th>few_shot_k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>gptneo-1b3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7584 ± 0.1475</td>\n",
       "      <td>0.0734 ± 0.0279</td>\n",
       "      <td>3.3621 ± 0.3649</td>\n",
       "      <td>1.5616 ± 0.2029</td>\n",
       "      <td>1.6504 ± 0.1312</td>\n",
       "      <td>0.07263 ± 0.00036</td>\n",
       "      <td>FewShot-Freezing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>gptneo-1b3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8453 ± 0.314</td>\n",
       "      <td>0.1002 ± 0.0504</td>\n",
       "      <td>3.1159 ± 0.0594</td>\n",
       "      <td>1.4986 ± 0.4039</td>\n",
       "      <td>1.5639 ± 0.0325</td>\n",
       "      <td>0.07842 ± 0.00021</td>\n",
       "      <td>FewShot-LoRA</td>\n",
       "      <td>512.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>gptneo-1b3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.531 ± 0.0338</td>\n",
       "      <td>0.5736 ± 0.0769</td>\n",
       "      <td>1.4783 ± 0.3746</td>\n",
       "      <td>2.2858 ± 0.1486</td>\n",
       "      <td>1.4952 ± 0.0342</td>\n",
       "      <td>0.40460 ± 0.00102</td>\n",
       "      <td>Freezing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>gptneo-1b3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1,-2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5124 ± 0.0677</td>\n",
       "      <td>0.5663 ± 0.071</td>\n",
       "      <td>1.4508 ± 0.2137</td>\n",
       "      <td>2.3459 ± 0.1544</td>\n",
       "      <td>1.4892 ± 0.0518</td>\n",
       "      <td>0.44785 ± 0.00081</td>\n",
       "      <td>Freezing-[-1, -2]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BPI12</td>\n",
       "      <td>gptneo-1b3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5131 ± 0.0366</td>\n",
       "      <td>0.5474 ± 0.0859</td>\n",
       "      <td>1.4897 ± 0.2106</td>\n",
       "      <td>2.3335 ± 0.0808</td>\n",
       "      <td>1.492 ± 0.0327</td>\n",
       "      <td>0.42557 ± 0.00100</td>\n",
       "      <td>Freezing-[-1]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     log    backbone  batch_size  embedding_size  epochs freeze_layers  \\\n",
       "0  BPI12  gptneo-1b3         8.0          2048.0    10.0           NaN   \n",
       "1  BPI12  gptneo-1b3         8.0          2048.0    10.0           NaN   \n",
       "2  BPI12  gptneo-1b3         8.0          2048.0    10.0           NaN   \n",
       "3  BPI12  gptneo-1b3         8.0          2048.0    10.0         -1,-2   \n",
       "4  BPI12  gptneo-1b3         8.0          2048.0    10.0            -1   \n",
       "\n",
       "   grad_clip  hidden_size       lr  n_layers  ...  \\\n",
       "0        5.0       2048.0  0.00005       1.0  ...   \n",
       "1        5.0       2048.0  0.00005       1.0  ...   \n",
       "2        5.0       2048.0  0.00005       1.0  ...   \n",
       "3        5.0       2048.0  0.00005       1.0  ...   \n",
       "4        5.0       2048.0  0.00005       1.0  ...   \n",
       "\n",
       "  test_next_time_to_next_event_loss_mean_std  \\\n",
       "0                            1.7584 ± 0.1475   \n",
       "1                             1.8453 ± 0.314   \n",
       "2                             1.531 ± 0.0338   \n",
       "3                            1.5124 ± 0.0677   \n",
       "4                            1.5131 ± 0.0366   \n",
       "\n",
       "  best_test_next_activity_acc_mean_std  best_test_next_activity_loss_mean_std  \\\n",
       "0                      0.0734 ± 0.0279                        3.3621 ± 0.3649   \n",
       "1                      0.1002 ± 0.0504                        3.1159 ± 0.0594   \n",
       "2                      0.5736 ± 0.0769                        1.4783 ± 0.3746   \n",
       "3                       0.5663 ± 0.071                        1.4508 ± 0.2137   \n",
       "4                      0.5474 ± 0.0859                        1.4897 ± 0.2106   \n",
       "\n",
       "   best_test_next_remaining_time_loss_mean_std  \\\n",
       "0                              1.5616 ± 0.2029   \n",
       "1                              1.4986 ± 0.4039   \n",
       "2                              2.2858 ± 0.1486   \n",
       "3                              2.3459 ± 0.1544   \n",
       "4                              2.3335 ± 0.0808   \n",
       "\n",
       "   best_test_next_time_to_next_event_loss_mean_std        Runtime (h)  \\\n",
       "0                                  1.6504 ± 0.1312  0.07263 ± 0.00036   \n",
       "1                                  1.5639 ± 0.0325  0.07842 ± 0.00021   \n",
       "2                                  1.4952 ± 0.0342  0.40460 ± 0.00102   \n",
       "3                                  1.4892 ± 0.0518  0.44785 ± 0.00081   \n",
       "4                                   1.492 ± 0.0327  0.42557 ± 0.00100   \n",
       "\n",
       "             Setting  lora_alpha      r  few_shot_k  \n",
       "0   FewShot-Freezing         NaN    NaN         8.0  \n",
       "1       FewShot-LoRA       512.0  256.0         8.0  \n",
       "2           Freezing         NaN    NaN         NaN  \n",
       "3  Freezing-[-1, -2]         NaN    NaN         NaN  \n",
       "4      Freezing-[-1]         NaN    NaN         NaN  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi = pd.concat([baseline_all, llm_all], ignore_index=True, sort=False)\n",
    "\n",
    "multi = (\n",
    "    multi\n",
    "    .sort_values([\"Dataset\", \"Backbone_pretty\", \"Setting\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "csv_path = os.path.join(output_dir_csv, \"multi_task_benchmark_results.csv\")\n",
    "multi.to_csv(csv_path, index=False)\n",
    "print(\"Saved combined multi-task table to:\", csv_path)\n",
    "\n",
    "multi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa585d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved per-log table for BPI12 to /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI12/multi_task_benchmark_results_BPI12.csv\n",
      "Saved per-log table for BPI17 to /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI17/multi_task_benchmark_results_BPI17.csv\n",
      "Saved per-log table for BPI20PrepaidTravelCosts to /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20PrepaidTravelCosts/multi_task_benchmark_results_BPI20PrepaidTravelCosts.csv\n",
      "Saved per-log table for BPI20RequestForPayment to /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20RequestForPayment/multi_task_benchmark_results_BPI20RequestForPayment.csv\n",
      "Saved per-log table for BPI20TravelPermitData to /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20TravelPermitData/multi_task_benchmark_results_BPI20TravelPermitData.csv\n"
     ]
    }
   ],
   "source": [
    "multi_path = os.path.join(output_dir_csv, \"multi_task_benchmark_results.csv\")\n",
    "multi = pd.read_csv(multi_path)\n",
    "\n",
    "for log_name, df_log in multi.groupby(\"log\"):\n",
    "    log_dir = os.path.join(output_dir_csv, \"per_dataset\", log_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    csv_path = os.path.join(log_dir, f\"multi_task_benchmark_results_{log_name}.csv\")\n",
    "    \n",
    "    df_log.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved per-log table for {log_name} to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "931c0b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/multi_task_results_r256_a512.csv\n"
     ]
    }
   ],
   "source": [
    "FIXED_R = 256\n",
    "FIXED_ALPHA = 512\n",
    "\n",
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "LORA_SETTINGS_REPLACE = [\"LoRA\", \"FewShot-LoRA\"]  # set to [\"LoRA\"] if you only want to fix full LoRA\n",
    "\n",
    "base_path = os.path.join(output_dir_csv, \"multi_task_benchmark_results.csv\")\n",
    "multi_base = pd.read_csv(base_path)\n",
    "\n",
    "if \"Setting\" not in multi_base.columns:\n",
    "    multi_base[\"Setting\"] = np.nan\n",
    "\n",
    "if \"mt_score\" not in df.columns:\n",
    "    tmp = df[\n",
    "        df[\"test_next_activity_acc\"].notna()\n",
    "        & df[\"test_next_remaining_time_loss\"].notna()\n",
    "        & df[\"test_next_time_to_next_event_loss\"].notna()\n",
    "    ].copy()\n",
    "    sc_acc = MinMaxScaler()\n",
    "    sc_rt  = MinMaxScaler()\n",
    "    sc_nt  = MinMaxScaler()\n",
    "    tmp[\"na_norm\"] = sc_acc.fit_transform(tmp[[\"test_next_activity_acc\"]])\n",
    "    tmp[\"rt_norm\"] = sc_rt.fit_transform(-tmp[[\"test_next_remaining_time_loss\"]])\n",
    "    tmp[\"nt_norm\"] = sc_nt.fit_transform(-tmp[[\"test_next_time_to_next_event_loss\"]])\n",
    "    tmp[\"mt_score\"] = tmp[\"na_norm\"] + tmp[\"rt_norm\"] + tmp[\"nt_norm\"]\n",
    "    df = df.merge(tmp[[\"id\", \"mt_score\"]], on=\"id\", how=\"left\")\n",
    "\n",
    "llm = df[df[\"backbone\"].isin(LLM_BACKBONES)].copy()\n",
    "llm[\"Setting\"] = llm.apply(map_setting, axis=1)\n",
    "\n",
    "llm = llm[\n",
    "    (llm[\"Setting\"].isin(LORA_SETTINGS_REPLACE))\n",
    "    & (llm[\"r\"] == FIXED_R)\n",
    "    & (llm[\"lora_alpha\"] == FIXED_ALPHA)\n",
    "].copy()\n",
    "\n",
    "HP = [\"lr\",\"batch_size\",\"epochs\",\"embedding_size\",\"hidden_size\",\"strategy\",\"weight_decay\",\"grad_clip\",\"n_layers\",\"r\",\"lora_alpha\",\"few_shot_k\"]\n",
    "HP = [c for c in HP if c in llm.columns]\n",
    "group_cols = [\"log\", \"backbone\", \"Setting\"] + HP\n",
    "\n",
    "llm_grouped = llm.groupby(group_cols, dropna=False).apply(agg_over_seeds).reset_index()\n",
    "\n",
    "score_col = \"mt_score_mean\" if \"mt_score_mean\" in llm_grouped.columns else \"test_next_activity_acc_mean\"\n",
    "idx = llm_grouped.groupby([\"log\", \"backbone\", \"Setting\"])[score_col].idxmax()\n",
    "llm_fixed_best = llm_grouped.loc[idx].reset_index(drop=True)\n",
    "\n",
    "if \"Dataset\" in multi_base.columns and \"Dataset\" not in llm_fixed_best.columns:\n",
    "    llm_fixed_best[\"Dataset\"] = llm_fixed_best[\"log\"].map(DATASET_MAP).fillna(llm_fixed_best[\"log\"])\n",
    "if \"Backbone_pretty\" in multi_base.columns and \"Backbone_pretty\" not in llm_fixed_best.columns:\n",
    "    llm_fixed_best[\"Backbone_pretty\"] = llm_fixed_best[\"backbone\"].map(BACKBONE_MAP_LLM).fillna(llm_fixed_best[\"backbone\"])\n",
    "\n",
    "for m in METRICS:\n",
    "    mc, sc = m + \"_mean\", m + \"_std\"\n",
    "    outc = m + \"_mean_std\"\n",
    "    if mc in llm_fixed_best.columns and sc in llm_fixed_best.columns:\n",
    "        llm_fixed_best[outc] = llm_fixed_best[mc].round(4).astype(str) + \" ± \" + llm_fixed_best[sc].round(4).astype(str)\n",
    "\n",
    "if \"_runtime_mean\" in llm_fixed_best.columns:\n",
    "    llm_fixed_best[\"runtime_mean_h\"] = llm_fixed_best[\"_runtime_mean\"] / 3600.0\n",
    "if \"_runtime_std\" in llm_fixed_best.columns:\n",
    "    llm_fixed_best[\"runtime_std_h\"] = llm_fixed_best[\"_runtime_std\"] / 3600.0\n",
    "if {\"runtime_mean_h\", \"runtime_std_h\"}.issubset(llm_fixed_best.columns):\n",
    "    llm_fixed_best[\"Runtime (h)\"] = (\n",
    "        llm_fixed_best[\"runtime_mean_h\"].map(lambda x: f\"{x:.5f}\")\n",
    "        + \" ± \"\n",
    "        + llm_fixed_best[\"runtime_std_h\"].map(lambda x: f\"{x:.5f}\")\n",
    "    )\n",
    "\n",
    "drop_rt = [c for c in [\"_runtime_mean\",\"_runtime_std\",\"runtime_mean_h\",\"runtime_std_h\"] if c in llm_fixed_best.columns]\n",
    "if drop_rt:\n",
    "    llm_fixed_best = llm_fixed_best.drop(columns=drop_rt)\n",
    "\n",
    "multi_new = multi_base[\n",
    "    ~(\n",
    "        multi_base[\"backbone\"].isin(LLM_BACKBONES)\n",
    "        & multi_base[\"Setting\"].isin(LORA_SETTINGS_REPLACE)\n",
    "    )\n",
    "].copy()\n",
    "\n",
    "llm_fixed_best = llm_fixed_best.reindex(columns=multi_new.columns, fill_value=np.nan)\n",
    "multi_new = pd.concat([multi_new, llm_fixed_best], ignore_index=True)\n",
    "\n",
    "sort_cols = [c for c in [\"Dataset\", \"Backbone_pretty\", \"Setting\"] if c in multi_new.columns]\n",
    "if sort_cols:\n",
    "    multi_new = multi_new.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "out_path = os.path.join(output_dir_csv, \"multi_task_results_r256_a512.csv\")\n",
    "multi_new.to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2e0e563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved baseline vs LoRA table for BPI12 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI12/baseline_vs_lora_multi_task_original_results_BPI12.csv\n",
      "Saved baseline vs LoRA table for BPI17 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI17/baseline_vs_lora_multi_task_original_results_BPI17.csv\n",
      "Saved baseline vs LoRA table for BPI20PrepaidTravelCosts to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20PrepaidTravelCosts/baseline_vs_lora_multi_task_original_results_BPI20PrepaidTravelCosts.csv\n",
      "Saved baseline vs LoRA table for BPI20RequestForPayment to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20RequestForPayment/baseline_vs_lora_multi_task_original_results_BPI20RequestForPayment.csv\n",
      "Saved baseline vs LoRA table for BPI20TravelPermitData to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20TravelPermitData/baseline_vs_lora_multi_task_original_results_BPI20TravelPermitData.csv\n"
     ]
    }
   ],
   "source": [
    "multi_path = os.path.join(output_dir_csv, \"multi_task_results_r256_a512.csv\")\n",
    "multi = pd.read_csv(multi_path)\n",
    "\n",
    "if \"Setting\" not in multi.columns:\n",
    "    multi[\"Setting\"] = np.nan\n",
    "\n",
    "BASELINE_BACKBONES = [\"majority\", \"rnn\", \"transformer\", \"tabpfn\", \"saprpt\"]\n",
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "\n",
    "subset = multi[\n",
    "    multi[\"backbone\"].isin(BASELINE_BACKBONES)\n",
    "    | (multi[\"backbone\"].isin(LLM_BACKBONES) & (multi[\"Setting\"] == \"LoRA\"))\n",
    "].copy()\n",
    "\n",
    "if \"Runtime (h)\" not in subset.columns:\n",
    "    subset[\"Runtime (h)\"] = np.nan\n",
    "\n",
    "EXCLUDE_JOIN = {\n",
    "    \"Dataset\", \"Backbone_pretty\",\n",
    "    \"Runtime (h)\", \"n_runs\",\n",
    "    \"mt_score\", \"mt_score_mean\", \"mt_score_std\",\n",
    "    \"total_params\", \"trainable_params\",\n",
    "}\n",
    "for c in subset.columns:\n",
    "    if c.endswith(\"_mean\") or c.endswith(\"_std\") or c.endswith(\"_mean_std\"):\n",
    "        EXCLUDE_JOIN.add(c)\n",
    "\n",
    "def _infer_join_cols(sub_df: pd.DataFrame, raw_df: pd.DataFrame) -> list[str]:\n",
    "    cols = [c for c in sub_df.columns if c in raw_df.columns and c not in EXCLUDE_JOIN]\n",
    "    for k in [\"log\", \"backbone\"]:\n",
    "        if k not in cols and k in sub_df.columns and k in raw_df.columns:\n",
    "            cols.insert(0, k)\n",
    "    return cols\n",
    "\n",
    "def _round_float_cols(df_in: pd.DataFrame, cols: list[str], ndigits: int = 10) -> pd.DataFrame:\n",
    "    df_out = df_in.copy()\n",
    "    for c in cols:\n",
    "        if c in df_out.columns and pd.api.types.is_float_dtype(df_out[c]):\n",
    "            df_out[c] = df_out[c].round(ndigits)\n",
    "    return df_out\n",
    "\n",
    "def _fill_runtime(part_idx: pd.Index, raw_df: pd.DataFrame):\n",
    "    if part_idx.empty or raw_df.empty or \"_runtime\" not in raw_df.columns:\n",
    "        return\n",
    "    sub_part = subset.loc[part_idx].copy()\n",
    "    join_cols = _infer_join_cols(sub_part, raw_df)\n",
    "    if not join_cols:\n",
    "        return\n",
    "\n",
    "    sub_r = _round_float_cols(sub_part, join_cols)\n",
    "    raw_r = _round_float_cols(raw_df, join_cols)\n",
    "\n",
    "    rt = (\n",
    "        raw_r.groupby(join_cols, dropna=False)\n",
    "        .agg(_runtime_mean=(\"_runtime\", \"mean\"), _runtime_std=(\"_runtime\", \"std\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "    rt[\"_runtime_std\"] = rt[\"_runtime_std\"].fillna(0.0)\n",
    "    rt[\"Runtime (h)_recalc\"] = (\n",
    "        (rt[\"_runtime_mean\"] / 3600.0).map(lambda x: f\"{x:.5f}\")\n",
    "        + \" ± \"\n",
    "        + (rt[\"_runtime_std\"] / 3600.0).map(lambda x: f\"{x:.5f}\")\n",
    "    )\n",
    "\n",
    "    merged = sub_r.merge(rt[join_cols + [\"Runtime (h)_recalc\"]], on=join_cols, how=\"left\")\n",
    "    subset.loc[part_idx, \"Runtime (h)\"] = (\n",
    "        subset.loc[part_idx, \"Runtime (h)\"].fillna(merged[\"Runtime (h)_recalc\"].to_numpy()).values\n",
    "    )\n",
    "\n",
    "need_runtime = subset[\"Runtime (h)\"].isna().any()\n",
    "if need_runtime:\n",
    "    df_base = df[df[\"backbone\"].isin(BASELINE_BACKBONES)].copy()\n",
    "\n",
    "    df_lora = df[(df[\"backbone\"].isin(LLM_BACKBONES)) & (df[\"fine_tuning\"] == \"lora\")].copy()\n",
    "    if \"few_shot_k\" in df_lora.columns:\n",
    "        df_lora = df_lora[df_lora[\"few_shot_k\"].isna()].copy()\n",
    "\n",
    "    idx_base = subset.index[subset[\"backbone\"].isin(BASELINE_BACKBONES)]\n",
    "    idx_lora = subset.index[subset[\"backbone\"].isin(LLM_BACKBONES) & (subset[\"Setting\"] == \"LoRA\")]\n",
    "\n",
    "    _fill_runtime(idx_base, df_base)\n",
    "    _fill_runtime(idx_lora, df_lora)\n",
    "\n",
    "def _fmt_params(total, trainable) -> str:\n",
    "    if pd.isna(total) or float(total) == 0.0:\n",
    "        return \"\"\n",
    "    total_fmt = np.format_float_scientific(float(total), precision=1)\n",
    "    if pd.isna(trainable):\n",
    "        return total_fmt\n",
    "    pct = (float(trainable) / float(total)) * 100.0\n",
    "    return f\"{total_fmt} ({pct:.0f}%)\"\n",
    "\n",
    "subset[\"# params (%trainable)\"] = [\n",
    "    _fmt_params(t, tr) for t, tr in zip(subset.get(\"total_params\", np.nan), subset.get(\"trainable_params\", np.nan))\n",
    "]\n",
    "\n",
    "for log_name, df_log in subset.groupby(\"log\"):\n",
    "    log_dir = os.path.join(output_dir_csv, \"per_dataset\", log_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    out_path = os.path.join(log_dir, f\"baseline_vs_lora_multi_task_original_results_{log_name}.csv\")\n",
    "    df_log.to_csv(out_path, index=False)\n",
    "    print(f\"Saved baseline vs LoRA table for {log_name} to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0b9f2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved baseline vs LoRA table for BPI12 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI12/baseline_vs_lora_multi_task_results_BPI12.csv\n",
      "Saved baseline vs LoRA table for BPI17 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI17/baseline_vs_lora_multi_task_results_BPI17.csv\n",
      "Saved baseline vs LoRA table for BPI20PrepaidTravelCosts to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20PrepaidTravelCosts/baseline_vs_lora_multi_task_results_BPI20PrepaidTravelCosts.csv\n",
      "Saved baseline vs LoRA table for BPI20RequestForPayment to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20RequestForPayment/baseline_vs_lora_multi_task_results_BPI20RequestForPayment.csv\n",
      "Saved baseline vs LoRA table for BPI20TravelPermitData to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20TravelPermitData/baseline_vs_lora_multi_task_results_BPI20TravelPermitData.csv\n"
     ]
    }
   ],
   "source": [
    "multi_path = os.path.join(output_dir_csv, \"multi_task_benchmark_results.csv\")\n",
    "multi = pd.read_csv(multi_path)\n",
    "\n",
    "if \"Setting\" not in multi.columns:\n",
    "    multi[\"Setting\"] = np.nan\n",
    "\n",
    "BASELINE_BACKBONES = [\"majority\", \"rnn\", \"transformer\", \"tabpfn\", \"saprpt\"]\n",
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "\n",
    "subset = multi[\n",
    "    multi[\"backbone\"].isin(BASELINE_BACKBONES)\n",
    "    | (multi[\"backbone\"].isin(LLM_BACKBONES) & (multi[\"Setting\"] == \"LoRA\"))\n",
    "].copy()\n",
    "\n",
    "if \"Runtime (h)\" not in subset.columns:\n",
    "    subset[\"Runtime (h)\"] = np.nan\n",
    "\n",
    "EXCLUDE_JOIN = {\n",
    "    \"Dataset\", \"Backbone_pretty\",\n",
    "    \"Runtime (h)\", \"n_runs\",\n",
    "    \"mt_score\", \"mt_score_mean\", \"mt_score_std\",\n",
    "    \"total_params\", \"trainable_params\",\n",
    "}\n",
    "for c in subset.columns:\n",
    "    if c.endswith(\"_mean\") or c.endswith(\"_std\") or c.endswith(\"_mean_std\"):\n",
    "        EXCLUDE_JOIN.add(c)\n",
    "\n",
    "def _infer_join_cols(sub_df: pd.DataFrame, raw_df: pd.DataFrame) -> list[str]:\n",
    "    cols = [c for c in sub_df.columns if c in raw_df.columns and c not in EXCLUDE_JOIN]\n",
    "    for k in [\"log\", \"backbone\"]:\n",
    "        if k not in cols and k in sub_df.columns and k in raw_df.columns:\n",
    "            cols.insert(0, k)\n",
    "    return cols\n",
    "\n",
    "def _round_float_cols(df_in: pd.DataFrame, cols: list[str], ndigits: int = 10) -> pd.DataFrame:\n",
    "    df_out = df_in.copy()\n",
    "    for c in cols:\n",
    "        if c in df_out.columns and pd.api.types.is_float_dtype(df_out[c]):\n",
    "            df_out[c] = df_out[c].round(ndigits)\n",
    "    return df_out\n",
    "\n",
    "def _fill_runtime(part_idx: pd.Index, raw_df: pd.DataFrame):\n",
    "    if part_idx.empty or raw_df.empty or \"_runtime\" not in raw_df.columns:\n",
    "        return\n",
    "    sub_part = subset.loc[part_idx].copy()\n",
    "    join_cols = _infer_join_cols(sub_part, raw_df)\n",
    "    if not join_cols:\n",
    "        return\n",
    "\n",
    "    sub_r = _round_float_cols(sub_part, join_cols)\n",
    "    raw_r = _round_float_cols(raw_df, join_cols)\n",
    "\n",
    "    rt = (\n",
    "        raw_r.groupby(join_cols, dropna=False)\n",
    "        .agg(_runtime_mean=(\"_runtime\", \"mean\"), _runtime_std=(\"_runtime\", \"std\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "    rt[\"_runtime_std\"] = rt[\"_runtime_std\"].fillna(0.0)\n",
    "    rt[\"Runtime (h)_recalc\"] = (\n",
    "        (rt[\"_runtime_mean\"] / 3600.0).map(lambda x: f\"{x:.5f}\")\n",
    "        + \" ± \"\n",
    "        + (rt[\"_runtime_std\"] / 3600.0).map(lambda x: f\"{x:.5f}\")\n",
    "    )\n",
    "\n",
    "    merged = sub_r.merge(rt[join_cols + [\"Runtime (h)_recalc\"]], on=join_cols, how=\"left\")\n",
    "    subset.loc[part_idx, \"Runtime (h)\"] = (\n",
    "        subset.loc[part_idx, \"Runtime (h)\"].fillna(merged[\"Runtime (h)_recalc\"].to_numpy()).values\n",
    "    )\n",
    "\n",
    "need_runtime = subset[\"Runtime (h)\"].isna().any()\n",
    "if need_runtime:\n",
    "    df_base = df[df[\"backbone\"].isin(BASELINE_BACKBONES)].copy()\n",
    "\n",
    "    df_lora = df[(df[\"backbone\"].isin(LLM_BACKBONES)) & (df[\"fine_tuning\"] == \"lora\")].copy()\n",
    "    if \"few_shot_k\" in df_lora.columns:\n",
    "        df_lora = df_lora[df_lora[\"few_shot_k\"].isna()].copy()\n",
    "\n",
    "    idx_base = subset.index[subset[\"backbone\"].isin(BASELINE_BACKBONES)]\n",
    "    idx_lora = subset.index[subset[\"backbone\"].isin(LLM_BACKBONES) & (subset[\"Setting\"] == \"LoRA\")]\n",
    "\n",
    "    _fill_runtime(idx_base, df_base)\n",
    "    _fill_runtime(idx_lora, df_lora)\n",
    "\n",
    "def _fmt_params(total, trainable) -> str:\n",
    "    if pd.isna(total) or float(total) == 0.0:\n",
    "        return \"\"\n",
    "    total_fmt = np.format_float_scientific(float(total), precision=1)\n",
    "    if pd.isna(trainable):\n",
    "        return total_fmt\n",
    "    pct = (float(trainable) / float(total)) * 100.0\n",
    "    return f\"{total_fmt} ({pct:.0f}%)\"\n",
    "\n",
    "subset[\"# params (%trainable)\"] = [\n",
    "    _fmt_params(t, tr) for t, tr in zip(subset.get(\"total_params\", np.nan), subset.get(\"trainable_params\", np.nan))\n",
    "]\n",
    "\n",
    "for log_name, df_log in subset.groupby(\"log\"):\n",
    "    log_dir = os.path.join(output_dir_csv, \"per_dataset\", log_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    out_path = os.path.join(log_dir, f\"baseline_vs_lora_multi_task_results_{log_name}.csv\")\n",
    "    df_log.to_csv(out_path, index=False)\n",
    "    print(f\"Saved baseline vs LoRA table for {log_name} to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0c13022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LLM methods table for log=BPI12, backbone=gemma-2-2b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI12/llm_methods_BPI12_gemma-2-2b.csv\n",
      "Saved LLM methods table for log=BPI12, backbone=gpt2 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI12/llm_methods_BPI12_gpt2.csv\n",
      "Saved LLM methods table for log=BPI12, backbone=gptneo-1b3 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI12/llm_methods_BPI12_gptneo-1b3.csv\n",
      "Saved LLM methods table for log=BPI12, backbone=llama32-1b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI12/llm_methods_BPI12_llama32-1b.csv\n",
      "Saved LLM methods table for log=BPI12, backbone=qwen25-05b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI12/llm_methods_BPI12_qwen25-05b.csv\n",
      "Saved LLM methods table for log=BPI17, backbone=gemma-2-2b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI17/llm_methods_BPI17_gemma-2-2b.csv\n",
      "Saved LLM methods table for log=BPI17, backbone=gpt2 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI17/llm_methods_BPI17_gpt2.csv\n",
      "Saved LLM methods table for log=BPI17, backbone=gptneo-1b3 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI17/llm_methods_BPI17_gptneo-1b3.csv\n",
      "Saved LLM methods table for log=BPI17, backbone=llama32-1b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI17/llm_methods_BPI17_llama32-1b.csv\n",
      "Saved LLM methods table for log=BPI17, backbone=qwen25-05b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI17/llm_methods_BPI17_qwen25-05b.csv\n",
      "Saved LLM methods table for log=BPI20PrepaidTravelCosts, backbone=gemma-2-2b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20PrepaidTravelCosts/llm_methods_BPI20PrepaidTravelCosts_gemma-2-2b.csv\n",
      "Saved LLM methods table for log=BPI20PrepaidTravelCosts, backbone=gpt2 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20PrepaidTravelCosts/llm_methods_BPI20PrepaidTravelCosts_gpt2.csv\n",
      "Saved LLM methods table for log=BPI20PrepaidTravelCosts, backbone=gptneo-1b3 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20PrepaidTravelCosts/llm_methods_BPI20PrepaidTravelCosts_gptneo-1b3.csv\n",
      "Saved LLM methods table for log=BPI20PrepaidTravelCosts, backbone=llama32-1b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20PrepaidTravelCosts/llm_methods_BPI20PrepaidTravelCosts_llama32-1b.csv\n",
      "Saved LLM methods table for log=BPI20PrepaidTravelCosts, backbone=qwen25-05b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20PrepaidTravelCosts/llm_methods_BPI20PrepaidTravelCosts_qwen25-05b.csv\n",
      "Saved LLM methods table for log=BPI20RequestForPayment, backbone=gemma-2-2b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20RequestForPayment/llm_methods_BPI20RequestForPayment_gemma-2-2b.csv\n",
      "Saved LLM methods table for log=BPI20RequestForPayment, backbone=gpt2 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20RequestForPayment/llm_methods_BPI20RequestForPayment_gpt2.csv\n",
      "Saved LLM methods table for log=BPI20RequestForPayment, backbone=gptneo-1b3 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20RequestForPayment/llm_methods_BPI20RequestForPayment_gptneo-1b3.csv\n",
      "Saved LLM methods table for log=BPI20RequestForPayment, backbone=llama32-1b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20RequestForPayment/llm_methods_BPI20RequestForPayment_llama32-1b.csv\n",
      "Saved LLM methods table for log=BPI20RequestForPayment, backbone=qwen25-05b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20RequestForPayment/llm_methods_BPI20RequestForPayment_qwen25-05b.csv\n",
      "Saved LLM methods table for log=BPI20TravelPermitData, backbone=gemma-2-2b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20TravelPermitData/llm_methods_BPI20TravelPermitData_gemma-2-2b.csv\n",
      "Saved LLM methods table for log=BPI20TravelPermitData, backbone=gpt2 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20TravelPermitData/llm_methods_BPI20TravelPermitData_gpt2.csv\n",
      "Saved LLM methods table for log=BPI20TravelPermitData, backbone=gptneo-1b3 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20TravelPermitData/llm_methods_BPI20TravelPermitData_gptneo-1b3.csv\n",
      "Saved LLM methods table for log=BPI20TravelPermitData, backbone=llama32-1b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20TravelPermitData/llm_methods_BPI20TravelPermitData_llama32-1b.csv\n",
      "Saved LLM methods table for log=BPI20TravelPermitData, backbone=qwen25-05b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20TravelPermitData/llm_methods_BPI20TravelPermitData_qwen25-05b.csv\n"
     ]
    }
   ],
   "source": [
    "# %% Export LLM methods per dataset/backbone (+ \"# params (%trainable)\")\n",
    "\n",
    "multi_path = os.path.join(output_dir_csv, \"multi_task_results_r256_a512.csv\")\n",
    "multi = pd.read_csv(multi_path)\n",
    "\n",
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "llm_multi = multi[multi[\"backbone\"].isin(LLM_BACKBONES)].copy()\n",
    "\n",
    "# --- add \"# params (%trainable)\" column ---\n",
    "def _fmt_params(total, trainable) -> str:\n",
    "    if pd.isna(total) or float(total) == 0.0:\n",
    "        return \"\"\n",
    "    total_fmt = np.format_float_scientific(float(total), precision=1)  # e.g., 1.2e+09\n",
    "    if pd.isna(trainable):\n",
    "        return total_fmt\n",
    "    pct = (float(trainable) / float(total)) * 100.0\n",
    "    return f\"{total_fmt} ({pct:.0f}%)\"\n",
    "\n",
    "if \"total_params\" in llm_multi.columns and \"trainable_params\" in llm_multi.columns:\n",
    "    llm_multi[\"# params (%trainable)\"] = [\n",
    "        _fmt_params(t, tr) for t, tr in zip(llm_multi[\"total_params\"], llm_multi[\"trainable_params\"])\n",
    "    ]\n",
    "else:\n",
    "    llm_multi[\"# params (%trainable)\"] = \"\"\n",
    "\n",
    "# --- export per dataset/backbone ---\n",
    "for (log_name, backbone), df_sub in llm_multi.groupby([\"log\", \"backbone\"]):\n",
    "    log_dir = os.path.join(output_dir_csv, \"per_dataset\", log_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    csv_path = os.path.join(log_dir, f\"llm_methods_{log_name}_{backbone}.csv\")\n",
    "    df_sub.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved LLM methods table for log={log_name}, backbone={backbone} to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd3673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LoRA sweeps table for log=BPI12, backbone=gemma-2-2b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI12/llm_methods_BPI12_gemma-2-2b_lora_sweeps.csv\n",
      "Saved LoRA sweeps table for log=BPI12, backbone=gpt2 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI12/llm_methods_BPI12_gpt2_lora_sweeps.csv\n",
      "Saved LoRA sweeps table for log=BPI12, backbone=gptneo-1b3 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI12/llm_methods_BPI12_gptneo-1b3_lora_sweeps.csv\n",
      "Saved LoRA sweeps table for log=BPI12, backbone=llama32-1b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI12/llm_methods_BPI12_llama32-1b_lora_sweeps.csv\n",
      "Saved LoRA sweeps table for log=BPI12, backbone=qwen25-05b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI12/llm_methods_BPI12_qwen25-05b_lora_sweeps.csv\n",
      "Saved LoRA sweeps table for log=BPI17, backbone=gemma-2-2b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI17/llm_methods_BPI17_gemma-2-2b_lora_sweeps.csv\n",
      "Saved LoRA sweeps table for log=BPI17, backbone=gpt2 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI17/llm_methods_BPI17_gpt2_lora_sweeps.csv\n",
      "Saved LoRA sweeps table for log=BPI17, backbone=gptneo-1b3 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI17/llm_methods_BPI17_gptneo-1b3_lora_sweeps.csv\n",
      "Saved LoRA sweeps table for log=BPI17, backbone=llama32-1b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI17/llm_methods_BPI17_llama32-1b_lora_sweeps.csv\n",
      "Saved LoRA sweeps table for log=BPI17, backbone=qwen25-05b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI17/llm_methods_BPI17_qwen25-05b_lora_sweeps.csv\n",
      "Saved LoRA sweeps table for log=BPI20PrepaidTravelCosts, backbone=gemma-2-2b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20PrepaidTravelCosts/llm_methods_BPI20PrepaidTravelCosts_gemma-2-2b_lora_sweeps.csv\n",
      "Saved LoRA sweeps table for log=BPI20PrepaidTravelCosts, backbone=gpt2 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20PrepaidTravelCosts/llm_methods_BPI20PrepaidTravelCosts_gpt2_lora_sweeps.csv\n",
      "Saved LoRA sweeps table for log=BPI20PrepaidTravelCosts, backbone=gptneo-1b3 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20PrepaidTravelCosts/llm_methods_BPI20PrepaidTravelCosts_gptneo-1b3_lora_sweeps.csv\n",
      "Saved LoRA sweeps table for log=BPI20PrepaidTravelCosts, backbone=llama32-1b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20PrepaidTravelCosts/llm_methods_BPI20PrepaidTravelCosts_llama32-1b_lora_sweeps.csv\n",
      "Saved LoRA sweeps table for log=BPI20PrepaidTravelCosts, backbone=qwen25-05b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20PrepaidTravelCosts/llm_methods_BPI20PrepaidTravelCosts_qwen25-05b_lora_sweeps.csv\n",
      "Saved LoRA sweeps table for log=BPI20RequestForPayment, backbone=gemma-2-2b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20RequestForPayment/llm_methods_BPI20RequestForPayment_gemma-2-2b_lora_sweeps.csv\n",
      "Saved LoRA sweeps table for log=BPI20RequestForPayment, backbone=gpt2 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20RequestForPayment/llm_methods_BPI20RequestForPayment_gpt2_lora_sweeps.csv\n",
      "Saved LoRA sweeps table for log=BPI20RequestForPayment, backbone=gptneo-1b3 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20RequestForPayment/llm_methods_BPI20RequestForPayment_gptneo-1b3_lora_sweeps.csv\n",
      "Saved LoRA sweeps table for log=BPI20RequestForPayment, backbone=llama32-1b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20RequestForPayment/llm_methods_BPI20RequestForPayment_llama32-1b_lora_sweeps.csv\n",
      "Saved LoRA sweeps table for log=BPI20RequestForPayment, backbone=qwen25-05b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20RequestForPayment/llm_methods_BPI20RequestForPayment_qwen25-05b_lora_sweeps.csv\n",
      "Saved LoRA sweeps table for log=BPI20TravelPermitData, backbone=gemma-2-2b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20TravelPermitData/llm_methods_BPI20TravelPermitData_gemma-2-2b_lora_sweeps.csv\n",
      "Saved LoRA sweeps table for log=BPI20TravelPermitData, backbone=gpt2 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20TravelPermitData/llm_methods_BPI20TravelPermitData_gpt2_lora_sweeps.csv\n",
      "Saved LoRA sweeps table for log=BPI20TravelPermitData, backbone=gptneo-1b3 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20TravelPermitData/llm_methods_BPI20TravelPermitData_gptneo-1b3_lora_sweeps.csv\n",
      "Saved LoRA sweeps table for log=BPI20TravelPermitData, backbone=llama32-1b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20TravelPermitData/llm_methods_BPI20TravelPermitData_llama32-1b_lora_sweeps.csv\n",
      "Saved LoRA sweeps table for log=BPI20TravelPermitData, backbone=qwen25-05b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20TravelPermitData/llm_methods_BPI20TravelPermitData_qwen25-05b_lora_sweeps.csv\n"
     ]
    }
   ],
   "source": [
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "\n",
    "lora_sweeps = df[\n",
    "    df[\"backbone\"].isin(LLM_BACKBONES)\n",
    "    & (df[\"fine_tuning\"] == \"lora\")\n",
    "].copy()\n",
    "\n",
    "if \"few_shot_k\" in lora_sweeps.columns:\n",
    "    lora_sweeps = lora_sweeps[lora_sweeps[\"few_shot_k\"].isna()].copy()\n",
    "\n",
    "HP_SWEEP_COLS = [\n",
    "    \"lr\",\n",
    "    \"batch_size\",\n",
    "    \"epochs\",\n",
    "    \"r\",\n",
    "    \"lora_alpha\",\n",
    "    \"embedding_size\",\n",
    "    \"hidden_size\",\n",
    "    \"strategy\",\n",
    "]\n",
    "HP_SWEEP_COLS = [c for c in HP_SWEEP_COLS if c in lora_sweeps.columns]\n",
    "\n",
    "group_cols_sweep = [\"log\", \"backbone\"] + HP_SWEEP_COLS\n",
    "\n",
    "lora_sweeps_grouped = (\n",
    "    lora_sweeps\n",
    "    .groupby(group_cols_sweep, dropna=False)\n",
    "    .apply(agg_over_seeds)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "for m in METRICS:\n",
    "    mean_col = m + \"_mean\"\n",
    "    std_col  = m + \"_std\"\n",
    "    if mean_col in lora_sweeps_grouped.columns and std_col in lora_sweeps_grouped.columns:\n",
    "        lora_sweeps_grouped[m + \"_mean_std\"] = (\n",
    "            lora_sweeps_grouped[mean_col].round(4).astype(str)\n",
    "            + \" ± \"\n",
    "            + lora_sweeps_grouped[std_col].round(4).astype(str)\n",
    "        )\n",
    "\n",
    "if \"_runtime_mean\" in lora_sweeps_grouped.columns:\n",
    "    lora_sweeps_grouped[\"runtime_mean_h\"] = lora_sweeps_grouped[\"_runtime_mean\"] / 3600.0\n",
    "if \"_runtime_std\" in lora_sweeps_grouped.columns:\n",
    "    lora_sweeps_grouped[\"runtime_std_h\"]  = lora_sweeps_grouped[\"_runtime_std\"]  / 3600.0\n",
    "\n",
    "if {\"runtime_mean_h\", \"runtime_std_h\"}.issubset(lora_sweeps_grouped.columns):\n",
    "    mean_str = lora_sweeps_grouped[\"runtime_mean_h\"].map(lambda x: f\"{x:.5f}\")\n",
    "    std_str  = lora_sweeps_grouped[\"runtime_std_h\"].map(lambda x: f\"{x:.5f}\")\n",
    "    lora_sweeps_grouped[\"Runtime (h)\"] = mean_str + \" ± \" + std_str\n",
    "\n",
    "cols_to_drop_sweeps = [\"_runtime_mean\", \"_runtime_std\", \"runtime_mean_h\", \"runtime_std_h\"]\n",
    "cols_to_drop_sweeps = [c for c in cols_to_drop_sweeps if c in lora_sweeps_grouped.columns]\n",
    "lora_sweeps_grouped = lora_sweeps_grouped.drop(columns=cols_to_drop_sweeps)\n",
    "\n",
    "def _fmt_params(total, trainable) -> str:\n",
    "    if pd.isna(total) or float(total) == 0.0:\n",
    "        return \"\"\n",
    "    total_fmt = np.format_float_scientific(float(total), precision=1)  # e.g., 1.2e+09\n",
    "    if pd.isna(trainable):\n",
    "        return total_fmt\n",
    "    pct = (float(trainable) / float(total)) * 100.0\n",
    "    return f\"{total_fmt} ({pct:.0f}%)\"\n",
    "\n",
    "if \"total_params\" in lora_sweeps_grouped.columns and \"trainable_params\" in lora_sweeps_grouped.columns:\n",
    "    lora_sweeps_grouped[\"# params (%trainable)\"] = [\n",
    "        _fmt_params(t, tr)\n",
    "        for t, tr in zip(lora_sweeps_grouped[\"total_params\"], lora_sweeps_grouped[\"trainable_params\"])\n",
    "    ]\n",
    "else:\n",
    "    lora_sweeps_grouped[\"# params (%trainable)\"] = \"\"\n",
    "\n",
    "for (log_name, backbone), df_sub in lora_sweeps_grouped.groupby([\"log\", \"backbone\"]):\n",
    "    log_dir = os.path.join(output_dir_csv, \"per_dataset\", log_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    out_path = os.path.join(\n",
    "        log_dir,\n",
    "        f\"llm_methods_{log_name}_{backbone}_lora_sweeps.csv\"\n",
    "    )\n",
    "    df_sub.to_csv(out_path, index=False)\n",
    "    print(f\"Saved LoRA sweeps table for log={log_name}, backbone={backbone} to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77607748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LLM-backbone comparison for log=BPI12, setting=ZeroShot to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI12/llm_backbones_boxplot_BPI12_ZeroShot.png\n",
      "Saved LLM-backbone comparison for log=BPI12, setting=LoRA to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI12/llm_backbones_boxplot_BPI12_LoRA.png\n",
      "Saved LLM-backbone comparison for log=BPI12, setting=FewShot-LoRA to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI12/llm_backbones_boxplot_BPI12_FewShot-LoRA.png\n",
      "Saved LLM-backbone comparison for log=BPI17, setting=ZeroShot to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI17/llm_backbones_boxplot_BPI17_ZeroShot.png\n",
      "Saved LLM-backbone comparison for log=BPI17, setting=LoRA to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI17/llm_backbones_boxplot_BPI17_LoRA.png\n",
      "Saved LLM-backbone comparison for log=BPI17, setting=FewShot-LoRA to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI17/llm_backbones_boxplot_BPI17_FewShot-LoRA.png\n",
      "Saved LLM-backbone comparison for log=BPI20PrepaidTravelCosts, setting=ZeroShot to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20PrepaidTravelCosts/llm_backbones_boxplot_BPI20PrepaidTravelCosts_ZeroShot.png\n",
      "Saved LLM-backbone comparison for log=BPI20PrepaidTravelCosts, setting=LoRA to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20PrepaidTravelCosts/llm_backbones_boxplot_BPI20PrepaidTravelCosts_LoRA.png\n",
      "Saved LLM-backbone comparison for log=BPI20PrepaidTravelCosts, setting=FewShot-LoRA to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20PrepaidTravelCosts/llm_backbones_boxplot_BPI20PrepaidTravelCosts_FewShot-LoRA.png\n",
      "Saved LLM-backbone comparison for log=BPI20RequestForPayment, setting=ZeroShot to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20RequestForPayment/llm_backbones_boxplot_BPI20RequestForPayment_ZeroShot.png\n",
      "Saved LLM-backbone comparison for log=BPI20RequestForPayment, setting=LoRA to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20RequestForPayment/llm_backbones_boxplot_BPI20RequestForPayment_LoRA.png\n",
      "Saved LLM-backbone comparison for log=BPI20RequestForPayment, setting=FewShot-LoRA to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20RequestForPayment/llm_backbones_boxplot_BPI20RequestForPayment_FewShot-LoRA.png\n",
      "Saved LLM-backbone comparison for log=BPI20TravelPermitData, setting=ZeroShot to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20TravelPermitData/llm_backbones_boxplot_BPI20TravelPermitData_ZeroShot.png\n",
      "Saved LLM-backbone comparison for log=BPI20TravelPermitData, setting=LoRA to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20TravelPermitData/llm_backbones_boxplot_BPI20TravelPermitData_LoRA.png\n",
      "Saved LLM-backbone comparison for log=BPI20TravelPermitData, setting=FewShot-LoRA to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20TravelPermitData/llm_backbones_boxplot_BPI20TravelPermitData_FewShot-LoRA.png\n"
     ]
    }
   ],
   "source": [
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "\n",
    "multi_path = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/multi_task_results_r256_a512.csv\"\n",
    "llm = pd.read_csv(multi_path)\n",
    "\n",
    "# --- sanity checks ---\n",
    "required_cols = {\"log\", \"backbone\", \"Setting\"}\n",
    "missing = required_cols - set(llm.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns in {multi_path}: {sorted(missing)}\")\n",
    "\n",
    "# keep only LLM backbones\n",
    "llm = llm[llm[\"backbone\"].isin(LLM_BACKBONES)].copy()\n",
    "\n",
    "# keep only selected adaptation settings (this implicitly removes Freezing / FewShot-Freezing etc.)\n",
    "MAIN_SETTING_ORDER = [\"ZeroShot\", \"LoRA\", \"FewShot-LoRA\"]\n",
    "llm = llm[llm[\"Setting\"].isin(MAIN_SETTING_ORDER)].copy()\n",
    "\n",
    "# keep the same column name used downstream\n",
    "llm[\"Setting_main\"] = llm[\"Setting\"]\n",
    "\n",
    "# pretty model names (as before)\n",
    "BACKBONE_MAP_LLM = {\n",
    "    \"gpt2\":        \"GPT2\",\n",
    "    \"gptneo-1b3\":  \"GPT-Neo-1.3B\",\n",
    "    \"qwen25-05b\":  \"Qwen2.5-0.5B\",\n",
    "    \"llama32-1b\":  \"Llama3.2-1B\",\n",
    "    \"gemma-2-2b\":  \"Gemma-2-2B\",\n",
    "}\n",
    "llm[\"Backbone_pretty\"] = llm[\"backbone\"].map(BACKBONE_MAP_LLM).fillna(llm[\"backbone\"])\n",
    "\n",
    "plots_base_dir = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset\"\n",
    "\n",
    "PLOTS = [\n",
    "    (\"test_next_activity_acc\",            \"NA Acc.\"),\n",
    "    (\"test_next_remaining_time_loss\",     \"RT MSE\"),\n",
    "    (\"test_next_time_to_next_event_loss\", \"NT MSE\"),\n",
    "]\n",
    "\n",
    "def _parse_mean_std_cell(x) -> tuple[float, float]:\n",
    "    \"\"\"Parse 'mean ± std' or plain numeric cell.\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return np.nan, 0.0\n",
    "    if isinstance(x, (int, float, np.number)):\n",
    "        return float(x), 0.0\n",
    "\n",
    "    s = str(x).strip()\n",
    "    if \"±\" in s:\n",
    "        left, right = s.split(\"±\", 1)\n",
    "        try:\n",
    "            return float(left.strip()), float(right.strip())\n",
    "        except Exception:\n",
    "            return np.nan, 0.0\n",
    "\n",
    "    try:\n",
    "        return float(s), 0.0\n",
    "    except Exception:\n",
    "        return np.nan, 0.0\n",
    "\n",
    "def _get_mean_std(row_df: pd.DataFrame, metric: str) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Extract (mean, std) from a single-row aggregated table.\n",
    "    Supports:\n",
    "      - metric_mean / metric_std\n",
    "      - metric_mean_std formatted like '0.1234 ± 0.0056'\n",
    "      - fallback: metric (std=0)\n",
    "    \"\"\"\n",
    "    mean_col = f\"{metric}_mean\"\n",
    "    std_col = f\"{metric}_std\"\n",
    "    mean_std_col = f\"{metric}_mean_std\"\n",
    "\n",
    "    if mean_col in row_df.columns and pd.notna(row_df[mean_col].iloc[0]):\n",
    "        mean = float(row_df[mean_col].iloc[0])\n",
    "        std = float(row_df[std_col].iloc[0]) if std_col in row_df.columns and pd.notna(row_df[std_col].iloc[0]) else 0.0\n",
    "        return mean, std\n",
    "\n",
    "    if mean_std_col in row_df.columns:\n",
    "        return _parse_mean_std_cell(row_df[mean_std_col].iloc[0])\n",
    "\n",
    "    if metric in row_df.columns:\n",
    "        return _parse_mean_std_cell(row_df[metric].iloc[0])\n",
    "\n",
    "    raise KeyError(f\"Could not find '{mean_col}'/'{std_col}' or '{mean_std_col}' (or '{metric}') in {multi_path}.\")\n",
    "\n",
    "for log_name, df_log in llm.groupby(\"log\"):\n",
    "    log_dir = os.path.join(plots_base_dir, log_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    for setting in MAIN_SETTING_ORDER:\n",
    "        df_s = df_log[df_log[\"Setting_main\"] == setting].copy()\n",
    "        if df_s.empty:\n",
    "            continue\n",
    "\n",
    "        # backbone order (as before)\n",
    "        backbone_order_pretty = [\n",
    "            BACKBONE_MAP_LLM[b]\n",
    "            for b in LLM_BACKBONES\n",
    "            if b in df_s[\"backbone\"].unique()\n",
    "        ]\n",
    "        if not backbone_order_pretty:\n",
    "            continue\n",
    "\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(8, 9), sharex=True)\n",
    "\n",
    "        x = np.arange(len(backbone_order_pretty))\n",
    "\n",
    "        for ax, (metric, ylabel) in zip(axes, PLOTS):\n",
    "            means, stds = [], []\n",
    "\n",
    "            for pretty_name in backbone_order_pretty:\n",
    "                row = df_s[df_s[\"Backbone_pretty\"] == pretty_name]\n",
    "                if row.empty:\n",
    "                    means.append(np.nan)\n",
    "                    stds.append(0.0)\n",
    "                else:\n",
    "                    m, sd = _get_mean_std(row, metric)\n",
    "                    means.append(m)\n",
    "                    stds.append(sd)\n",
    "\n",
    "            keep = [i for i, v in enumerate(means) if pd.notna(v)]\n",
    "            if not keep:\n",
    "                ax.set_ylabel(ylabel)\n",
    "                continue\n",
    "\n",
    "            x_k = x[keep]\n",
    "            means_k = [means[i] for i in keep]\n",
    "            stds_k = [stds[i] for i in keep]\n",
    "            labels_k = [backbone_order_pretty[i] for i in keep]\n",
    "\n",
    "            ax.bar(x_k, means_k, yerr=stds_k, capsize=4)\n",
    "            ax.set_ylabel(ylabel)\n",
    "            ax.set_xticks(x_k)\n",
    "            ax.set_xticklabels(labels_k, rotation=45, ha=\"right\")\n",
    "            ax.set_ylim(bottom=0)\n",
    "\n",
    "        axes[-1].set_xlabel(\"LLM backbone\")\n",
    "        fig.suptitle(f\"{log_name} – {setting}\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # keep the SAME filename pattern as your original code\n",
    "        out_path = os.path.join(\n",
    "            log_dir,\n",
    "            f\"llm_backbones_boxplot_{log_name}_{setting}.png\"\n",
    "        )\n",
    "        plt.savefig(out_path, dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "        print(f\"Saved LLM-backbone comparison for log={log_name}, setting={setting} to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "701bdf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pooled boxplot for setting=ZeroShot to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/all_datasets/llm_backbones_boxplot_ALL_ZeroShot.png\n",
      "Saved pooled boxplot for setting=LoRA to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/all_datasets/llm_backbones_boxplot_ALL_LoRA.png\n",
      "Saved pooled boxplot for setting=FewShot-LoRA to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/all_datasets/llm_backbones_boxplot_ALL_FewShot-LoRA.png\n"
     ]
    }
   ],
   "source": [
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "KEEP_SETTINGS = [\"ZeroShot\", \"LoRA\", \"FewShot-LoRA\"]\n",
    "\n",
    "# --- (A) load selected configs (best HP per log/backbone/Setting) ---\n",
    "multi_path = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/multi_task_results_r256_a512.csv\"\n",
    "selected = pd.read_csv(multi_path)\n",
    "\n",
    "if \"Setting\" not in selected.columns:\n",
    "    raise ValueError(\"Column 'Setting' not found in multi_task_results_r256_a512.csv\")\n",
    "\n",
    "selected = selected[\n",
    "    selected[\"backbone\"].isin(LLM_BACKBONES) & selected[\"Setting\"].isin(KEEP_SETTINGS)\n",
    "].copy()\n",
    "\n",
    "# --- (B) take raw runs (df), compute Setting, and keep only relevant ones ---\n",
    "raw = df[df[\"backbone\"].isin(LLM_BACKBONES)].copy()\n",
    "raw[\"Setting\"] = raw.apply(map_setting, axis=1)\n",
    "raw = raw[raw[\"Setting\"].isin(KEEP_SETTINGS)].copy()\n",
    "\n",
    "# --- (C) join raw runs to selected configs on log/backbone/Setting + HP columns ---\n",
    "HP_CANDIDATES = [\n",
    "    \"lr\", \"batch_size\", \"epochs\",\n",
    "    \"embedding_size\", \"hidden_size\", \"strategy\",\n",
    "    \"weight_decay\", \"grad_clip\", \"n_layers\",\n",
    "    \"r\", \"lora_alpha\", \"few_shot_k\",\n",
    "]\n",
    "HP_COLS = [c for c in HP_CANDIDATES if c in raw.columns and c in selected.columns]\n",
    "\n",
    "join_cols = [\"log\", \"backbone\", \"Setting\"] + HP_COLS\n",
    "\n",
    "def _prep_join(df_in: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    out = df_in.copy()\n",
    "    for c in cols:\n",
    "        if c not in out.columns:\n",
    "            continue\n",
    "        if pd.api.types.is_numeric_dtype(out[c]) or c in HP_CANDIDATES:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\").round(10).fillna(-999999.0)\n",
    "        else:\n",
    "            out[c] = out[c].astype(str).fillna(\"__NA__\")\n",
    "    return out\n",
    "\n",
    "raw_j = _prep_join(raw, join_cols)\n",
    "sel_j = _prep_join(selected, join_cols)\n",
    "\n",
    "raw_selected = raw_j.merge(\n",
    "    sel_j[join_cols].drop_duplicates(),\n",
    "    on=join_cols,\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "if raw_selected.empty:\n",
    "    raise RuntimeError(\n",
    "        \"No raw runs matched the selected configs from multi_task_results_r256_a512.csv. \"\n",
    "        \"Check that the HP columns in the CSV match those in df.\"\n",
    "    )\n",
    "\n",
    "# --- (D) pretty names ---\n",
    "BACKBONE_MAP_LLM = {\n",
    "    \"gpt2\":         \"GPT2\",\n",
    "    \"gptneo-1b3\":   \"GPT-Neo-1.3B\",\n",
    "    \"qwen25-05b\":   \"Qwen2.5-0.5B\",\n",
    "    \"llama32-1b\":   \"Llama3.2-1B\",\n",
    "    \"gemma-2-2b\":   \"Gemma-2-2B\",\n",
    "}\n",
    "raw_selected[\"Backbone_pretty\"] = raw_selected[\"backbone\"].map(BACKBONE_MAP_LLM).fillna(raw_selected[\"backbone\"])\n",
    "backbone_order = [BACKBONE_MAP_LLM[b] for b in LLM_BACKBONES if b in raw_selected[\"backbone\"].unique()]\n",
    "\n",
    "# --- (E) plot config (raw per-seed metrics) ---\n",
    "PLOTS = [\n",
    "    (\"test_next_activity_acc\",            \"NA Acc.\"),\n",
    "    (\"test_next_remaining_time_loss\",     \"RT MSE\"),\n",
    "    (\"test_next_time_to_next_event_loss\", \"NT MSE\"),\n",
    "]\n",
    "\n",
    "plots_base_dir = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset\"\n",
    "out_dir = os.path.join(plots_base_dir, \"all_datasets\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "def annotate_medians_top(ax, data: pd.DataFrame, x_col: str, y_col: str, order: list[str], fmt: str = \"{:.4f}\"):\n",
    "    \"\"\"\n",
    "    Writes the median value per category in the *top margin* of the axes,\n",
    "    aligned with each box. Uses x in data coords, y in axes fraction.\n",
    "    \"\"\"\n",
    "    meds = data.groupby(x_col)[y_col].median()\n",
    "    for i, cat in enumerate(order):\n",
    "        if cat not in meds.index:\n",
    "            continue\n",
    "        val = meds.loc[cat]\n",
    "        if pd.isna(val):\n",
    "            continue\n",
    "        ax.text(\n",
    "            i, 1.02, fmt.format(float(val)),\n",
    "            transform=ax.get_xaxis_transform(),  # x=data, y=axes fraction\n",
    "            ha=\"center\", va=\"bottom\",\n",
    "            fontsize=9,\n",
    "            bbox=dict(facecolor=\"white\", edgecolor=\"none\", alpha=0.85, pad=1.5),\n",
    "            clip_on=False,\n",
    "        )\n",
    "\n",
    "# --- (F) pooled across datasets: boxplot per backbone (distribution over datasets x seeds) ---\n",
    "for setting in KEEP_SETTINGS:\n",
    "    df_s = raw_selected[raw_selected[\"Setting\"] == setting].copy()\n",
    "    if df_s.empty:\n",
    "        continue\n",
    "\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(8, 9), sharex=True)\n",
    "\n",
    "    for ax, (metric, ylabel) in zip(axes, PLOTS):\n",
    "        if metric not in df_s.columns:\n",
    "            raise ValueError(f\"Required raw metric column '{metric}' not found in df.\")\n",
    "\n",
    "        sns.boxplot(\n",
    "            data=df_s,\n",
    "            x=\"Backbone_pretty\",\n",
    "            y=metric,\n",
    "            order=backbone_order,\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "        # label medians in the top margin (readable, no overlap with whiskers/outliers)\n",
    "        annotate_medians_top(\n",
    "            ax=ax,\n",
    "            data=df_s,\n",
    "            x_col=\"Backbone_pretty\",\n",
    "            y_col=metric,\n",
    "            order=backbone_order,\n",
    "            fmt=\"{:.4f}\",\n",
    "        )\n",
    "\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.set_xticklabels(backbone_order, rotation=45, ha=\"right\")\n",
    "\n",
    "    axes[-1].set_xlabel(\"LLM backbone\")\n",
    "    fig.suptitle(f\"All datasets – {setting}\", fontsize=12)\n",
    "\n",
    "    # leave a bit of headroom for the top-margin labels + suptitle\n",
    "    plt.tight_layout(rect=(0, 0, 1, 0.96))\n",
    "\n",
    "    out_path = os.path.join(out_dir, f\"llm_backbones_boxplot_ALL_{setting}.png\")\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved pooled boxplot for setting={setting} to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "48a9ca0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CSV table to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/rq3_llm_tradeoff_table_selected_hp_raw_seeds.csv\n",
      "Saved pooled 15-row CSV table to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/rq3_llm_tradeoff_table_pooled15_across_datasets.csv\n"
     ]
    }
   ],
   "source": [
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "KEEP_SETTINGS = [\"ZeroShot\", \"LoRA\", \"FewShot-LoRA\"]\n",
    "\n",
    "multi_path = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/multi_task_results_r256_a512.csv\"\n",
    "selected = pd.read_csv(multi_path)\n",
    "\n",
    "if \"Setting\" not in selected.columns:\n",
    "    raise ValueError(\"Column 'Setting' not found in multi_task_results_r256_a512.csv\")\n",
    "\n",
    "selected = selected[\n",
    "    selected[\"backbone\"].isin(LLM_BACKBONES) & selected[\"Setting\"].isin(KEEP_SETTINGS)\n",
    "].copy()\n",
    "\n",
    "# --- (B) take raw runs (df), compute Setting, and keep only relevant ones ---\n",
    "raw = df[df[\"backbone\"].isin(LLM_BACKBONES)].copy()\n",
    "raw[\"Setting\"] = raw.apply(map_setting, axis=1)\n",
    "raw = raw[raw[\"Setting\"].isin(KEEP_SETTINGS)].copy()\n",
    "\n",
    "# --- (C) join raw runs to selected configs on log/backbone/Setting + HP columns ---\n",
    "HP_CANDIDATES = [\n",
    "    \"lr\", \"batch_size\", \"epochs\",\n",
    "    \"embedding_size\", \"hidden_size\", \"strategy\",\n",
    "    \"weight_decay\", \"grad_clip\", \"n_layers\",\n",
    "    \"r\", \"lora_alpha\", \"few_shot_k\",\n",
    "]\n",
    "HP_COLS = [c for c in HP_CANDIDATES if c in raw.columns and c in selected.columns]\n",
    "join_cols = [\"log\", \"backbone\", \"Setting\"] + HP_COLS\n",
    "\n",
    "def _prep_join(df_in: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    out = df_in.copy()\n",
    "    for c in cols:\n",
    "        if c not in out.columns:\n",
    "            continue\n",
    "        if pd.api.types.is_numeric_dtype(out[c]) or c in HP_CANDIDATES:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\").round(10).fillna(-999999.0)\n",
    "        else:\n",
    "            out[c] = out[c].astype(str).fillna(\"__NA__\")\n",
    "    return out\n",
    "\n",
    "raw_j = _prep_join(raw, join_cols)\n",
    "sel_j = _prep_join(selected, join_cols)\n",
    "\n",
    "raw_selected = raw_j.merge(\n",
    "    sel_j[join_cols].drop_duplicates(),\n",
    "    on=join_cols,\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "if raw_selected.empty:\n",
    "    raise RuntimeError(\n",
    "        \"No raw runs matched the selected configs from multi_task_results_r256_a512.csv. \"\n",
    "        \"Check that the HP columns in the CSV match those in df.\"\n",
    "    )\n",
    "\n",
    "# --- (D) pretty names ---\n",
    "BACKBONE_MAP_LLM = {\n",
    "    \"gpt2\":         \"GPT2\",\n",
    "    \"gptneo-1b3\":   \"GPT-Neo-1.3B\",\n",
    "    \"qwen25-05b\":   \"Qwen2.5-0.5B\",\n",
    "    \"llama32-1b\":   \"Llama3.2-1B\",\n",
    "    \"gemma-2-2b\":   \"Gemma-2-2B\",\n",
    "}\n",
    "raw_selected[\"Backbone_pretty\"] = raw_selected[\"backbone\"].map(BACKBONE_MAP_LLM).fillna(raw_selected[\"backbone\"])\n",
    "backbone_order = [BACKBONE_MAP_LLM[b] for b in LLM_BACKBONES if b in raw_selected[\"backbone\"].unique()]\n",
    "\n",
    "# metrics to aggregate\n",
    "METRICS = [m for (m, _) in PLOTS]\n",
    "\n",
    "# runtime column (try a few common candidates)\n",
    "RUNTIME_CANDIDATES = [\"_runtime\", \"runtime\", \"train_runtime\", \"_runtime_sec\", \"_runtime_seconds\"]\n",
    "runtime_col = next((c for c in RUNTIME_CANDIDATES if c in raw_selected.columns), None)\n",
    "\n",
    "# make sure metrics are numeric\n",
    "for m in METRICS:\n",
    "    raw_selected[m] = pd.to_numeric(raw_selected[m], errors=\"coerce\")\n",
    "\n",
    "if runtime_col is not None:\n",
    "    raw_selected[runtime_col] = pd.to_numeric(raw_selected[runtime_col], errors=\"coerce\")\n",
    "\n",
    "# aggregate per dataset/log, backbone, setting (this produces ~ 5*5*3 = 75 rows)\n",
    "group_cols = [\"log\", \"backbone\", \"Setting\"]\n",
    "agg_dict = {}\n",
    "\n",
    "for m in METRICS:\n",
    "    agg_dict[m + \"_mean\"] = (m, \"mean\")\n",
    "    agg_dict[m + \"_std\"]  = (m, \"std\")\n",
    "\n",
    "if runtime_col is not None:\n",
    "    agg_dict[\"runtime_s_mean\"] = (runtime_col, \"mean\")\n",
    "    agg_dict[\"runtime_s_std\"]  = (runtime_col, \"std\")\n",
    "\n",
    "table = raw_selected.groupby(group_cols, dropna=False).agg(**agg_dict).reset_index()\n",
    "\n",
    "# runtime in hours + formatted column\n",
    "if runtime_col is not None:\n",
    "    table[\"runtime_h_mean\"] = table[\"runtime_s_mean\"] / 3600.0\n",
    "    table[\"runtime_h_std\"]  = table[\"runtime_s_std\"] / 3600.0\n",
    "\n",
    "# pretty backbone + short mode labels\n",
    "MODE_MAP = {\"ZeroShot\": \"ZS\", \"FewShot-LoRA\": \"FS\", \"LoRA\": \"LoRA\"}\n",
    "table[\"Mode\"] = table[\"Setting\"].map(MODE_MAP).fillna(table[\"Setting\"])\n",
    "table[\"Backbone_pretty\"] = table[\"backbone\"].map(BACKBONE_MAP_LLM).fillna(table[\"backbone\"])\n",
    "\n",
    "# add mean±std string columns (nice for LaTeX export)\n",
    "def _fmt_pm(mu, sd, d=4):\n",
    "    if pd.isna(mu):\n",
    "        return \"\"\n",
    "    if pd.isna(sd):\n",
    "        sd = 0.0\n",
    "    return f\"{mu:.{d}f} ± {sd:.{d}f}\"\n",
    "\n",
    "table[\"NA Acc.\"] = table.apply(lambda r: _fmt_pm(r[\"test_next_activity_acc_mean\"], r[\"test_next_activity_acc_std\"], d=4), axis=1)\n",
    "table[\"RT MSE\"]  = table.apply(lambda r: _fmt_pm(r[\"test_next_remaining_time_loss_mean\"], r[\"test_next_remaining_time_loss_std\"], d=4), axis=1)\n",
    "table[\"NT MSE\"]  = table.apply(lambda r: _fmt_pm(r[\"test_next_time_to_next_event_loss_mean\"], r[\"test_next_time_to_next_event_loss_std\"], d=4), axis=1)\n",
    "\n",
    "if runtime_col is not None:\n",
    "    table[\"Runtime (h)\"] = table.apply(lambda r: _fmt_pm(r[\"runtime_h_mean\"], r[\"runtime_h_std\"], d=5), axis=1)\n",
    "else:\n",
    "    table[\"Runtime (h)\"] = \"\"\n",
    "\n",
    "# order columns for the final table-like CSV\n",
    "final_cols = [\n",
    "    \"log\", \"Backbone_pretty\", \"Mode\",\n",
    "    \"NA Acc.\", \"RT MSE\", \"NT MSE\", \"Runtime (h)\",\n",
    "    # keep numeric columns too (useful for sorting later)\n",
    "    \"test_next_activity_acc_mean\", \"test_next_activity_acc_std\",\n",
    "    \"test_next_remaining_time_loss_mean\", \"test_next_remaining_time_loss_std\",\n",
    "    \"test_next_time_to_next_event_loss_mean\", \"test_next_time_to_next_event_loss_std\",\n",
    "]\n",
    "if runtime_col is not None:\n",
    "    final_cols += [\"runtime_h_mean\", \"runtime_h_std\"]\n",
    "\n",
    "final_cols = [c for c in final_cols if c in table.columns]\n",
    "table_out = table[final_cols].copy()\n",
    "\n",
    "out_csv_dir = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv\"\n",
    "os.makedirs(out_csv_dir, exist_ok=True)\n",
    "out_csv_path = os.path.join(out_csv_dir, \"rq3_llm_tradeoff_table_selected_hp_raw_seeds.csv\")\n",
    "table_out.to_csv(out_csv_path, index=False)\n",
    "\n",
    "print(\"Saved CSV table to:\", out_csv_path)\n",
    "\n",
    "# ===================== (H) 15-row TABLE POOLED ACROSS ALL DATASETS =====================\n",
    "# Step 1: per-dataset (log) aggregation over seeds\n",
    "group_log = [\"log\", \"backbone\", \"Setting\"]\n",
    "\n",
    "per_log = raw_selected.groupby(group_log, dropna=False).agg(\n",
    "    test_next_activity_acc_mean=(\"test_next_activity_acc\", \"mean\"),\n",
    "    test_next_activity_acc_std =(\"test_next_activity_acc\", \"std\"),\n",
    "    test_next_remaining_time_loss_mean=(\"test_next_remaining_time_loss\", \"mean\"),\n",
    "    test_next_remaining_time_loss_std =(\"test_next_remaining_time_loss\", \"std\"),\n",
    "    test_next_time_to_next_event_loss_mean=(\"test_next_time_to_next_event_loss\", \"mean\"),\n",
    "    test_next_time_to_next_event_loss_std =(\"test_next_time_to_next_event_loss\", \"std\"),\n",
    ").reset_index()\n",
    "\n",
    "# runtime (optional)\n",
    "if runtime_col is not None:\n",
    "    per_log = per_log.merge(\n",
    "        raw_selected.groupby(group_log, dropna=False).agg(\n",
    "            runtime_s_mean=(runtime_col, \"mean\"),\n",
    "            runtime_s_std =(runtime_col, \"std\"),\n",
    "        ).reset_index(),\n",
    "        on=group_log,\n",
    "        how=\"left\",\n",
    "    )\n",
    "    per_log[\"runtime_h_mean\"] = per_log[\"runtime_s_mean\"] / 3600.0\n",
    "    per_log[\"runtime_h_std\"]  = per_log[\"runtime_s_std\"]  / 3600.0\n",
    "\n",
    "# Step 2: pooled across datasets/logs (each dataset contributes ONE point)\n",
    "group_pooled = [\"backbone\", \"Setting\"]\n",
    "\n",
    "pooled15 = per_log.groupby(group_pooled, dropna=False).agg(\n",
    "    # mean across datasets\n",
    "    test_next_activity_acc_mean=(\"test_next_activity_acc_mean\", \"mean\"),\n",
    "    test_next_remaining_time_loss_mean=(\"test_next_remaining_time_loss_mean\", \"mean\"),\n",
    "    test_next_time_to_next_event_loss_mean=(\"test_next_time_to_next_event_loss_mean\", \"mean\"),\n",
    "\n",
    "    # std across datasets (variation between datasets)\n",
    "    test_next_activity_acc_std=(\"test_next_activity_acc_mean\", \"std\"),\n",
    "    test_next_remaining_time_loss_std=(\"test_next_remaining_time_loss_mean\", \"std\"),\n",
    "    test_next_time_to_next_event_loss_std=(\"test_next_time_to_next_event_loss_mean\", \"std\"),\n",
    ").reset_index()\n",
    "\n",
    "if runtime_col is not None:\n",
    "    pooled15 = pooled15.merge(\n",
    "        per_log.groupby(group_pooled, dropna=False).agg(\n",
    "            runtime_h_mean=(\"runtime_h_mean\", \"mean\"),\n",
    "            runtime_h_std =(\"runtime_h_mean\", \"std\"),  # std across datasets\n",
    "        ).reset_index(),\n",
    "        on=group_pooled,\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "# pretty columns + formatted mean±std strings\n",
    "MODE_MAP = {\"ZeroShot\": \"ZS\", \"FewShot-LoRA\": \"FS\", \"LoRA\": \"LoRA\"}\n",
    "pooled15[\"Mode\"] = pooled15[\"Setting\"].map(MODE_MAP).fillna(pooled15[\"Setting\"])\n",
    "pooled15[\"Backbone_pretty\"] = pooled15[\"backbone\"].map(BACKBONE_MAP_LLM).fillna(pooled15[\"backbone\"])\n",
    "\n",
    "def _fmt_pm(mu, sd, d=4):\n",
    "    if pd.isna(mu):\n",
    "        return \"\"\n",
    "    if pd.isna(sd):\n",
    "        sd = 0.0\n",
    "    return f\"{mu:.{d}f} ± {sd:.{d}f}\"\n",
    "\n",
    "pooled15[\"NA Acc.\"] = pooled15.apply(lambda r: _fmt_pm(r[\"test_next_activity_acc_mean\"], r[\"test_next_activity_acc_std\"], d=4), axis=1)\n",
    "pooled15[\"RT MSE\"]  = pooled15.apply(lambda r: _fmt_pm(r[\"test_next_remaining_time_loss_mean\"], r[\"test_next_remaining_time_loss_std\"], d=4), axis=1)\n",
    "pooled15[\"NT MSE\"]  = pooled15.apply(lambda r: _fmt_pm(r[\"test_next_time_to_next_event_loss_mean\"], r[\"test_next_time_to_next_event_loss_std\"], d=4), axis=1)\n",
    "\n",
    "if runtime_col is not None:\n",
    "    pooled15[\"Runtime (h)\"] = pooled15.apply(lambda r: _fmt_pm(r[\"runtime_h_mean\"], r[\"runtime_h_std\"], d=5), axis=1)\n",
    "else:\n",
    "    pooled15[\"Runtime (h)\"] = \"\"\n",
    "\n",
    "# final column order\n",
    "final_cols_15 = [\n",
    "    \"Backbone_pretty\", \"Mode\",\n",
    "    \"NA Acc.\", \"RT MSE\", \"NT MSE\", \"Runtime (h)\",\n",
    "    \"test_next_activity_acc_mean\", \"test_next_activity_acc_std\",\n",
    "    \"test_next_remaining_time_loss_mean\", \"test_next_remaining_time_loss_std\",\n",
    "    \"test_next_time_to_next_event_loss_mean\", \"test_next_time_to_next_event_loss_std\",\n",
    "]\n",
    "if runtime_col is not None:\n",
    "    final_cols_15 += [\"runtime_h_mean\", \"runtime_h_std\"]\n",
    "\n",
    "final_cols_15 = [c for c in final_cols_15 if c in pooled15.columns]\n",
    "pooled15_out = pooled15[final_cols_15].copy()\n",
    "\n",
    "# save\n",
    "out_csv_dir = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv\"\n",
    "os.makedirs(out_csv_dir, exist_ok=True)\n",
    "out_csv_path_15 = os.path.join(out_csv_dir, \"rq3_llm_tradeoff_table_pooled15_across_datasets.csv\")\n",
    "pooled15_out.to_csv(out_csv_path_15, index=False)\n",
    "\n",
    "print(\"Saved pooled 15-row CSV table to:\", out_csv_path_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8141b2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved selected-methods boxplot for log=BPI12, backbone=gemma-2-2b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI12/llm_methods_boxplot_selected_BPI12_gemma-2-2b.png\n",
      "Saved selected-methods boxplot for log=BPI12, backbone=gpt2 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI12/llm_methods_boxplot_selected_BPI12_gpt2.png\n",
      "Saved selected-methods boxplot for log=BPI12, backbone=gptneo-1b3 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI12/llm_methods_boxplot_selected_BPI12_gptneo-1b3.png\n",
      "Saved selected-methods boxplot for log=BPI12, backbone=llama32-1b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI12/llm_methods_boxplot_selected_BPI12_llama32-1b.png\n",
      "Saved selected-methods boxplot for log=BPI12, backbone=qwen25-05b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI12/llm_methods_boxplot_selected_BPI12_qwen25-05b.png\n",
      "Saved selected-methods boxplot for log=BPI17, backbone=gemma-2-2b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI17/llm_methods_boxplot_selected_BPI17_gemma-2-2b.png\n",
      "Saved selected-methods boxplot for log=BPI17, backbone=gpt2 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI17/llm_methods_boxplot_selected_BPI17_gpt2.png\n",
      "Saved selected-methods boxplot for log=BPI17, backbone=gptneo-1b3 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI17/llm_methods_boxplot_selected_BPI17_gptneo-1b3.png\n",
      "Saved selected-methods boxplot for log=BPI17, backbone=llama32-1b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI17/llm_methods_boxplot_selected_BPI17_llama32-1b.png\n",
      "Saved selected-methods boxplot for log=BPI17, backbone=qwen25-05b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI17/llm_methods_boxplot_selected_BPI17_qwen25-05b.png\n",
      "Saved selected-methods boxplot for log=BPI20PrepaidTravelCosts, backbone=gemma-2-2b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20PrepaidTravelCosts/llm_methods_boxplot_selected_BPI20PrepaidTravelCosts_gemma-2-2b.png\n",
      "Saved selected-methods boxplot for log=BPI20PrepaidTravelCosts, backbone=gpt2 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20PrepaidTravelCosts/llm_methods_boxplot_selected_BPI20PrepaidTravelCosts_gpt2.png\n",
      "Saved selected-methods boxplot for log=BPI20PrepaidTravelCosts, backbone=gptneo-1b3 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20PrepaidTravelCosts/llm_methods_boxplot_selected_BPI20PrepaidTravelCosts_gptneo-1b3.png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/ceph/lfertig/miniconda3/envs/llm-peft-ppm/lib/python3.12/site-packages/PIL/ImageFile.py:648\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 648\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m()\n\u001b[1;32m    649\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 115\u001b[0m\n\u001b[1;32m    112\u001b[0m plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[1;32m    114\u001b[0m out_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(log_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_methods_boxplot_selected_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackbone\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 115\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose(fig)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved selected-methods boxplot for log=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, backbone=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackbone\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/ceph/lfertig/miniconda3/envs/llm-peft-ppm/lib/python3.12/site-packages/matplotlib/pyplot.py:1250\u001b[0m, in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1247\u001b[0m fig \u001b[38;5;241m=\u001b[39m gcf()\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;66;03m# savefig default implementation has no return, so mypy is unhappy\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;66;03m# presumably this is here because subclasses can return?\u001b[39;00m\n\u001b[0;32m-> 1250\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[func-returns-value]\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mdraw_idle()  \u001b[38;5;66;03m# Need this if 'transparent=True', to reset colors.\u001b[39;00m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m/ceph/lfertig/miniconda3/envs/llm-peft-ppm/lib/python3.12/site-packages/matplotlib/figure.py:3490\u001b[0m, in \u001b[0;36mFigure.savefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3488\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes:\n\u001b[1;32m   3489\u001b[0m         _recursively_make_axes_transparent(stack, ax)\n\u001b[0;32m-> 3490\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ceph/lfertig/miniconda3/envs/llm-peft-ppm/lib/python3.12/site-packages/matplotlib/backend_bases.py:2186\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2182\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2183\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2184\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2185\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[0;32m-> 2186\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2187\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2188\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2189\u001b[0m \u001b[43m            \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2190\u001b[0m \u001b[43m            \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2192\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2193\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[0;32m/ceph/lfertig/miniconda3/envs/llm-peft-ppm/lib/python3.12/site-packages/matplotlib/backend_bases.py:2042\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m   2041\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[0;32m-> 2042\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2043\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2044\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[1;32m   2045\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
      "File \u001b[0;32m/ceph/lfertig/miniconda3/envs/llm-peft-ppm/lib/python3.12/site-packages/matplotlib/backends/backend_agg.py:481\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    435\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 481\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ceph/lfertig/miniconda3/envs/llm-peft-ppm/lib/python3.12/site-packages/matplotlib/backends/backend_agg.py:430\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;124;03mDraw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;124;03m*pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    429\u001b[0m FigureCanvasAgg\u001b[38;5;241m.\u001b[39mdraw(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 430\u001b[0m \u001b[43mmpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimsave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_rgba\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ceph/lfertig/miniconda3/envs/llm-peft-ppm/lib/python3.12/site-packages/matplotlib/image.py:1657\u001b[0m, in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1656\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (dpi, dpi))\n\u001b[0;32m-> 1657\u001b[0m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ceph/lfertig/miniconda3/envs/llm-peft-ppm/lib/python3.12/site-packages/PIL/Image.py:2571\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2568\u001b[0m     fp \u001b[38;5;241m=\u001b[39m cast(IO[\u001b[38;5;28mbytes\u001b[39m], fp)\n\u001b[1;32m   2570\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2571\u001b[0m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2572\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   2573\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "File \u001b[0;32m/ceph/lfertig/miniconda3/envs/llm-peft-ppm/lib/python3.12/site-packages/PIL/PngImagePlugin.py:1497\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1493\u001b[0m     single_im \u001b[38;5;241m=\u001b[39m _write_multiple_frames(\n\u001b[1;32m   1494\u001b[0m         im, fp, chunk, mode, rawmode, default_image, append_images\n\u001b[1;32m   1495\u001b[0m     )\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m single_im:\n\u001b[0;32m-> 1497\u001b[0m     \u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m        \u001b[49m\u001b[43msingle_im\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIO\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_idat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Tile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msingle_im\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[1;32m   1504\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m info_chunk \u001b[38;5;129;01min\u001b[39;00m info\u001b[38;5;241m.\u001b[39mchunks:\n",
      "File \u001b[0;32m/ceph/lfertig/miniconda3/envs/llm-peft-ppm/lib/python3.12/site-packages/PIL/ImageFile.py:652\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    650\u001b[0m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 652\u001b[0m     \u001b[43m_encode_tile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflush\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    654\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "File \u001b[0;32m/ceph/lfertig/miniconda3/envs/llm-peft-ppm/lib/python3.12/site-packages/PIL/ImageFile.py:678\u001b[0m, in \u001b[0;36m_encode_tile\u001b[0;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc:\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;66;03m# compress to Python file-compatible object\u001b[39;00m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 678\u001b[0m         errcode, data \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    679\u001b[0m         fp\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[1;32m    680\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m errcode:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x900 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "\n",
    "multi_path = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/multi_task_results_r256_a512.csv\"\n",
    "llm = pd.read_csv(multi_path)\n",
    "\n",
    "if \"Setting\" not in llm.columns:\n",
    "    raise ValueError(\"Column 'Setting' not found in multi_task_results_r256_a512.csv\")\n",
    "\n",
    "# keep only LLM backbones + selected settings\n",
    "llm = llm[llm[\"backbone\"].isin(LLM_BACKBONES)].copy()\n",
    "\n",
    "KEEP_SETTINGS = [\"ZeroShot\", \"LoRA\", \"FewShot-LoRA\"]\n",
    "llm = llm[llm[\"Setting\"].isin(KEEP_SETTINGS)].copy()\n",
    "\n",
    "llm[\"Setting_main\"] = llm[\"Setting\"]\n",
    "\n",
    "plots_base_dir = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset\"\n",
    "\n",
    "SETTING_ORDER_FULL = [\n",
    "    \"ZeroShot\",\n",
    "    \"FewShot-LoRA\",\n",
    "    \"LoRA\",\n",
    "]\n",
    "\n",
    "SETTING_ORDER_MAIN = [\n",
    "    \"ZeroShot\",\n",
    "    \"FewShot-LoRA\",\n",
    "    \"LoRA\",\n",
    "]\n",
    "\n",
    "PLOTS = [\n",
    "    (\"test_next_activity_acc\",            \"NA Acc.\"),\n",
    "    (\"test_next_remaining_time_loss\",     \"RT MSE\"),\n",
    "    (\"test_next_time_to_next_event_loss\", \"NT MSE\"),\n",
    "]\n",
    "\n",
    "def _get_mean_std(df_one: pd.DataFrame, metric: str) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Extract (mean, std) for a metric from an aggregated table row.\n",
    "    Supports either:\n",
    "      - columns: f\"{metric}_mean\", f\"{metric}_std\"\n",
    "      - or column: f\"{metric}_mean_std\" formatted like \"0.1234 ± 0.0056\"\n",
    "      - or (fallback) raw metric column (std=0)\n",
    "    \"\"\"\n",
    "    mean_col = f\"{metric}_mean\"\n",
    "    std_col = f\"{metric}_std\"\n",
    "    mean_std_col = f\"{metric}_mean_std\"\n",
    "\n",
    "    if mean_col in df_one.columns:\n",
    "        mean = float(df_one[mean_col].iloc[0])\n",
    "        std = float(df_one[std_col].iloc[0]) if std_col in df_one.columns and pd.notna(df_one[std_col].iloc[0]) else 0.0\n",
    "        return mean, std\n",
    "\n",
    "    if mean_std_col in df_one.columns:\n",
    "        s = str(df_one[mean_std_col].iloc[0])\n",
    "        if \"±\" in s:\n",
    "            left, right = s.split(\"±\", 1)\n",
    "            return float(left.strip()), float(right.strip())\n",
    "        # if it's just a number string\n",
    "        return float(s.strip()), 0.0\n",
    "\n",
    "    if metric in df_one.columns:\n",
    "        return float(df_one[metric].iloc[0]), 0.0\n",
    "\n",
    "    raise KeyError(\n",
    "        f\"Could not find '{mean_col}'/'{std_col}' or '{mean_std_col}' (or '{metric}') in the aggregated CSV.\"\n",
    "    )\n",
    "\n",
    "for log_name, df_log in llm.groupby(\"log\"):\n",
    "    log_dir = os.path.join(plots_base_dir, log_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    for backbone, df_b in df_log.groupby(\"backbone\"):\n",
    "\n",
    "        settings_full = [s for s in SETTING_ORDER_FULL if s in df_b[\"Setting\"].unique()]\n",
    "        if not settings_full:\n",
    "            continue\n",
    "\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(8, 9), sharex=True)\n",
    "\n",
    "        x_labels = settings_full\n",
    "        x = np.arange(len(x_labels))\n",
    "\n",
    "        for ax, (metric, ylabel) in zip(axes, PLOTS):\n",
    "            means = []\n",
    "            stds = []\n",
    "\n",
    "            for s in x_labels:\n",
    "                row = df_b[df_b[\"Setting\"] == s]\n",
    "                if row.empty:\n",
    "                    means.append(np.nan)\n",
    "                    stds.append(0.0)\n",
    "                else:\n",
    "                    m, sd = _get_mean_std(row, metric)\n",
    "                    means.append(m)\n",
    "                    stds.append(sd)\n",
    "\n",
    "            # drop missing entries (should rarely happen if your table is complete)\n",
    "            keep = [i for i, v in enumerate(means) if pd.notna(v)]\n",
    "            x_k = x[keep]\n",
    "            means_k = [means[i] for i in keep]\n",
    "            stds_k = [stds[i] for i in keep]\n",
    "            labels_k = [x_labels[i] for i in keep]\n",
    "\n",
    "            ax.bar(x_k, means_k, yerr=stds_k, capsize=4)\n",
    "            ax.set_ylabel(ylabel)\n",
    "            ax.set_xticks(x_k)\n",
    "            ax.set_xticklabels(labels_k, rotation=45, ha=\"right\")\n",
    "\n",
    "        axes[-1].set_xlabel(\"LLM adaptation setting\")\n",
    "        fig.suptitle(f\"{log_name} – {backbone}\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        out_path = os.path.join(log_dir, f\"llm_methods_boxplot_selected_{log_name}_{backbone}.png\")\n",
    "        plt.savefig(out_path, dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "        print(f\"Saved selected-methods boxplot for log={log_name}, backbone={backbone} to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f93e7ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Freezing-only boxplot for log=BPI12, backbone=gemma-2-2b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI12/llm_methods_boxplot_freezing_BPI12_gemma-2-2b.png\n",
      "Saved Freezing-only boxplot for log=BPI12, backbone=gpt2 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI12/llm_methods_boxplot_freezing_BPI12_gpt2.png\n",
      "Saved Freezing-only boxplot for log=BPI12, backbone=gptneo-1b3 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI12/llm_methods_boxplot_freezing_BPI12_gptneo-1b3.png\n",
      "Saved Freezing-only boxplot for log=BPI12, backbone=llama32-1b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI12/llm_methods_boxplot_freezing_BPI12_llama32-1b.png\n",
      "Saved Freezing-only boxplot for log=BPI12, backbone=qwen25-05b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI12/llm_methods_boxplot_freezing_BPI12_qwen25-05b.png\n",
      "Saved Freezing-only boxplot for log=BPI17, backbone=gemma-2-2b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI17/llm_methods_boxplot_freezing_BPI17_gemma-2-2b.png\n",
      "Saved Freezing-only boxplot for log=BPI17, backbone=gpt2 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI17/llm_methods_boxplot_freezing_BPI17_gpt2.png\n",
      "Saved Freezing-only boxplot for log=BPI17, backbone=gptneo-1b3 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI17/llm_methods_boxplot_freezing_BPI17_gptneo-1b3.png\n",
      "Saved Freezing-only boxplot for log=BPI17, backbone=llama32-1b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI17/llm_methods_boxplot_freezing_BPI17_llama32-1b.png\n",
      "Saved Freezing-only boxplot for log=BPI17, backbone=qwen25-05b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI17/llm_methods_boxplot_freezing_BPI17_qwen25-05b.png\n",
      "Saved Freezing-only boxplot for log=BPI20PrepaidTravelCosts, backbone=gemma-2-2b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20PrepaidTravelCosts/llm_methods_boxplot_freezing_BPI20PrepaidTravelCosts_gemma-2-2b.png\n",
      "Saved Freezing-only boxplot for log=BPI20PrepaidTravelCosts, backbone=gpt2 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20PrepaidTravelCosts/llm_methods_boxplot_freezing_BPI20PrepaidTravelCosts_gpt2.png\n",
      "Saved Freezing-only boxplot for log=BPI20PrepaidTravelCosts, backbone=gptneo-1b3 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20PrepaidTravelCosts/llm_methods_boxplot_freezing_BPI20PrepaidTravelCosts_gptneo-1b3.png\n",
      "Saved Freezing-only boxplot for log=BPI20PrepaidTravelCosts, backbone=llama32-1b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20PrepaidTravelCosts/llm_methods_boxplot_freezing_BPI20PrepaidTravelCosts_llama32-1b.png\n",
      "Saved Freezing-only boxplot for log=BPI20PrepaidTravelCosts, backbone=qwen25-05b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20PrepaidTravelCosts/llm_methods_boxplot_freezing_BPI20PrepaidTravelCosts_qwen25-05b.png\n",
      "Saved Freezing-only boxplot for log=BPI20RequestForPayment, backbone=gemma-2-2b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20RequestForPayment/llm_methods_boxplot_freezing_BPI20RequestForPayment_gemma-2-2b.png\n",
      "Saved Freezing-only boxplot for log=BPI20RequestForPayment, backbone=gpt2 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20RequestForPayment/llm_methods_boxplot_freezing_BPI20RequestForPayment_gpt2.png\n",
      "Saved Freezing-only boxplot for log=BPI20RequestForPayment, backbone=gptneo-1b3 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20RequestForPayment/llm_methods_boxplot_freezing_BPI20RequestForPayment_gptneo-1b3.png\n",
      "Saved Freezing-only boxplot for log=BPI20RequestForPayment, backbone=llama32-1b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20RequestForPayment/llm_methods_boxplot_freezing_BPI20RequestForPayment_llama32-1b.png\n",
      "Saved Freezing-only boxplot for log=BPI20RequestForPayment, backbone=qwen25-05b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20RequestForPayment/llm_methods_boxplot_freezing_BPI20RequestForPayment_qwen25-05b.png\n",
      "Saved Freezing-only boxplot for log=BPI20TravelPermitData, backbone=gemma-2-2b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20TravelPermitData/llm_methods_boxplot_freezing_BPI20TravelPermitData_gemma-2-2b.png\n",
      "Saved Freezing-only boxplot for log=BPI20TravelPermitData, backbone=gpt2 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20TravelPermitData/llm_methods_boxplot_freezing_BPI20TravelPermitData_gpt2.png\n",
      "Saved Freezing-only boxplot for log=BPI20TravelPermitData, backbone=gptneo-1b3 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20TravelPermitData/llm_methods_boxplot_freezing_BPI20TravelPermitData_gptneo-1b3.png\n",
      "Saved Freezing-only boxplot for log=BPI20TravelPermitData, backbone=llama32-1b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20TravelPermitData/llm_methods_boxplot_freezing_BPI20TravelPermitData_llama32-1b.png\n",
      "Saved Freezing-only boxplot for log=BPI20TravelPermitData, backbone=qwen25-05b to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20TravelPermitData/llm_methods_boxplot_freezing_BPI20TravelPermitData_qwen25-05b.png\n"
     ]
    }
   ],
   "source": [
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "\n",
    "llm = df[df[\"backbone\"].isin(LLM_BACKBONES)].copy()\n",
    "llm[\"Setting\"] = llm.apply(map_setting, axis=1)\n",
    "\n",
    "# Keep only freezing variants\n",
    "llm_freezing = llm[\n",
    "    (llm[\"Setting\"] == \"Freezing\") | (llm[\"Setting\"].astype(str).str.startswith(\"Freezing-\"))\n",
    "].copy()\n",
    "llm_freezing = llm_freezing[llm_freezing[\"Setting\"] != \"FewShot-Freezing\"].copy()\n",
    "\n",
    "plots_base_dir = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset\"\n",
    "\n",
    "# Order as in your example plot (will be filtered to what exists per run)\n",
    "FREEZING_ORDER = [\n",
    "    \"Freezing\",\n",
    "    \"Freezing-[-1]\",\n",
    "    \"Freezing-[0]\",\n",
    "    \"Freezing-[0, 1]\",\n",
    "    \"Freezing-[-1, -2]\",\n",
    "]\n",
    "\n",
    "PLOTS = [\n",
    "    (\"test_next_activity_acc\",            \"NA Acc.\"),\n",
    "    (\"test_next_remaining_time_loss\",     \"RT MSE\"),\n",
    "    (\"test_next_time_to_next_event_loss\", \"NT MSE\"),\n",
    "]\n",
    "\n",
    "for log_name, df_log in llm_freezing.groupby(\"log\"):\n",
    "    log_dir = os.path.join(plots_base_dir, log_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    for backbone, df_b in df_log.groupby(\"backbone\"):\n",
    "        # keep only the freezing settings that are present for this (log, backbone)\n",
    "        settings = [s for s in FREEZING_ORDER if s in df_b[\"Setting\"].unique()]\n",
    "        # fallback: if something unexpected appears, include it (stable alphabetical after the known ones)\n",
    "        extra = sorted([s for s in df_b[\"Setting\"].unique() if s not in settings])\n",
    "        settings = settings + extra\n",
    "\n",
    "        if not settings:\n",
    "            continue\n",
    "\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(8, 9), sharex=True)\n",
    "\n",
    "        for ax, (metric, ylabel) in zip(axes, PLOTS):\n",
    "            sns.boxplot(\n",
    "                data=df_b,\n",
    "                x=\"Setting\",\n",
    "                y=metric,\n",
    "                order=settings,\n",
    "                ax=ax,\n",
    "            )\n",
    "            ax.set_ylabel(ylabel)\n",
    "            ax.set_xticks(range(len(settings)))\n",
    "            ax.set_xticklabels(settings, rotation=45, ha=\"right\")\n",
    "\n",
    "        axes[-1].set_xlabel(\"Freezing configuration\")\n",
    "\n",
    "        fig.suptitle(f\"{log_name} – {backbone} (Freezing variants)\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        out_path = os.path.join(\n",
    "            log_dir,\n",
    "            f\"llm_methods_boxplot_freezing_{log_name}_{backbone}.png\"\n",
    "        )\n",
    "        plt.savefig(out_path, dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "        print(f\"Saved Freezing-only boxplot for log={log_name}, backbone={backbone} to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4bda212b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss curves shape (LoRA best runs): (250, 7)\n"
     ]
    }
   ],
   "source": [
    "def fetch_single(\n",
    "    wandb_id: str,\n",
    "    targets=[\"na\", \"rt\", \"nt\"],\n",
    "    project_name: str | None = None,\n",
    "    entity: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Holt Verlaufskurven (pro Epoch) für einen einzelnen W&B-Run.\n",
    "    Gibt (na_acc, na_loss, rt_loss, nt_loss) als Listen zurück.\n",
    "    Fehlende Targets -> entsprechende Liste = None.\n",
    "    \"\"\"\n",
    "    if isinstance(targets, str):\n",
    "        targets = [targets]\n",
    "\n",
    "    if project_name is None:\n",
    "        raise ValueError(\"fetch_single requires an explicit project_name.\")\n",
    "\n",
    "    if entity is None:\n",
    "        entity = os.environ.get(\"ENTITY\")\n",
    "        if entity is None:\n",
    "            raise ValueError(\"ENTITY not set and no entity passed to fetch_single().\")\n",
    "\n",
    "    api = wandb.Api()\n",
    "    run = api.run(f\"{entity}/{project_name}/{wandb_id}\")\n",
    "    history = list(run.scan_history())\n",
    "\n",
    "    na_acc, na_loss, rt_loss, nt_loss = None, None, None, None\n",
    "\n",
    "    if \"rt\" in targets:\n",
    "        rt_loss = [\n",
    "            row[\"test_next_remaining_time_loss\"]\n",
    "            for row in history\n",
    "            if \"test_next_remaining_time_loss\" in row\n",
    "        ]\n",
    "\n",
    "    if \"na\" in targets:\n",
    "        na_loss = [\n",
    "            row[\"test_next_activity_loss\"]\n",
    "            for row in history\n",
    "            if \"test_next_activity_loss\" in row\n",
    "        ]\n",
    "        na_acc = [\n",
    "            row[\"test_next_activity_acc\"]\n",
    "            for row in history\n",
    "            if \"test_next_activity_acc\" in row\n",
    "        ]\n",
    "\n",
    "    if \"nt\" in targets:\n",
    "        nt_loss = [\n",
    "            row[\"test_next_time_to_next_event_loss\"]\n",
    "            for row in history\n",
    "            if \"test_next_time_to_next_event_loss\" in row\n",
    "        ]\n",
    "\n",
    "    return na_acc, na_loss, rt_loss, nt_loss\n",
    "\n",
    "\n",
    "# Pfad für Loss-Curves-CSV\n",
    "loss_csv_path = os.path.join(output_dir_csv, \"loss_curves_multitask_lora_best.csv\")\n",
    "\n",
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "\n",
    "if os.path.exists(loss_csv_path):\n",
    "    losses = pd.read_csv(loss_csv_path)\n",
    "else:\n",
    "    # Nur LLM + LoRA\n",
    "    df_lora = df[\n",
    "        df[\"backbone\"].isin(LLM_BACKBONES)\n",
    "        & (df[\"fine_tuning\"] == \"lora\")\n",
    "    ].copy()\n",
    "\n",
    "    # Nur \"Full-LoRA\" (keine Few-Shot-LoRA)\n",
    "    if \"few_shot_k\" in df_lora.columns:\n",
    "        df_lora = df_lora[df_lora[\"few_shot_k\"].isna()].copy()\n",
    "\n",
    "    # Score-Spalte für beste Runs\n",
    "    score_col = \"mt_score\"\n",
    "    if score_col not in df_lora.columns:\n",
    "        score_col = \"test_next_activity_acc\"\n",
    "\n",
    "    # Bester LoRA-Run pro (log, backbone)\n",
    "    best_runs = (\n",
    "        df_lora\n",
    "        .sort_values(score_col, ascending=False)\n",
    "        .groupby([\"log\", \"backbone\"], as_index=False)\n",
    "        .head(1)\n",
    "    )\n",
    "\n",
    "    losses_list = []\n",
    "\n",
    "    for _, row in best_runs.iterrows():\n",
    "        na_acc, na_loss, rt_loss, nt_loss = fetch_single(\n",
    "            wandb_id=row[\"id\"],\n",
    "            project_name=row[\"project\"],\n",
    "            entity=entity,\n",
    "            targets=[\"na\", \"rt\", \"nt\"],\n",
    "        )\n",
    "\n",
    "        # falls etwas fehlt → überspringen\n",
    "        if na_loss is None or rt_loss is None or nt_loss is None:\n",
    "            continue\n",
    "\n",
    "        tmp = pd.DataFrame({\n",
    "            \"epoch\": range(len(na_loss)),\n",
    "            \"na_acc\": na_acc,\n",
    "            \"na_loss\": na_loss,\n",
    "            \"rt_loss\": rt_loss,\n",
    "            \"nt_loss\": nt_loss,\n",
    "        })\n",
    "        tmp[\"log\"] = row[\"log\"]\n",
    "        tmp[\"backbone\"] = row[\"backbone\"]\n",
    "        losses_list.append(tmp)\n",
    "\n",
    "    if not losses_list:\n",
    "        raise RuntimeError(\"Keine Loss-Curves für LoRA-Runs gefunden.\")\n",
    "\n",
    "    losses = pd.concat(losses_list, axis=0, ignore_index=True)\n",
    "    losses.to_csv(loss_csv_path, index=False)\n",
    "    print(\"Saved LoRA loss curves to:\", loss_csv_path)\n",
    "\n",
    "print(\"Loss curves shape (LoRA best runs):\", losses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "becd653b",
   "metadata": {
    "title": "Loss curve visualization (multi-task)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LoRA loss curve plot to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/loss_curves_multitask_lora_best.png\n"
     ]
    }
   ],
   "source": [
    "LOGS_TO_PLOT = sorted(losses[\"log\"].unique())\n",
    "\n",
    "HUE_MAP = {\n",
    "    \"gpt2\":         \"GPT2\",\n",
    "    \"gptneo-1b3\":   \"GPT-Neo-1.3B\",\n",
    "    \"qwen25-05b\":   \"Qwen2.5-0.5B\",\n",
    "    \"llama32-1b\":   \"Llama3.2-1B\",\n",
    "    \"gemma-2-2b\":   \"Gemma-2-2B\",\n",
    "}\n",
    "\n",
    "HUE_ORDER = [\n",
    "    \"GPT2\",\n",
    "    \"GPT-Neo-1.3B\",\n",
    "    \"Qwen2.5-0.5B\",\n",
    "    \"Llama3.2-1B\",\n",
    "    \"Gemma-2-2B\",\n",
    "]\n",
    "\n",
    "# Long-Format\n",
    "l = losses.melt(\n",
    "    id_vars=[\"log\", \"backbone\", \"epoch\"],\n",
    "    value_vars=[\"na_loss\", \"rt_loss\", \"nt_loss\"],\n",
    "    var_name=\"Loss\",\n",
    "    value_name=\"Value\",\n",
    ").dropna(subset=[\"Value\"])\n",
    "\n",
    "l[\"Backbone\"] = l[\"backbone\"].map(HUE_MAP)\n",
    "l = l[l[\"Backbone\"].notna()]\n",
    "\n",
    "LOSS_LABELS = {\n",
    "    \"na_loss\": \"NA Loss\",\n",
    "    \"rt_loss\": \"RT Loss\",\n",
    "    \"nt_loss\": \"NT Loss\",\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    3, len(LOGS_TO_PLOT),\n",
    "    figsize=(4 * len(LOGS_TO_PLOT), 8),\n",
    "    sharex=True\n",
    ")\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "legend_handles, legend_labels = None, None  # globale Legende\n",
    "\n",
    "for loss_name in [\"na_loss\", \"rt_loss\", \"nt_loss\"]:\n",
    "    for log_name in LOGS_TO_PLOT:\n",
    "        ax = next(axes_iter)\n",
    "        tmp = l[(l[\"Loss\"] == loss_name) & (l[\"log\"] == log_name)]\n",
    "\n",
    "        sns.lineplot(\n",
    "            data=tmp,\n",
    "            x=\"epoch\",\n",
    "            y=\"Value\",\n",
    "            hue=\"Backbone\",\n",
    "            hue_order=[h for h in HUE_ORDER if h in tmp[\"Backbone\"].unique()],\n",
    "            ax=ax,\n",
    "            linewidth=2.0,\n",
    "        )\n",
    "\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(LOSS_LABELS[loss_name])\n",
    "        ax.set_title(log_name)\n",
    "\n",
    "        # Legend nur einmal abgreifen\n",
    "        leg = ax.get_legend()\n",
    "        if leg is not None:\n",
    "            handles, labels = leg.legend_handles, [t.get_text() for t in leg.get_texts()]\n",
    "            legend_handles, legend_labels = handles, labels\n",
    "            leg.remove()\n",
    "\n",
    "# globale Legende unter der Figure\n",
    "if legend_handles is not None:\n",
    "    fig.legend(\n",
    "        legend_handles,\n",
    "        legend_labels,\n",
    "        title=\"\",\n",
    "        loc=\"lower center\",\n",
    "        ncol=len(legend_labels),\n",
    "        bbox_to_anchor=(0.5, -0.02),\n",
    "    )\n",
    "\n",
    "plt.tight_layout(rect=(0, 0.05, 1, 1))  # unten Platz für Legende lassen\n",
    "\n",
    "plot_path = os.path.join(output_dir_plots, \"loss_curves_multitask_lora_best.png\")\n",
    "plt.savefig(plot_path, dpi=300)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"Saved LoRA loss curve plot to:\", plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2b476c2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved param summary for BPI12 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI12/param_summary_multitask.csv\n",
      "Saved param summary for BPI17 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI17/param_summary_multitask.csv\n",
      "Saved param summary for BPI20PrepaidTravelCosts to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20PrepaidTravelCosts/param_summary_multitask.csv\n",
      "Saved param summary for BPI20RequestForPayment to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20RequestForPayment/param_summary_multitask.csv\n",
      "Saved param summary for BPI20TravelPermitData to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/per_dataset/BPI20TravelPermitData/param_summary_multitask.csv\n"
     ]
    }
   ],
   "source": [
    "# PARAMETER-SUMMARY \n",
    "\n",
    "multi_path = os.path.join(output_dir_csv, \"multi_task_benchmark_results.csv\")\n",
    "multi = pd.read_csv(multi_path)\n",
    "\n",
    "# Nur Zeilen mit Parameterinfos\n",
    "param_summary = (\n",
    "    multi[\n",
    "        [\n",
    "            \"log\",\n",
    "            \"backbone\",\n",
    "            \"Setting\",\n",
    "            \"total_params\",\n",
    "            \"trainable_params\",\n",
    "        ]\n",
    "    ]\n",
    "    .dropna(subset=[\"total_params\", \"trainable_params\"])\n",
    "    .drop_duplicates()\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "# Anteil trainierbarer Parameter in %\n",
    "param_summary[\"trainable_percent\"] = (\n",
    "    param_summary[\"trainable_params\"] / param_summary[\"total_params\"] * 100.0\n",
    ")\n",
    "\n",
    "param_summary[\"trainable_percent_fmt\"] = (\n",
    "    param_summary[\"trainable_percent\"].round(1).astype(str) + \"%\"\n",
    ")\n",
    "\n",
    "# total_params schön formatiert (wissenschaftliche Notation)\n",
    "param_summary[\"total_params_fmt\"] = param_summary[\"total_params\"].apply(\n",
    "    lambda x: np.format_float_scientific(x, precision=1)\n",
    ")\n",
    "\n",
    "param_summary[\"# params\\n(%trainable)\"] = (\n",
    "    param_summary[\"total_params_fmt\"]\n",
    "    + \" (\"\n",
    "    + param_summary[\"trainable_percent_fmt\"]\n",
    "    + \")\"\n",
    ")\n",
    "\n",
    "# falls vorhanden, Dataset & hübsche Namen mitnehmen\n",
    "if \"Dataset\" in multi.columns:\n",
    "    param_summary[\"Dataset\"] = multi.set_index(\n",
    "        [\"log\", \"backbone\", \"Setting\"]\n",
    "    ).loc[\n",
    "        param_summary.set_index([\"log\", \"backbone\", \"Setting\"]).index,\n",
    "        \"Dataset\"\n",
    "    ].values\n",
    "else:\n",
    "    param_summary[\"Dataset\"] = param_summary[\"log\"]\n",
    "\n",
    "if \"Backbone_pretty\" in multi.columns:\n",
    "    param_summary[\"Backbone_pretty\"] = multi.set_index(\n",
    "        [\"log\", \"backbone\", \"Setting\"]\n",
    "    ).loc[\n",
    "        param_summary.set_index([\"log\", \"backbone\", \"Setting\"]).index,\n",
    "        \"Backbone_pretty\"\n",
    "    ].values\n",
    "else:\n",
    "    param_summary[\"Backbone_pretty\"] = param_summary[\"backbone\"]\n",
    "\n",
    "# --- pro Datensatz (log) speichern ---\n",
    "for log_name, df_log in param_summary.groupby(\"log\"):\n",
    "    log_dir = os.path.join(output_dir_csv, \"per_dataset\", log_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    csv_path = os.path.join(log_dir, \"param_summary_multitask.csv\")\n",
    "    df_log.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved param summary for {log_name} to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03f4bdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Pareto plot for log=BPI12 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI12/pareto_llm_lora_BPI12.png\n",
      "Saved Pareto plot for log=BPI17 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI17/pareto_llm_lora_BPI17.png\n",
      "Saved Pareto plot for log=BPI20PrepaidTravelCosts to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20PrepaidTravelCosts/pareto_llm_lora_BPI20PrepaidTravelCosts.png\n",
      "Saved Pareto plot for log=BPI20RequestForPayment to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20RequestForPayment/pareto_llm_lora_BPI20RequestForPayment.png\n",
      "Saved Pareto plot for log=BPI20TravelPermitData to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20TravelPermitData/pareto_llm_lora_BPI20TravelPermitData.png\n"
     ]
    }
   ],
   "source": [
    "# === PARETO: MT-Score vs. trainierbare Parameter (LoRA, best per LLM-Backbone) ===\n",
    "\n",
    "multi_path = os.path.join(output_dir_csv, \"multi_task_benchmark_results.csv\")\n",
    "multi = pd.read_csv(multi_path)\n",
    "\n",
    "# Relevante LLM-Backbones + Pretty Names\n",
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "BACKBONE_MAP_LLM = {\n",
    "    \"gpt2\":         \"GPT2\",\n",
    "    \"gptneo-1b3\":   \"GPT-Neo-1.3B\",\n",
    "    \"qwen25-05b\":   \"Qwen2.5-0.5B\",\n",
    "    \"llama32-1b\":   \"Llama3.2-1B\",\n",
    "    \"gemma-2-2b\":   \"Gemma-2-2B\",\n",
    "}\n",
    "\n",
    "# Nur LLM + LoRA + benötigte Spalten\n",
    "pareto_source = multi[\n",
    "    (multi[\"backbone\"].isin(LLM_BACKBONES))\n",
    "    & (multi[\"Setting\"] == \"LoRA\")\n",
    "    & multi[\"trainable_params\"].notna()\n",
    "    & multi[\"mt_score_mean\"].notna()\n",
    "].copy()\n",
    "\n",
    "# Pretty-Namen ergänzen (falls noch nicht vorhanden)\n",
    "if \"Backbone_pretty\" not in pareto_source.columns:\n",
    "    pareto_source[\"Backbone_pretty\"] = (\n",
    "        pareto_source[\"backbone\"]\n",
    "        .map(BACKBONE_MAP_LLM)\n",
    "        .fillna(pareto_source[\"backbone\"])\n",
    "    )\n",
    "\n",
    "plots_base_dir = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset\"\n",
    "\n",
    "for log_name, df_log in pareto_source.groupby(\"log\"):\n",
    "    # pro Datensatz: bester LoRA-Run je Backbone\n",
    "    df_best = (\n",
    "        df_log\n",
    "        .sort_values(\n",
    "            [\"backbone\", \"mt_score_mean\", \"trainable_params\"],\n",
    "            ascending=[True, False, True],  # Score ↓, bei Tie weniger Params ↑\n",
    "        )\n",
    "        .drop_duplicates(subset=[\"backbone\"], keep=\"first\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    if df_best.empty:\n",
    "        continue\n",
    "\n",
    "    # etwas breiter, damit die Legend unten gut passt\n",
    "    fig, ax = plt.subplots(figsize=(9, 4.5))\n",
    "\n",
    "    # Scatter: ein Punkt pro Backbone\n",
    "    for bb, df_b in df_best.groupby(\"backbone\"):\n",
    "        label = df_b[\"Backbone_pretty\"].iloc[0]\n",
    "        ax.scatter(\n",
    "            df_b[\"trainable_params\"],\n",
    "            df_b[\"mt_score_mean\"],\n",
    "            label=label,\n",
    "            s=70,\n",
    "        )\n",
    "\n",
    "    # Pareto-Front hervorheben (min trainable_params, max mt_score_mean)\n",
    "    df_pf = df_best.sort_values(\"trainable_params\")\n",
    "    best_so_far = -np.inf\n",
    "    pareto_mask = []\n",
    "    for _, row in df_pf.iterrows():\n",
    "        if row[\"mt_score_mean\"] >= best_so_far - 1e-9:\n",
    "            pareto_mask.append(True)\n",
    "            best_so_far = row[\"mt_score_mean\"]\n",
    "        else:\n",
    "            pareto_mask.append(False)\n",
    "    df_pf_pareto = df_pf[pareto_mask]\n",
    "\n",
    "    if not df_pf_pareto.empty:\n",
    "        ax.scatter(\n",
    "            df_pf_pareto[\"trainable_params\"],\n",
    "            df_pf_pareto[\"mt_score_mean\"],\n",
    "            s=140,\n",
    "            facecolors=\"none\",\n",
    "            edgecolors=\"black\",\n",
    "            linewidths=1.5,\n",
    "        )\n",
    "\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(\"Trainable parameters (LoRA, log scale)\")\n",
    "    ax.set_ylabel(\"MT-Score (mean across seeds)\")\n",
    "    ax.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "    # Titel: wenn Dataset-Spalte existiert, nimm diese Bezeichnung\n",
    "    if \"Dataset\" in df_best.columns:\n",
    "        ds_label = df_best[\"Dataset\"].iloc[0]\n",
    "    else:\n",
    "        ds_label = log_name\n",
    "    ax.set_title(f\"{ds_label} – LLM LoRA Pareto (best per backbone)\")\n",
    "\n",
    "    # Legend unten über die ganze Breite\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(\n",
    "        handles,\n",
    "        labels,\n",
    "        title=\"Backbone\",\n",
    "        loc=\"lower center\",\n",
    "        ncol=len(labels),\n",
    "        frameon=False,\n",
    "        bbox_to_anchor=(0.5, -0.02),\n",
    "    )\n",
    "\n",
    "    # Platz für die Legend unten lassen\n",
    "    plt.tight_layout(rect=(0, 0.12, 1, 1))\n",
    "\n",
    "    log_dir = os.path.join(plots_base_dir, log_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    out_path = os.path.join(log_dir, f\"pareto_llm_lora_{log_name}.png\")\n",
    "\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved Pareto plot for log={log_name} to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8eb02c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TRUE Pareto-front LoRA-sweeps plot for log=BPI12 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI12/pareto_llm_lora_sweeps_true_BPI12.png\n",
      "Saved TRUE Pareto-front LoRA-sweeps plot for log=BPI17 to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI17/pareto_llm_lora_sweeps_true_BPI17.png\n",
      "Saved TRUE Pareto-front LoRA-sweeps plot for log=BPI20PrepaidTravelCosts to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20PrepaidTravelCosts/pareto_llm_lora_sweeps_true_BPI20PrepaidTravelCosts.png\n",
      "Saved TRUE Pareto-front LoRA-sweeps plot for log=BPI20RequestForPayment to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20RequestForPayment/pareto_llm_lora_sweeps_true_BPI20RequestForPayment.png\n",
      "Saved TRUE Pareto-front LoRA-sweeps plot for log=BPI20TravelPermitData to: /ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset/BPI20TravelPermitData/pareto_llm_lora_sweeps_true_BPI20TravelPermitData.png\n"
     ]
    }
   ],
   "source": [
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "BACKBONE_MAP_LLM = {\n",
    "    \"gpt2\":         \"GPT2\",\n",
    "    \"gptneo-1b3\":   \"GPT-Neo-1.3B\",\n",
    "    \"qwen25-05b\":   \"Qwen2.5-0.5B\",\n",
    "    \"llama32-1b\":   \"Llama3.2-1B\",\n",
    "    \"gemma-2-2b\":   \"Gemma-2-2B\",\n",
    "}\n",
    "\n",
    "# Datensatz-Kürzel (wie bisher)\n",
    "DATASET_MAP = {\n",
    "    \"BPI12\": \"BPI12\",\n",
    "    \"BPI17\": \"BPI17\",\n",
    "    \"BPI20PrepaidTravelCosts\": \"BPI20PTC\",\n",
    "    \"BPI20RequestForPayment\": \"BPI20RfP\",\n",
    "    \"BPI20TravelPermitData\": \"BPI20TPD\",\n",
    "}\n",
    "\n",
    "# 1) Alle LLM-LoRA-Runs aus den Roh-Runs df ziehen\n",
    "lora_all = df[\n",
    "    df[\"backbone\"].isin(LLM_BACKBONES)\n",
    "    & (df[\"fine_tuning\"] == \"lora\")\n",
    "].copy()\n",
    "\n",
    "# Voll-LoRA (ohne Few-Shot-LoRA)\n",
    "if \"few_shot_k\" in lora_all.columns:\n",
    "    lora_all = lora_all[lora_all[\"few_shot_k\"].isna()].copy()\n",
    "\n",
    "# nur Zeilen mit Parametern & MT-Score\n",
    "if \"mt_score\" not in lora_all.columns:\n",
    "    raise ValueError(\"Spalte 'mt_score' fehlt in df – bitte sicherstellen, dass sie vorher berechnet wird.\")\n",
    "\n",
    "lora_all = lora_all[\n",
    "    lora_all[\"trainable_params\"].notna()\n",
    "    & lora_all[\"mt_score\"].notna()\n",
    "].copy()\n",
    "\n",
    "if lora_all.empty:\n",
    "    print(\"Keine LoRA-Sweeps mit trainable_params + mt_score gefunden – Pareto-Front wird übersprungen.\")\n",
    "else:\n",
    "    # 2) HParam-Kombi definieren (deine Sweep-Parameter)\n",
    "    HP_SWEEP_COLS = [\n",
    "        \"lr\",\n",
    "        \"batch_size\",\n",
    "        \"epochs\",\n",
    "        \"r\",\n",
    "        \"lora_alpha\",\n",
    "        \"embedding_size\",\n",
    "        \"hidden_size\",\n",
    "        \"strategy\",\n",
    "    ]\n",
    "    HP_SWEEP_COLS = [c for c in HP_SWEEP_COLS if c in lora_all.columns]\n",
    "\n",
    "    group_cols_sweep = [\"log\", \"backbone\"] + HP_SWEEP_COLS\n",
    "\n",
    "    # 3) Über Seeds mitteln: MT-Score + trainable_params\n",
    "    lora_sweeps_grouped = (\n",
    "        lora_all\n",
    "        .groupby(group_cols_sweep, dropna=False)\n",
    "        .agg(\n",
    "            mt_score_mean=(\"mt_score\", \"mean\"),\n",
    "            mt_score_std=(\"mt_score\", \"std\"),\n",
    "            trainable_params=(\"trainable_params\", \"mean\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Backbone-Label\n",
    "    lora_sweeps_grouped[\"Backbone_pretty\"] = (\n",
    "        lora_sweeps_grouped[\"backbone\"]\n",
    "        .map(BACKBONE_MAP_LLM)\n",
    "        .fillna(lora_sweeps_grouped[\"backbone\"])\n",
    "    )\n",
    "\n",
    "    plots_base_dir = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset\"\n",
    "\n",
    "    # 4) Pro Datensatz: All-Sweeps + echte Pareto-Front\n",
    "    for log_name, df_log in lora_sweeps_grouped.groupby(\"log\"):\n",
    "        if df_log.empty:\n",
    "            continue\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "        # --- alle Sweeps: Linien je Backbone (wie zuvor) ---\n",
    "        for bb, df_b in df_log.groupby(\"backbone\"):\n",
    "            df_b = df_b.sort_values(\"trainable_params\")\n",
    "            label = df_b[\"Backbone_pretty\"].iloc[0]\n",
    "\n",
    "            ax.plot(\n",
    "                df_b[\"trainable_params\"],\n",
    "                df_b[\"mt_score_mean\"],\n",
    "                marker=\"o\",\n",
    "                linestyle=\"-\",\n",
    "                linewidth=1.0,\n",
    "                markersize=6,\n",
    "                label=label,\n",
    "                alpha=0.9,\n",
    "            )\n",
    "\n",
    "        # --- echte Pareto-Front über ALLE Sweeps dieses Datensatzes ---\n",
    "        # Ziele: min trainable_params, max mt_score_mean\n",
    "        df_sorted = df_log.sort_values(\"trainable_params\")\n",
    "        best_score = -np.inf\n",
    "        pareto_rows = []\n",
    "\n",
    "        for _, row in df_sorted.iterrows():\n",
    "            score = row[\"mt_score_mean\"]\n",
    "            if score >= best_score - 1e-9:\n",
    "                pareto_rows.append(row)\n",
    "                best_score = score\n",
    "\n",
    "        pareto_df = pd.DataFrame(pareto_rows)\n",
    "\n",
    "        if not pareto_df.empty:\n",
    "            pareto_df = pareto_df.sort_values(\"trainable_params\")\n",
    "            ax.plot(\n",
    "                pareto_df[\"trainable_params\"],\n",
    "                pareto_df[\"mt_score_mean\"],\n",
    "                color=\"black\",\n",
    "                linewidth=1.3,\n",
    "                marker=\"o\",\n",
    "                markersize=4,\n",
    "                label=\"Pareto front\",\n",
    "            )\n",
    "\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.set_xlabel(\"Trainable parameters (LoRA, log scale)\")\n",
    "        ax.set_ylabel(\"MT-Score (mean across seeds)\")\n",
    "\n",
    "        ds_label = DATASET_MAP.get(log_name, log_name)\n",
    "        ax.set_title(f\"{ds_label} – LLM LoRA sweeps (Pareto front)\")\n",
    "\n",
    "        ax.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "        # Legend unten zentriert wie beim anderen Plot\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        fig.legend(\n",
    "            handles,\n",
    "            labels,\n",
    "            title=\"Backbone / front\",\n",
    "            loc=\"lower center\",\n",
    "            ncol=len(labels),\n",
    "            bbox_to_anchor=(0.5, -0.02),\n",
    "        )\n",
    "\n",
    "        # Platz für Legende lassen\n",
    "        plt.tight_layout(rect=(0, 0.10, 1, 1))\n",
    "\n",
    "        log_dir = os.path.join(plots_base_dir, log_name)\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        out_path = os.path.join(log_dir, f\"pareto_llm_lora_sweeps_true_{log_name}.png\")\n",
    "\n",
    "        plt.savefig(out_path, dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "        print(f\"Saved TRUE Pareto-front LoRA-sweeps plot for log={log_name} to: {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "llm-peft-ppm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
