{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b979f1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'llm-peft-ppm (Python 3.12.12)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import wandb\n",
    "\n",
    "project_root = os.path.abspath(\"..\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from ppm.wandb_utils import fetch_experiments\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ.setdefault(\"ENTITY\", \"privajet-university-of-mannheim\")\n",
    "entity = os.environ[\"ENTITY\"]\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"project_root in sys.path:\", project_root in sys.path)\n",
    "print(\"ENTITY:\", entity)\n",
    "print(\"WANDB_MODE:\", os.environ.get(\"WANDB_MODE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d407aeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_csv = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv\"\n",
    "output_dir_plots = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots\"\n",
    "os.makedirs(output_dir_csv, exist_ok=True)\n",
    "os.makedirs(output_dir_plots, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3755ae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_png_and_pdf(fig, out_path_png: str, dpi: int = 300):\n",
    "    # expects out_path_png like \".../name.png\"\n",
    "    fig.savefig(out_path_png, dpi=dpi)\n",
    "    out_path_pdf = os.path.splitext(out_path_png)[0] + \".pdf\"\n",
    "    fig.savefig(out_path_pdf)  # vector PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adf8acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all lines pandas\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "mpl.rcParams.update({\n",
    "    \"figure.figsize\": (6, 4),          \n",
    "    \"font.size\": 10,                   \n",
    "    \"axes.labelsize\": 10,              \n",
    "    \"axes.titlesize\": 10,              \n",
    "    \"legend.fontsize\": 9,              \n",
    "    \"xtick.labelsize\": 9,              \n",
    "    \"ytick.labelsize\": 9,\n",
    "    \"lines.linewidth\": 1.5,            \n",
    "    \"lines.markersize\": 5,             \n",
    "    \"axes.grid\": True,                 \n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"grid.linewidth\": 0.5,\n",
    "    \"legend.frameon\": False,           \n",
    "    \"pdf.fonttype\": 42,                \n",
    "    \"ps.fonttype\": 42,\n",
    "    \"savefig.bbox\": \"tight\",           \n",
    "    \"savefig.dpi\": 300,                \n",
    "})\n",
    "\n",
    "colors = [\n",
    "    \"#9467bd\",\n",
    "    \"#2ca02c\",\n",
    "    \"#bcbd22\",\n",
    "    \"#7f7f7f\",\n",
    "    \"#e377c2\",\n",
    "    \"#8c564b\",\n",
    "    \"#d62728\",\n",
    "    \"#17becf\",\n",
    "    \"#1f77b4\",\n",
    "    \"#ff7f0e\",\n",
    "]\n",
    "\n",
    "plt.rcParams[\"axes.prop_cycle\"] = plt.cycler(color=colors)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689e01c4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _to_int_or_none(x):\n",
    "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "        return None\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def map_setting(row):\n",
    "    ft = row.get(\"fine_tuning\")\n",
    "    k_raw = row.get(\"few_shot_k\", None)\n",
    "    fl    = row.get(\"freeze_layers\", None)\n",
    "    ep_raw = row.get(\"epochs\", None)\n",
    "\n",
    "    k  = _to_int_or_none(k_raw)\n",
    "    ep = _to_int_or_none(ep_raw)\n",
    "\n",
    "    # LoRA Few-Shot\n",
    "    if ft == \"lora\" and k == 8:\n",
    "        return \"FewShot-LoRA\"\n",
    "\n",
    "    # LoRA Full\n",
    "    if ft == \"lora\" and k is None:\n",
    "        return \"LoRA\"\n",
    "\n",
    "    # Zero-Shot (epochs = 0)\n",
    "    if ft == \"freeze\" and ep == 0:\n",
    "        return \"ZeroShot\"\n",
    "\n",
    "    # Freezing Few-Shot\n",
    "    if ft == \"freeze\" and k == 8:\n",
    "        return \"FewShot-Freezing\"\n",
    "\n",
    "    # Freezing standard (keine freeze_layers angegeben)\n",
    "    if ft == \"freeze\" and fl in (None, \"\", [], ()):\n",
    "        return \"Freezing\"\n",
    "\n",
    "    # Freezing layer configs (z.B. -1, -2 / 0, 1)\n",
    "    if ft == \"freeze\" and fl is not None:\n",
    "        if isinstance(fl, (list, tuple)):\n",
    "            fl_clean = [_to_int_or_none(x) for x in fl]\n",
    "        else:\n",
    "            tokens = str(fl).replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \" \").split()\n",
    "            fl_clean = [_to_int_or_none(x) for x in tokens]\n",
    "        fl_clean = [x for x in fl_clean if x is not None]\n",
    "        return f\"Freezing-{fl_clean}\"\n",
    "\n",
    "    return \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ea8e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_path = os.path.join(output_dir_csv, \"global_results.pkl\")\n",
    "\n",
    "BACKBONE_PROJECTS = {\n",
    "    \"majority\":         \"llm-peft-ppm_majority_baseline\",\n",
    "    \"rnn\":              \"llm-peft-ppm_rnn\",\n",
    "    \"transformer\":      \"llm-peft-ppm_transformer_baseline\",\n",
    "    \"tabpfn\":           \"llm-peft-ppm_tabpfn_baseline\",\n",
    "    \"saprpt\":           \"llm-peft-ppm_saprpt_baseline\",\n",
    "    \"gpt2\":             \"llm-peft-ppm_gpt2\",\n",
    "    \"gptneo-1b3\":       \"llm-peft-ppm_gpt-neo-1.3B\",\n",
    "    \"qwen25-05b\":       \"llm-peft-ppm_qwen25-05b\",\n",
    "    \"llama32-1b\":       \"llm-peft-ppm_llama32-1b\",\n",
    "    \"gemma-2-2b\":       \"llm-peft-ppm_gemma-2-2b\",\n",
    "}\n",
    "\n",
    "def build_global_results():\n",
    "    all_results = []\n",
    "    for backbone, project_name in BACKBONE_PROJECTS.items():\n",
    "        df_tmp = fetch_experiments(project=project_name, entity=entity, include_metrics=True)\n",
    "        df_tmp[\"backbone\"] = backbone\n",
    "        df_tmp[\"project\"] = project_name\n",
    "        all_results.append(df_tmp)\n",
    "\n",
    "    gr = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "    safe_cols = [\n",
    "        \"id\", \"log\", \"backbone\", \"project\", \"fine_tuning\",\n",
    "        \"total_params\", \"trainable_params\", \"seed\", \"_runtime\", \"_timestamp\",\n",
    "        \"categorical_features\", \"categorical_targets\",\n",
    "        \"continuous_features\", \"continuous_targets\", \"device\", \"model\", \"name\",\n",
    "\n",
    "        \"test_next_activity_acc\",\n",
    "        \"test_next_activity_loss\",\n",
    "        \"test_next_remaining_time_loss\",\n",
    "        \"test_next_time_to_next_event_loss\",\n",
    "        \"best_test_next_activity_acc\",\n",
    "        \"best_test_next_activity_loss\",\n",
    "        \"best_test_next_remaining_time_loss\",\n",
    "        \"best_test_next_time_to_next_event_loss\",\n",
    "\n",
    "        \"batch_size\",\n",
    "        \"embedding_size\",\n",
    "        \"epochs\",\n",
    "        \"freeze_layers\",\n",
    "        \"grad_clip\",\n",
    "        \"hidden_size\",\n",
    "        \"lr\",\n",
    "        \"n_layers\",\n",
    "        \"rnn_type\",\n",
    "        \"strategy\",\n",
    "        \"weight_decay\",\n",
    "        \"lora_alpha\",\n",
    "        \"r\",\n",
    "        \"few_shot_k\",\n",
    "    ]\n",
    "\n",
    "    safe_cols = [c for c in safe_cols if c in gr.columns]\n",
    "    gr = gr[safe_cols]\n",
    "    return gr\n",
    "\n",
    "\n",
    "if os.path.exists(pkl_path):\n",
    "    try:\n",
    "        global_results = pd.read_pickle(pkl_path)\n",
    "    except Exception as e:\n",
    "        print(\"Fehler beim Laden von global_results.pkl, baue neu:\", repr(e))\n",
    "        global_results = build_global_results()\n",
    "        global_results.to_pickle(pkl_path)\n",
    "else:\n",
    "    global_results = build_global_results()\n",
    "    global_results.to_pickle(pkl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723b0b89",
   "metadata": {
    "title": "Checking best models"
   },
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"id\",\n",
    "    \"log\",\n",
    "    \"backbone\",\n",
    "    \"project\",\n",
    "    \"fine_tuning\",\n",
    "    \"total_params\",\n",
    "    \"trainable_params\",\n",
    "    \"test_next_activity_acc\",\n",
    "    \"test_next_activity_loss\",\n",
    "    \"test_next_remaining_time_loss\",\n",
    "    \"test_next_time_to_next_event_loss\",\n",
    "    \"best_test_next_activity_acc\",\n",
    "    \"best_test_next_activity_loss\",\n",
    "    \"best_test_next_remaining_time_loss\",\n",
    "    \"best_test_next_time_to_next_event_loss\",\n",
    "    \"_runtime\",\n",
    "    \"mt_score\",\n",
    "]\n",
    "\n",
    "df = global_results.copy()\n",
    "df = df[\n",
    "    df[\"test_next_activity_acc\"].notna()\n",
    "    & df[\"test_next_remaining_time_loss\"].notna()\n",
    "    & df[\"test_next_time_to_next_event_loss\"].notna()\n",
    "].copy()\n",
    "\n",
    "sc_acc = MinMaxScaler()\n",
    "sc_rt  = MinMaxScaler()\n",
    "sc_nt  = MinMaxScaler()\n",
    "\n",
    "df[\"na_norm\"] = sc_acc.fit_transform(df[[\"test_next_activity_acc\"]])\n",
    "df[\"rt_norm\"] = sc_rt.fit_transform(-df[[\"test_next_remaining_time_loss\"]])\n",
    "df[\"nt_norm\"] = sc_nt.fit_transform(-df[[\"test_next_time_to_next_event_loss\"]])\n",
    "df[\"mt_score\"] = df[\"na_norm\"] + df[\"rt_norm\"] + df[\"nt_norm\"]\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1682d10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "    \"test_next_activity_acc\",\n",
    "    \"test_next_activity_loss\",\n",
    "    \"test_next_remaining_time_loss\",\n",
    "    \"test_next_time_to_next_event_loss\",\n",
    "    \"best_test_next_activity_acc\",\n",
    "    \"best_test_next_activity_loss\",\n",
    "    \"best_test_next_remaining_time_loss\",\n",
    "    \"best_test_next_time_to_next_event_loss\",\n",
    "]\n",
    "\n",
    "def agg_over_seeds(group: pd.DataFrame) -> pd.Series:\n",
    "    out = {\"n_runs\": len(group)}\n",
    "    for c in [\"total_params\", \"trainable_params\"]:\n",
    "        if c in group.columns:\n",
    "            out[c] = group[c].iloc[0]\n",
    "    if \"mt_score\" in group.columns:\n",
    "        out[\"mt_score_mean\"] = group[\"mt_score\"].mean()\n",
    "        out[\"mt_score_std\"] = group[\"mt_score\"].std()\n",
    "    if \"_runtime\" in group.columns:\n",
    "        out[\"_runtime_mean\"] = group[\"_runtime\"].mean()\n",
    "        out[\"_runtime_std\"]  = group[\"_runtime\"].std()\n",
    "    for m in METRICS:\n",
    "        if m in group.columns:\n",
    "            vals = group[m].dropna()\n",
    "            out[m + \"_mean\"] = vals.mean()\n",
    "            out[m + \"_std\"] = vals.std()\n",
    "    return pd.Series(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889f1a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority = df[df[\"backbone\"] == \"majority\"].copy()\n",
    "majority_grouped = (\n",
    "    majority\n",
    "    .groupby([\"log\", \"backbone\"], dropna=False)\n",
    "    .apply(agg_over_seeds)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "BASELINE_BACKBONES = [\"rnn\", \"transformer\", \"tabpfn\", \"saprpt\"]\n",
    "baseline = df[df[\"backbone\"].isin(BASELINE_BACKBONES)].copy()\n",
    "\n",
    "NON_HP_COLS = set(\n",
    "    [\n",
    "        \"id\",\"log\",\"backbone\",\"categorical_features\",\"categorical_targets\",\n",
    "        \"continuous_features\",\"continuous_targets\",\"device\",\"project\",\"model\",\n",
    "        \"name\",\"fine_tuning\",\"lora_alpha\", \"r\", \"few_shot_k\", \"seed\",\"_runtime\",\"_timestamp\",\n",
    "        \"na_norm\",\"rt_norm\",\"nt_norm\",\"mt_score\",\"majority_stat\",\n",
    "        \"total_params\",\"trainable_params\",\"best_train_next_remaining_time_loss\",\n",
    "        \"_step\",\"best_train_next_activity_loss\",\"train_next_time_to_next_event_loss\",\n",
    "        \"best_train_next_time_to_next_event_loss\",\"train_next_activity_acc\",\n",
    "        \"train_next_activity_loss\",\"_wandb.runtime\",\"best_train_next_activity_acc\",\n",
    "        \"train_next_remaining_time_loss\",\"persist_model\",\"project_name\",\"wandb\",\n",
    "    ]\n",
    "    + METRICS\n",
    ")\n",
    "\n",
    "HP_COLS = [c for c in baseline.columns if c not in NON_HP_COLS]\n",
    "print(\"Hyperparameter columns:\", HP_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c1e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = [\"log\", \"backbone\"] + HP_COLS\n",
    "\n",
    "baseline_grouped = (\n",
    "    baseline\n",
    "    .groupby(group_cols, dropna=False)\n",
    "    .apply(agg_over_seeds)   # deine Funktion von oben\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "score_col = \"mt_score_mean\"\n",
    "if score_col not in baseline_grouped.columns:\n",
    "    score_col = \"test_next_activity_acc_mean\"\n",
    "\n",
    "idx_best = (\n",
    "    baseline_grouped\n",
    "    .groupby([\"log\", \"backbone\"])[score_col]\n",
    "    .idxmax()\n",
    ")\n",
    "baseline_best = baseline_grouped.loc[idx_best].reset_index(drop=True)\n",
    "\n",
    "baseline_all = pd.concat([baseline_best, majority_grouped], ignore_index=True)\n",
    "\n",
    "DATASET_MAP = {\n",
    "    \"BPI12\": \"BPI12\",\n",
    "    \"BPI17\": \"BPI17\",\n",
    "    \"BPI20PrepaidTravelCosts\": \"BPI20PTC\",\n",
    "    \"BPI20RequestForPayment\": \"BPI20RfP\",\n",
    "    \"BPI20TravelPermitData\": \"BPI20TPD\",\n",
    "}\n",
    "BACKBONE_MAP = {\n",
    "    \"majority\": \"Majority\",\n",
    "    \"rnn\": \"RNN\",\n",
    "    \"transformer\": \"Transformer\",\n",
    "    \"tabpfn\": \"TabPFN\",\n",
    "    \"saprpt\": \"SAP-RPT\",\n",
    "}\n",
    "\n",
    "baseline_all[\"Dataset\"] = baseline_all[\"log\"].map(DATASET_MAP).fillna(baseline_all[\"log\"])\n",
    "baseline_all[\"Backbone_pretty\"] = baseline_all[\"backbone\"].map(BACKBONE_MAP).fillna(baseline_all[\"backbone\"])\n",
    "\n",
    "for m in METRICS:\n",
    "    mean_col = m + \"_mean\"\n",
    "    std_col  = m + \"_std\"\n",
    "    if mean_col in baseline_all.columns and std_col in baseline_all.columns:\n",
    "        baseline_all[m + \"_mean_std\"] = (\n",
    "            baseline_all[mean_col].round(4).astype(str)\n",
    "            + \" ± \"\n",
    "            + baseline_all[std_col].round(4).astype(str)\n",
    "        )\n",
    "        \n",
    "if \"_runtime_mean\" in baseline_all.columns:\n",
    "    baseline_all[\"runtime_mean_h\"] = baseline_all[\"_runtime_mean\"] / 3600.0\n",
    "if \"_runtime_std\" in baseline_all.columns:\n",
    "    baseline_all[\"runtime_std_h\"]  = baseline_all[\"_runtime_std\"]  / 3600.0\n",
    "\n",
    "if {\"runtime_mean_h\", \"runtime_std_h\"}.issubset(baseline_all.columns):\n",
    "    mean_str = baseline_all[\"runtime_mean_h\"].map(lambda x: f\"{x:.5f}\")\n",
    "    std_str  = baseline_all[\"runtime_std_h\"].map(lambda x: f\"{x:.5f}\")\n",
    "\n",
    "    baseline_all[\"Runtime (h)\"] = mean_str + \" ± \" + std_str\n",
    "    \n",
    "cols_to_drop = [\"_runtime_mean\", \"_runtime_std\", \"runtime_mean_h\", \"runtime_std_h\"]\n",
    "cols_to_drop = [c for c in cols_to_drop if c in baseline_all.columns]\n",
    "\n",
    "baseline_all = baseline_all.drop(columns=cols_to_drop)\n",
    "\n",
    "csv_path = os.path.join(output_dir_csv, \"baseline_best_settings_mean_std.csv\")\n",
    "baseline_all.to_csv(csv_path, index=False)\n",
    "print(\"Saved baseline summary to:\", csv_path)\n",
    "\n",
    "baseline_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872e9b00",
   "metadata": {
    "title": "Multi-task models"
   },
   "outputs": [],
   "source": [
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "\n",
    "llm = df[df[\"backbone\"].isin(LLM_BACKBONES)].copy()\n",
    "\n",
    "llm[\"Setting\"] = llm.apply(map_setting, axis=1)\n",
    "\n",
    "NON_HP_COLS_LLM = set(NON_HP_COLS)\n",
    "for col in [\"lora_alpha\", \"r\", \"few_shot_k\"]:\n",
    "    NON_HP_COLS_LLM.discard(col)\n",
    "NON_HP_COLS_LLM.add(\"Setting\")\n",
    "\n",
    "HP_COLS_LLM = [c for c in llm.columns if c not in NON_HP_COLS_LLM]\n",
    "print(\"LLM Hyperparameter columns:\", HP_COLS_LLM)\n",
    "\n",
    "group_cols_llm = [\"log\", \"backbone\", \"Setting\"] + HP_COLS_LLM\n",
    "\n",
    "llm_grouped = (\n",
    "    llm\n",
    "    .groupby(group_cols_llm, dropna=False)\n",
    "    .apply(agg_over_seeds)   # gleiche Funktion wie bei Baselines\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "score_col = \"mt_score_mean\"\n",
    "if score_col not in llm_grouped.columns:\n",
    "    score_col = \"test_next_activity_acc_mean\"\n",
    "\n",
    "idx_best_llm = (\n",
    "    llm_grouped\n",
    "    .groupby([\"log\", \"backbone\", \"Setting\"])[score_col]\n",
    "    .idxmax()\n",
    ")\n",
    "\n",
    "llm_all = llm_grouped.loc[idx_best_llm].reset_index(drop=True)\n",
    "\n",
    "BACKBONE_MAP_LLM = {\n",
    "    \"gpt2\":         \"GPT2\",\n",
    "    \"gptneo-1b3\":   \"GPT-Neo-1.3B\",\n",
    "    \"qwen25-05b\":   \"Qwen2.5-0.5B\",\n",
    "    \"llama32-1b\":   \"Llama3.2-1B\",\n",
    "    \"gemma-2-2b\":   \"Gemma-2-2B\",\n",
    "}\n",
    "\n",
    "llm_all[\"Dataset\"] = llm_all[\"log\"].map(DATASET_MAP).fillna(llm_all[\"log\"])\n",
    "llm_all[\"Backbone_pretty\"] = llm_all[\"backbone\"].map(BACKBONE_MAP_LLM).fillna(llm_all[\"backbone\"])\n",
    "\n",
    "for m in METRICS:\n",
    "    mean_col = m + \"_mean\"\n",
    "    std_col  = m + \"_std\"\n",
    "    if mean_col in llm_all.columns and std_col in llm_all.columns:\n",
    "        llm_all[m + \"_mean_std\"] = (\n",
    "            llm_all[mean_col].round(4).astype(str)\n",
    "            + \" ± \"\n",
    "            + llm_all[std_col].round(4).astype(str)\n",
    "        )\n",
    "        \n",
    "if \"_runtime_mean\" in llm_all.columns:\n",
    "    llm_all[\"runtime_mean_h\"] = llm_all[\"_runtime_mean\"] / 3600.0\n",
    "if \"_runtime_std\" in llm_all.columns:\n",
    "    llm_all[\"runtime_std_h\"]  = llm_all[\"_runtime_std\"]  / 3600.0\n",
    "\n",
    "if {\"runtime_mean_h\", \"runtime_std_h\"}.issubset(llm_all.columns):\n",
    "    mean_str = llm_all[\"runtime_mean_h\"].map(lambda x: f\"{x:.5f}\")\n",
    "    std_str  = llm_all[\"runtime_std_h\"].map(lambda x: f\"{x:.5f}\")\n",
    "    llm_all[\"Runtime (h)\"] = mean_str + \" ± \" + std_str\n",
    "\n",
    "cols_to_drop_llm = [\"_runtime_mean\", \"_runtime_std\", \"runtime_mean_h\", \"runtime_std_h\"]\n",
    "cols_to_drop_llm = [c for c in cols_to_drop_llm if c in llm_all.columns]\n",
    "llm_all = llm_all.drop(columns=cols_to_drop_llm)\n",
    "\n",
    "csv_path = os.path.join(output_dir_csv, \"llm_all_settings_by_method_mean_std.csv\")\n",
    "llm_all.to_csv(csv_path, index=False)\n",
    "print(\"Saved LLM summary to:\", csv_path)\n",
    "\n",
    "llm_all.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6952eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm[\"Setting\"] = llm.apply(map_setting, axis=1)\n",
    "print(llm[\"Setting\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d95d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi = pd.concat([baseline_all, llm_all], ignore_index=True, sort=False)\n",
    "\n",
    "multi = (\n",
    "    multi\n",
    "    .sort_values([\"Dataset\", \"Backbone_pretty\", \"Setting\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "csv_path = os.path.join(output_dir_csv, \"multi_task_benchmark_results.csv\")\n",
    "multi.to_csv(csv_path, index=False)\n",
    "print(\"Saved combined multi-task table to:\", csv_path)\n",
    "\n",
    "multi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa585d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_path = os.path.join(output_dir_csv, \"multi_task_benchmark_results.csv\")\n",
    "multi = pd.read_csv(multi_path)\n",
    "\n",
    "for log_name, df_log in multi.groupby(\"log\"):\n",
    "    log_dir = os.path.join(output_dir_csv, \"per_dataset\", log_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    csv_path = os.path.join(log_dir, f\"multi_task_benchmark_results_{log_name}.csv\")\n",
    "    \n",
    "    df_log.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved per-log table for {log_name} to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931c0b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_R = 256\n",
    "FIXED_ALPHA = 512\n",
    "\n",
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "LORA_SETTINGS_REPLACE = [\"LoRA\", \"FewShot-LoRA\"]  # set to [\"LoRA\"] if you only want to fix full LoRA\n",
    "\n",
    "base_path = os.path.join(output_dir_csv, \"multi_task_benchmark_results.csv\")\n",
    "multi_base = pd.read_csv(base_path)\n",
    "\n",
    "if \"Setting\" not in multi_base.columns:\n",
    "    multi_base[\"Setting\"] = np.nan\n",
    "\n",
    "if \"mt_score\" not in df.columns:\n",
    "    tmp = df[\n",
    "        df[\"test_next_activity_acc\"].notna()\n",
    "        & df[\"test_next_remaining_time_loss\"].notna()\n",
    "        & df[\"test_next_time_to_next_event_loss\"].notna()\n",
    "    ].copy()\n",
    "    sc_acc = MinMaxScaler()\n",
    "    sc_rt  = MinMaxScaler()\n",
    "    sc_nt  = MinMaxScaler()\n",
    "    tmp[\"na_norm\"] = sc_acc.fit_transform(tmp[[\"test_next_activity_acc\"]])\n",
    "    tmp[\"rt_norm\"] = sc_rt.fit_transform(-tmp[[\"test_next_remaining_time_loss\"]])\n",
    "    tmp[\"nt_norm\"] = sc_nt.fit_transform(-tmp[[\"test_next_time_to_next_event_loss\"]])\n",
    "    tmp[\"mt_score\"] = tmp[\"na_norm\"] + tmp[\"rt_norm\"] + tmp[\"nt_norm\"]\n",
    "    df = df.merge(tmp[[\"id\", \"mt_score\"]], on=\"id\", how=\"left\")\n",
    "\n",
    "llm = df[df[\"backbone\"].isin(LLM_BACKBONES)].copy()\n",
    "llm[\"Setting\"] = llm.apply(map_setting, axis=1)\n",
    "\n",
    "llm = llm[\n",
    "    (llm[\"Setting\"].isin(LORA_SETTINGS_REPLACE))\n",
    "    & (llm[\"r\"] == FIXED_R)\n",
    "    & (llm[\"lora_alpha\"] == FIXED_ALPHA)\n",
    "].copy()\n",
    "\n",
    "HP = [\"lr\",\"batch_size\",\"epochs\",\"embedding_size\",\"hidden_size\",\"strategy\",\"weight_decay\",\"grad_clip\",\"n_layers\",\"r\",\"lora_alpha\",\"few_shot_k\"]\n",
    "HP = [c for c in HP if c in llm.columns]\n",
    "group_cols = [\"log\", \"backbone\", \"Setting\"] + HP\n",
    "\n",
    "llm_grouped = llm.groupby(group_cols, dropna=False).apply(agg_over_seeds).reset_index()\n",
    "\n",
    "score_col = \"mt_score_mean\" if \"mt_score_mean\" in llm_grouped.columns else \"test_next_activity_acc_mean\"\n",
    "idx = llm_grouped.groupby([\"log\", \"backbone\", \"Setting\"])[score_col].idxmax()\n",
    "llm_fixed_best = llm_grouped.loc[idx].reset_index(drop=True)\n",
    "\n",
    "if \"Dataset\" in multi_base.columns and \"Dataset\" not in llm_fixed_best.columns:\n",
    "    llm_fixed_best[\"Dataset\"] = llm_fixed_best[\"log\"].map(DATASET_MAP).fillna(llm_fixed_best[\"log\"])\n",
    "if \"Backbone_pretty\" in multi_base.columns and \"Backbone_pretty\" not in llm_fixed_best.columns:\n",
    "    llm_fixed_best[\"Backbone_pretty\"] = llm_fixed_best[\"backbone\"].map(BACKBONE_MAP_LLM).fillna(llm_fixed_best[\"backbone\"])\n",
    "\n",
    "for m in METRICS:\n",
    "    mc, sc = m + \"_mean\", m + \"_std\"\n",
    "    outc = m + \"_mean_std\"\n",
    "    if mc in llm_fixed_best.columns and sc in llm_fixed_best.columns:\n",
    "        llm_fixed_best[outc] = llm_fixed_best[mc].round(4).astype(str) + \" ± \" + llm_fixed_best[sc].round(4).astype(str)\n",
    "\n",
    "if \"_runtime_mean\" in llm_fixed_best.columns:\n",
    "    llm_fixed_best[\"runtime_mean_h\"] = llm_fixed_best[\"_runtime_mean\"] / 3600.0\n",
    "if \"_runtime_std\" in llm_fixed_best.columns:\n",
    "    llm_fixed_best[\"runtime_std_h\"] = llm_fixed_best[\"_runtime_std\"] / 3600.0\n",
    "if {\"runtime_mean_h\", \"runtime_std_h\"}.issubset(llm_fixed_best.columns):\n",
    "    llm_fixed_best[\"Runtime (h)\"] = (\n",
    "        llm_fixed_best[\"runtime_mean_h\"].map(lambda x: f\"{x:.5f}\")\n",
    "        + \" ± \"\n",
    "        + llm_fixed_best[\"runtime_std_h\"].map(lambda x: f\"{x:.5f}\")\n",
    "    )\n",
    "\n",
    "drop_rt = [c for c in [\"_runtime_mean\",\"_runtime_std\",\"runtime_mean_h\",\"runtime_std_h\"] if c in llm_fixed_best.columns]\n",
    "if drop_rt:\n",
    "    llm_fixed_best = llm_fixed_best.drop(columns=drop_rt)\n",
    "\n",
    "multi_new = multi_base[\n",
    "    ~(\n",
    "        multi_base[\"backbone\"].isin(LLM_BACKBONES)\n",
    "        & multi_base[\"Setting\"].isin(LORA_SETTINGS_REPLACE)\n",
    "    )\n",
    "].copy()\n",
    "\n",
    "llm_fixed_best = llm_fixed_best.reindex(columns=multi_new.columns, fill_value=np.nan)\n",
    "multi_new = pd.concat([multi_new, llm_fixed_best], ignore_index=True)\n",
    "\n",
    "sort_cols = [c for c in [\"Dataset\", \"Backbone_pretty\", \"Setting\"] if c in multi_new.columns]\n",
    "if sort_cols:\n",
    "    multi_new = multi_new.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "out_path = os.path.join(output_dir_csv, \"multi_task_results_r256_a512.csv\")\n",
    "multi_new.to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e0e563",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_path = os.path.join(output_dir_csv, \"multi_task_results_r256_a512.csv\")\n",
    "multi = pd.read_csv(multi_path)\n",
    "\n",
    "if \"Setting\" not in multi.columns:\n",
    "    multi[\"Setting\"] = np.nan\n",
    "\n",
    "BASELINE_BACKBONES = [\"majority\", \"rnn\", \"transformer\", \"tabpfn\", \"saprpt\"]\n",
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "\n",
    "subset = multi[\n",
    "    multi[\"backbone\"].isin(BASELINE_BACKBONES)\n",
    "    | (multi[\"backbone\"].isin(LLM_BACKBONES) & (multi[\"Setting\"] == \"LoRA\"))\n",
    "].copy()\n",
    "\n",
    "if \"Runtime (h)\" not in subset.columns:\n",
    "    subset[\"Runtime (h)\"] = np.nan\n",
    "\n",
    "EXCLUDE_JOIN = {\n",
    "    \"Dataset\", \"Backbone_pretty\",\n",
    "    \"Runtime (h)\", \"n_runs\",\n",
    "    \"mt_score\", \"mt_score_mean\", \"mt_score_std\",\n",
    "    \"total_params\", \"trainable_params\",\n",
    "}\n",
    "for c in subset.columns:\n",
    "    if c.endswith(\"_mean\") or c.endswith(\"_std\") or c.endswith(\"_mean_std\"):\n",
    "        EXCLUDE_JOIN.add(c)\n",
    "\n",
    "def _infer_join_cols(sub_df: pd.DataFrame, raw_df: pd.DataFrame) -> list[str]:\n",
    "    cols = [c for c in sub_df.columns if c in raw_df.columns and c not in EXCLUDE_JOIN]\n",
    "    for k in [\"log\", \"backbone\"]:\n",
    "        if k not in cols and k in sub_df.columns and k in raw_df.columns:\n",
    "            cols.insert(0, k)\n",
    "    return cols\n",
    "\n",
    "def _round_float_cols(df_in: pd.DataFrame, cols: list[str], ndigits: int = 10) -> pd.DataFrame:\n",
    "    df_out = df_in.copy()\n",
    "    for c in cols:\n",
    "        if c in df_out.columns and pd.api.types.is_float_dtype(df_out[c]):\n",
    "            df_out[c] = df_out[c].round(ndigits)\n",
    "    return df_out\n",
    "\n",
    "def _fill_runtime(part_idx: pd.Index, raw_df: pd.DataFrame):\n",
    "    if part_idx.empty or raw_df.empty or \"_runtime\" not in raw_df.columns:\n",
    "        return\n",
    "    sub_part = subset.loc[part_idx].copy()\n",
    "    join_cols = _infer_join_cols(sub_part, raw_df)\n",
    "    if not join_cols:\n",
    "        return\n",
    "\n",
    "    sub_r = _round_float_cols(sub_part, join_cols)\n",
    "    raw_r = _round_float_cols(raw_df, join_cols)\n",
    "\n",
    "    rt = (\n",
    "        raw_r.groupby(join_cols, dropna=False)\n",
    "        .agg(_runtime_mean=(\"_runtime\", \"mean\"), _runtime_std=(\"_runtime\", \"std\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "    rt[\"_runtime_std\"] = rt[\"_runtime_std\"].fillna(0.0)\n",
    "    rt[\"Runtime (h)_recalc\"] = (\n",
    "        (rt[\"_runtime_mean\"] / 3600.0).map(lambda x: f\"{x:.5f}\")\n",
    "        + \" ± \"\n",
    "        + (rt[\"_runtime_std\"] / 3600.0).map(lambda x: f\"{x:.5f}\")\n",
    "    )\n",
    "\n",
    "    merged = sub_r.merge(rt[join_cols + [\"Runtime (h)_recalc\"]], on=join_cols, how=\"left\")\n",
    "    subset.loc[part_idx, \"Runtime (h)\"] = (\n",
    "        subset.loc[part_idx, \"Runtime (h)\"].fillna(merged[\"Runtime (h)_recalc\"].to_numpy()).values\n",
    "    )\n",
    "\n",
    "need_runtime = subset[\"Runtime (h)\"].isna().any()\n",
    "if need_runtime:\n",
    "    df_base = df[df[\"backbone\"].isin(BASELINE_BACKBONES)].copy()\n",
    "\n",
    "    df_lora = df[(df[\"backbone\"].isin(LLM_BACKBONES)) & (df[\"fine_tuning\"] == \"lora\")].copy()\n",
    "    if \"few_shot_k\" in df_lora.columns:\n",
    "        df_lora = df_lora[df_lora[\"few_shot_k\"].isna()].copy()\n",
    "\n",
    "    idx_base = subset.index[subset[\"backbone\"].isin(BASELINE_BACKBONES)]\n",
    "    idx_lora = subset.index[subset[\"backbone\"].isin(LLM_BACKBONES) & (subset[\"Setting\"] == \"LoRA\")]\n",
    "\n",
    "    _fill_runtime(idx_base, df_base)\n",
    "    _fill_runtime(idx_lora, df_lora)\n",
    "\n",
    "def _fmt_params(total, trainable) -> str:\n",
    "    if pd.isna(total) or float(total) == 0.0:\n",
    "        return \"\"\n",
    "    total_fmt = np.format_float_scientific(float(total), precision=1)\n",
    "    if pd.isna(trainable):\n",
    "        return total_fmt\n",
    "    pct = (float(trainable) / float(total)) * 100.0\n",
    "    return f\"{total_fmt} ({pct:.0f}%)\"\n",
    "\n",
    "subset[\"# params (%trainable)\"] = [\n",
    "    _fmt_params(t, tr) for t, tr in zip(subset.get(\"total_params\", np.nan), subset.get(\"trainable_params\", np.nan))\n",
    "]\n",
    "\n",
    "for log_name, df_log in subset.groupby(\"log\"):\n",
    "    log_dir = os.path.join(output_dir_csv, \"per_dataset\", log_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    out_path = os.path.join(log_dir, f\"baseline_vs_lora_multi_task_original_results_{log_name}.csv\")\n",
    "    df_log.to_csv(out_path, index=False)\n",
    "    print(f\"Saved baseline vs LoRA table for {log_name} to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b9f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_path = os.path.join(output_dir_csv, \"multi_task_benchmark_results.csv\")\n",
    "multi = pd.read_csv(multi_path)\n",
    "\n",
    "if \"Setting\" not in multi.columns:\n",
    "    multi[\"Setting\"] = np.nan\n",
    "\n",
    "BASELINE_BACKBONES = [\"majority\", \"rnn\", \"transformer\", \"tabpfn\", \"saprpt\"]\n",
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "\n",
    "subset = multi[\n",
    "    multi[\"backbone\"].isin(BASELINE_BACKBONES)\n",
    "    | (multi[\"backbone\"].isin(LLM_BACKBONES) & (multi[\"Setting\"] == \"LoRA\"))\n",
    "].copy()\n",
    "\n",
    "if \"Runtime (h)\" not in subset.columns:\n",
    "    subset[\"Runtime (h)\"] = np.nan\n",
    "\n",
    "EXCLUDE_JOIN = {\n",
    "    \"Dataset\", \"Backbone_pretty\",\n",
    "    \"Runtime (h)\", \"n_runs\",\n",
    "    \"mt_score\", \"mt_score_mean\", \"mt_score_std\",\n",
    "    \"total_params\", \"trainable_params\",\n",
    "}\n",
    "for c in subset.columns:\n",
    "    if c.endswith(\"_mean\") or c.endswith(\"_std\") or c.endswith(\"_mean_std\"):\n",
    "        EXCLUDE_JOIN.add(c)\n",
    "\n",
    "def _infer_join_cols(sub_df: pd.DataFrame, raw_df: pd.DataFrame) -> list[str]:\n",
    "    cols = [c for c in sub_df.columns if c in raw_df.columns and c not in EXCLUDE_JOIN]\n",
    "    for k in [\"log\", \"backbone\"]:\n",
    "        if k not in cols and k in sub_df.columns and k in raw_df.columns:\n",
    "            cols.insert(0, k)\n",
    "    return cols\n",
    "\n",
    "def _round_float_cols(df_in: pd.DataFrame, cols: list[str], ndigits: int = 10) -> pd.DataFrame:\n",
    "    df_out = df_in.copy()\n",
    "    for c in cols:\n",
    "        if c in df_out.columns and pd.api.types.is_float_dtype(df_out[c]):\n",
    "            df_out[c] = df_out[c].round(ndigits)\n",
    "    return df_out\n",
    "\n",
    "def _fill_runtime(part_idx: pd.Index, raw_df: pd.DataFrame):\n",
    "    if part_idx.empty or raw_df.empty or \"_runtime\" not in raw_df.columns:\n",
    "        return\n",
    "    sub_part = subset.loc[part_idx].copy()\n",
    "    join_cols = _infer_join_cols(sub_part, raw_df)\n",
    "    if not join_cols:\n",
    "        return\n",
    "\n",
    "    sub_r = _round_float_cols(sub_part, join_cols)\n",
    "    raw_r = _round_float_cols(raw_df, join_cols)\n",
    "\n",
    "    rt = (\n",
    "        raw_r.groupby(join_cols, dropna=False)\n",
    "        .agg(_runtime_mean=(\"_runtime\", \"mean\"), _runtime_std=(\"_runtime\", \"std\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "    rt[\"_runtime_std\"] = rt[\"_runtime_std\"].fillna(0.0)\n",
    "    rt[\"Runtime (h)_recalc\"] = (\n",
    "        (rt[\"_runtime_mean\"] / 3600.0).map(lambda x: f\"{x:.5f}\")\n",
    "        + \" ± \"\n",
    "        + (rt[\"_runtime_std\"] / 3600.0).map(lambda x: f\"{x:.5f}\")\n",
    "    )\n",
    "\n",
    "    merged = sub_r.merge(rt[join_cols + [\"Runtime (h)_recalc\"]], on=join_cols, how=\"left\")\n",
    "    subset.loc[part_idx, \"Runtime (h)\"] = (\n",
    "        subset.loc[part_idx, \"Runtime (h)\"].fillna(merged[\"Runtime (h)_recalc\"].to_numpy()).values\n",
    "    )\n",
    "\n",
    "need_runtime = subset[\"Runtime (h)\"].isna().any()\n",
    "if need_runtime:\n",
    "    df_base = df[df[\"backbone\"].isin(BASELINE_BACKBONES)].copy()\n",
    "\n",
    "    df_lora = df[(df[\"backbone\"].isin(LLM_BACKBONES)) & (df[\"fine_tuning\"] == \"lora\")].copy()\n",
    "    if \"few_shot_k\" in df_lora.columns:\n",
    "        df_lora = df_lora[df_lora[\"few_shot_k\"].isna()].copy()\n",
    "\n",
    "    idx_base = subset.index[subset[\"backbone\"].isin(BASELINE_BACKBONES)]\n",
    "    idx_lora = subset.index[subset[\"backbone\"].isin(LLM_BACKBONES) & (subset[\"Setting\"] == \"LoRA\")]\n",
    "\n",
    "    _fill_runtime(idx_base, df_base)\n",
    "    _fill_runtime(idx_lora, df_lora)\n",
    "\n",
    "def _fmt_params(total, trainable) -> str:\n",
    "    if pd.isna(total) or float(total) == 0.0:\n",
    "        return \"\"\n",
    "    total_fmt = np.format_float_scientific(float(total), precision=1)\n",
    "    if pd.isna(trainable):\n",
    "        return total_fmt\n",
    "    pct = (float(trainable) / float(total)) * 100.0\n",
    "    return f\"{total_fmt} ({pct:.0f}%)\"\n",
    "\n",
    "subset[\"# params (%trainable)\"] = [\n",
    "    _fmt_params(t, tr) for t, tr in zip(subset.get(\"total_params\", np.nan), subset.get(\"trainable_params\", np.nan))\n",
    "]\n",
    "\n",
    "for log_name, df_log in subset.groupby(\"log\"):\n",
    "    log_dir = os.path.join(output_dir_csv, \"per_dataset\", log_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    out_path = os.path.join(log_dir, f\"baseline_vs_lora_multi_task_results_{log_name}.csv\")\n",
    "    df_log.to_csv(out_path, index=False)\n",
    "    print(f\"Saved baseline vs LoRA table for {log_name} to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c13022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Export LLM methods per dataset/backbone (+ \"# params (%trainable)\")\n",
    "\n",
    "multi_path = os.path.join(output_dir_csv, \"multi_task_results_r256_a512.csv\")\n",
    "multi = pd.read_csv(multi_path)\n",
    "\n",
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "llm_multi = multi[multi[\"backbone\"].isin(LLM_BACKBONES)].copy()\n",
    "\n",
    "# --- add \"# params (%trainable)\" column ---\n",
    "def _fmt_params(total, trainable) -> str:\n",
    "    if pd.isna(total) or float(total) == 0.0:\n",
    "        return \"\"\n",
    "    total_fmt = np.format_float_scientific(float(total), precision=1)  # e.g., 1.2e+09\n",
    "    if pd.isna(trainable):\n",
    "        return total_fmt\n",
    "    pct = (float(trainable) / float(total)) * 100.0\n",
    "    return f\"{total_fmt} ({pct:.0f}%)\"\n",
    "\n",
    "if \"total_params\" in llm_multi.columns and \"trainable_params\" in llm_multi.columns:\n",
    "    llm_multi[\"# params (%trainable)\"] = [\n",
    "        _fmt_params(t, tr) for t, tr in zip(llm_multi[\"total_params\"], llm_multi[\"trainable_params\"])\n",
    "    ]\n",
    "else:\n",
    "    llm_multi[\"# params (%trainable)\"] = \"\"\n",
    "\n",
    "# --- export per dataset/backbone ---\n",
    "for (log_name, backbone), df_sub in llm_multi.groupby([\"log\", \"backbone\"]):\n",
    "    log_dir = os.path.join(output_dir_csv, \"per_dataset\", log_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    csv_path = os.path.join(log_dir, f\"llm_methods_{log_name}_{backbone}.csv\")\n",
    "    df_sub.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved LLM methods table for log={log_name}, backbone={backbone} to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd3673",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "\n",
    "lora_sweeps = df[\n",
    "    df[\"backbone\"].isin(LLM_BACKBONES)\n",
    "    & (df[\"fine_tuning\"] == \"lora\")\n",
    "].copy()\n",
    "\n",
    "if \"few_shot_k\" in lora_sweeps.columns:\n",
    "    lora_sweeps = lora_sweeps[lora_sweeps[\"few_shot_k\"].isna()].copy()\n",
    "\n",
    "HP_SWEEP_COLS = [\n",
    "    \"lr\",\n",
    "    \"batch_size\",\n",
    "    \"epochs\",\n",
    "    \"r\",\n",
    "    \"lora_alpha\",\n",
    "    \"embedding_size\",\n",
    "    \"hidden_size\",\n",
    "    \"strategy\",\n",
    "]\n",
    "HP_SWEEP_COLS = [c for c in HP_SWEEP_COLS if c in lora_sweeps.columns]\n",
    "\n",
    "group_cols_sweep = [\"log\", \"backbone\"] + HP_SWEEP_COLS\n",
    "\n",
    "lora_sweeps_grouped = (\n",
    "    lora_sweeps\n",
    "    .groupby(group_cols_sweep, dropna=False)\n",
    "    .apply(agg_over_seeds)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "for m in METRICS:\n",
    "    mean_col = m + \"_mean\"\n",
    "    std_col  = m + \"_std\"\n",
    "    if mean_col in lora_sweeps_grouped.columns and std_col in lora_sweeps_grouped.columns:\n",
    "        lora_sweeps_grouped[m + \"_mean_std\"] = (\n",
    "            lora_sweeps_grouped[mean_col].round(4).astype(str)\n",
    "            + \" ± \"\n",
    "            + lora_sweeps_grouped[std_col].round(4).astype(str)\n",
    "        )\n",
    "\n",
    "if \"_runtime_mean\" in lora_sweeps_grouped.columns:\n",
    "    lora_sweeps_grouped[\"runtime_mean_h\"] = lora_sweeps_grouped[\"_runtime_mean\"] / 3600.0\n",
    "if \"_runtime_std\" in lora_sweeps_grouped.columns:\n",
    "    lora_sweeps_grouped[\"runtime_std_h\"]  = lora_sweeps_grouped[\"_runtime_std\"]  / 3600.0\n",
    "\n",
    "if {\"runtime_mean_h\", \"runtime_std_h\"}.issubset(lora_sweeps_grouped.columns):\n",
    "    mean_str = lora_sweeps_grouped[\"runtime_mean_h\"].map(lambda x: f\"{x:.5f}\")\n",
    "    std_str  = lora_sweeps_grouped[\"runtime_std_h\"].map(lambda x: f\"{x:.5f}\")\n",
    "    lora_sweeps_grouped[\"Runtime (h)\"] = mean_str + \" ± \" + std_str\n",
    "\n",
    "cols_to_drop_sweeps = [\"_runtime_mean\", \"_runtime_std\", \"runtime_mean_h\", \"runtime_std_h\"]\n",
    "cols_to_drop_sweeps = [c for c in cols_to_drop_sweeps if c in lora_sweeps_grouped.columns]\n",
    "lora_sweeps_grouped = lora_sweeps_grouped.drop(columns=cols_to_drop_sweeps)\n",
    "\n",
    "def _fmt_params(total, trainable) -> str:\n",
    "    if pd.isna(total) or float(total) == 0.0:\n",
    "        return \"\"\n",
    "    total_fmt = np.format_float_scientific(float(total), precision=1)  # e.g., 1.2e+09\n",
    "    if pd.isna(trainable):\n",
    "        return total_fmt\n",
    "    pct = (float(trainable) / float(total)) * 100.0\n",
    "    return f\"{total_fmt} ({pct:.0f}%)\"\n",
    "\n",
    "if \"total_params\" in lora_sweeps_grouped.columns and \"trainable_params\" in lora_sweeps_grouped.columns:\n",
    "    lora_sweeps_grouped[\"# params (%trainable)\"] = [\n",
    "        _fmt_params(t, tr)\n",
    "        for t, tr in zip(lora_sweeps_grouped[\"total_params\"], lora_sweeps_grouped[\"trainable_params\"])\n",
    "    ]\n",
    "else:\n",
    "    lora_sweeps_grouped[\"# params (%trainable)\"] = \"\"\n",
    "\n",
    "for (log_name, backbone), df_sub in lora_sweeps_grouped.groupby([\"log\", \"backbone\"]):\n",
    "    log_dir = os.path.join(output_dir_csv, \"per_dataset\", log_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    out_path = os.path.join(\n",
    "        log_dir,\n",
    "        f\"llm_methods_{log_name}_{backbone}_lora_sweeps.csv\"\n",
    "    )\n",
    "    df_sub.to_csv(out_path, index=False)\n",
    "    print(f\"Saved LoRA sweeps table for log={log_name}, backbone={backbone} to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77607748",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "\n",
    "multi_path = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/multi_task_results_r256_a512.csv\"\n",
    "llm = pd.read_csv(multi_path)\n",
    "\n",
    "# --- sanity checks ---\n",
    "required_cols = {\"log\", \"backbone\", \"Setting\"}\n",
    "missing = required_cols - set(llm.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns in {multi_path}: {sorted(missing)}\")\n",
    "\n",
    "# keep only LLM backbones\n",
    "llm = llm[llm[\"backbone\"].isin(LLM_BACKBONES)].copy()\n",
    "\n",
    "# keep only selected adaptation settings (this implicitly removes Freezing / FewShot-Freezing etc.)\n",
    "MAIN_SETTING_ORDER = [\"ZeroShot\", \"LoRA\", \"FewShot-LoRA\"]\n",
    "llm = llm[llm[\"Setting\"].isin(MAIN_SETTING_ORDER)].copy()\n",
    "\n",
    "# keep the same column name used downstream\n",
    "llm[\"Setting_main\"] = llm[\"Setting\"]\n",
    "\n",
    "# pretty model names (as before)\n",
    "BACKBONE_MAP_LLM = {\n",
    "    \"gpt2\":        \"GPT2\",\n",
    "    \"gptneo-1b3\":  \"GPT-Neo-1.3B\",\n",
    "    \"qwen25-05b\":  \"Qwen2.5-0.5B\",\n",
    "    \"llama32-1b\":  \"Llama3.2-1B\",\n",
    "    \"gemma-2-2b\":  \"Gemma-2-2B\",\n",
    "}\n",
    "llm[\"Backbone_pretty\"] = llm[\"backbone\"].map(BACKBONE_MAP_LLM).fillna(llm[\"backbone\"])\n",
    "\n",
    "plots_base_dir = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset\"\n",
    "\n",
    "PLOTS = [\n",
    "    (\"test_next_activity_acc\",            \"NA Acc.\"),\n",
    "    (\"test_next_remaining_time_loss\",     \"RT MSE\"),\n",
    "    (\"test_next_time_to_next_event_loss\", \"NT MSE\"),\n",
    "]\n",
    "\n",
    "def _parse_mean_std_cell(x) -> tuple[float, float]:\n",
    "    \"\"\"Parse 'mean ± std' or plain numeric cell.\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return np.nan, 0.0\n",
    "    if isinstance(x, (int, float, np.number)):\n",
    "        return float(x), 0.0\n",
    "\n",
    "    s = str(x).strip()\n",
    "    if \"±\" in s:\n",
    "        left, right = s.split(\"±\", 1)\n",
    "        try:\n",
    "            return float(left.strip()), float(right.strip())\n",
    "        except Exception:\n",
    "            return np.nan, 0.0\n",
    "\n",
    "    try:\n",
    "        return float(s), 0.0\n",
    "    except Exception:\n",
    "        return np.nan, 0.0\n",
    "\n",
    "def _get_mean_std(row_df: pd.DataFrame, metric: str) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Extract (mean, std) from a single-row aggregated table.\n",
    "    Supports:\n",
    "      - metric_mean / metric_std\n",
    "      - metric_mean_std formatted like '0.1234 ± 0.0056'\n",
    "      - fallback: metric (std=0)\n",
    "    \"\"\"\n",
    "    mean_col = f\"{metric}_mean\"\n",
    "    std_col = f\"{metric}_std\"\n",
    "    mean_std_col = f\"{metric}_mean_std\"\n",
    "\n",
    "    if mean_col in row_df.columns and pd.notna(row_df[mean_col].iloc[0]):\n",
    "        mean = float(row_df[mean_col].iloc[0])\n",
    "        std = float(row_df[std_col].iloc[0]) if std_col in row_df.columns and pd.notna(row_df[std_col].iloc[0]) else 0.0\n",
    "        return mean, std\n",
    "\n",
    "    if mean_std_col in row_df.columns:\n",
    "        return _parse_mean_std_cell(row_df[mean_std_col].iloc[0])\n",
    "\n",
    "    if metric in row_df.columns:\n",
    "        return _parse_mean_std_cell(row_df[metric].iloc[0])\n",
    "\n",
    "    raise KeyError(f\"Could not find '{mean_col}'/'{std_col}' or '{mean_std_col}' (or '{metric}') in {multi_path}.\")\n",
    "\n",
    "for log_name, df_log in llm.groupby(\"log\"):\n",
    "    log_dir = os.path.join(plots_base_dir, log_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    for setting in MAIN_SETTING_ORDER:\n",
    "        df_s = df_log[df_log[\"Setting_main\"] == setting].copy()\n",
    "        if df_s.empty:\n",
    "            continue\n",
    "\n",
    "        # backbone order (as before)\n",
    "        backbone_order_pretty = [\n",
    "            BACKBONE_MAP_LLM[b]\n",
    "            for b in LLM_BACKBONES\n",
    "            if b in df_s[\"backbone\"].unique()\n",
    "        ]\n",
    "        if not backbone_order_pretty:\n",
    "            continue\n",
    "\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(8, 9), sharex=True)\n",
    "\n",
    "        x = np.arange(len(backbone_order_pretty))\n",
    "\n",
    "        for ax, (metric, ylabel) in zip(axes, PLOTS):\n",
    "            means, stds = [], []\n",
    "\n",
    "            for pretty_name in backbone_order_pretty:\n",
    "                row = df_s[df_s[\"Backbone_pretty\"] == pretty_name]\n",
    "                if row.empty:\n",
    "                    means.append(np.nan)\n",
    "                    stds.append(0.0)\n",
    "                else:\n",
    "                    m, sd = _get_mean_std(row, metric)\n",
    "                    means.append(m)\n",
    "                    stds.append(sd)\n",
    "\n",
    "            keep = [i for i, v in enumerate(means) if pd.notna(v)]\n",
    "            if not keep:\n",
    "                ax.set_ylabel(ylabel)\n",
    "                continue\n",
    "\n",
    "            x_k = x[keep]\n",
    "            means_k = [means[i] for i in keep]\n",
    "            stds_k = [stds[i] for i in keep]\n",
    "            labels_k = [backbone_order_pretty[i] for i in keep]\n",
    "\n",
    "            ax.bar(x_k, means_k, yerr=stds_k, capsize=4)\n",
    "            ax.set_ylabel(ylabel)\n",
    "            ax.set_xticks(x_k)\n",
    "            ax.set_xticklabels(labels_k, rotation=45, ha=\"right\")\n",
    "            ax.set_ylim(bottom=0)\n",
    "\n",
    "        axes[-1].set_xlabel(\"LLM backbone\")\n",
    "        fig.suptitle(f\"{log_name} – {setting}\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # keep the SAME filename pattern as your original code\n",
    "        out_path = os.path.join(\n",
    "            log_dir,\n",
    "            f\"llm_backbones_boxplot_{log_name}_{setting}.png\"\n",
    "        )\n",
    "        save_png_and_pdf(fig, out_path, dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "        print(f\"Saved LLM-backbone comparison for log={log_name}, setting={setting} to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a9ca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "KEEP_SETTINGS = [\"ZeroShot\", \"LoRA\", \"FewShot-LoRA\"]\n",
    "\n",
    "BACKBONE_MAP_LLM = {\n",
    "    \"gpt2\":         \"GPT2\",\n",
    "    \"gptneo-1b3\":   \"GPT-Neo-1.3B\",\n",
    "    \"qwen25-05b\":   \"Qwen2.5-0.5B\",\n",
    "    \"llama32-1b\":   \"Llama3.2-1B\",\n",
    "    \"gemma-2-2b\":   \"Gemma-2-2B\",\n",
    "}\n",
    "MODE_MAP = {\"ZeroShot\": \"ZS\", \"FewShot-LoRA\": \"FS\", \"LoRA\": \"LoRA\"}\n",
    "\n",
    "# metrics (raw columns in df)\n",
    "PLOTS = [\n",
    "    (\"test_next_activity_acc\",            \"NA Acc.\"),\n",
    "    (\"test_next_remaining_time_loss\",     \"RT MSE\"),\n",
    "    (\"test_next_time_to_next_event_loss\", \"NT MSE\"),\n",
    "]\n",
    "METRICS = [m for (m, _) in PLOTS]\n",
    "\n",
    "# paths\n",
    "multi_path = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/multi_task_results_r256_a512.csv\"\n",
    "out_csv_dir = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv\"\n",
    "os.makedirs(out_csv_dir, exist_ok=True)\n",
    "\n",
    "out_csv_75 = os.path.join(out_csv_dir, \"rq3_llm_tradeoff_per_log_75_selected_hp.csv\")\n",
    "# NOTE: still same filename, but now pooled table uses MEDIAN + IQR (Q1/Q3) across logs\n",
    "out_csv_15 = os.path.join(out_csv_dir, \"rq3_llm_tradeoff_pooled15_median_selected_hp.csv\")\n",
    "\n",
    "# load selected configs (best HP per log/backbone/Setting)\n",
    "selected = pd.read_csv(multi_path)\n",
    "if \"Setting\" not in selected.columns:\n",
    "    raise ValueError(\"Column 'Setting' not found in multi_task_results_r256_a512.csv\")\n",
    "\n",
    "selected = selected[\n",
    "    selected[\"backbone\"].isin(LLM_BACKBONES) & selected[\"Setting\"].isin(KEEP_SETTINGS)\n",
    "].copy()\n",
    "\n",
    "# raw runs from df: add Setting and filter\n",
    "raw = df[df[\"backbone\"].isin(LLM_BACKBONES)].copy()\n",
    "raw[\"Setting\"] = raw.apply(map_setting, axis=1)\n",
    "raw = raw[raw[\"Setting\"].isin(KEEP_SETTINGS)].copy()\n",
    "\n",
    "# join raw runs to selected configs\n",
    "HP_CANDIDATES = [\n",
    "    \"lr\", \"batch_size\", \"epochs\",\n",
    "    \"embedding_size\", \"hidden_size\", \"strategy\",\n",
    "    \"weight_decay\", \"grad_clip\", \"n_layers\",\n",
    "    \"r\", \"lora_alpha\", \"few_shot_k\",\n",
    "]\n",
    "HP_COLS = [c for c in HP_CANDIDATES if c in raw.columns and c in selected.columns]\n",
    "join_cols = [\"log\", \"backbone\", \"Setting\"] + HP_COLS\n",
    "\n",
    "def _prep_join(df_in: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    out = df_in.copy()\n",
    "    for c in cols:\n",
    "        if c not in out.columns:\n",
    "            continue\n",
    "        # treat HP as numeric for robust joining\n",
    "        if c in HP_CANDIDATES or pd.api.types.is_numeric_dtype(out[c]):\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\").round(10).fillna(-999999.0)\n",
    "        else:\n",
    "            out[c] = out[c].astype(str).fillna(\"__NA__\")\n",
    "    return out\n",
    "\n",
    "raw_j = _prep_join(raw, join_cols)\n",
    "sel_j = _prep_join(selected, join_cols)\n",
    "\n",
    "raw_selected = raw_j.merge(\n",
    "    sel_j[join_cols].drop_duplicates(),\n",
    "    on=join_cols,\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "if raw_selected.empty:\n",
    "    raise RuntimeError(\n",
    "        \"No raw runs matched the selected configs from multi_task_results_r256_a512.csv. \"\n",
    "        \"Check that the HP columns in the CSV match those in df.\"\n",
    "    )\n",
    "\n",
    "# runtime column (seconds)\n",
    "RUNTIME_CANDIDATES = [\n",
    "    \"_runtime\", \"_runtime_mean\",\n",
    "    \"runtime\", \"train_runtime\",\n",
    "    \"_runtime_sec\", \"_runtime_seconds\", \"runtime_seconds\",\n",
    "]\n",
    "runtime_col = next((c for c in RUNTIME_CANDIDATES if c in raw_selected.columns), None)\n",
    "\n",
    "# numeric coercion\n",
    "for m in METRICS:\n",
    "    if m not in raw_selected.columns:\n",
    "        raise ValueError(f\"Required raw metric column '{m}' not found in df.\")\n",
    "    raw_selected[m] = pd.to_numeric(raw_selected[m], errors=\"coerce\")\n",
    "\n",
    "if runtime_col is not None:\n",
    "    raw_selected[runtime_col] = pd.to_numeric(raw_selected[runtime_col], errors=\"coerce\")\n",
    "\n",
    "# -------------------- helpers --------------------\n",
    "def _fmt_pm(mu, sd, d=4):\n",
    "    \"\"\"mean ± std\"\"\"\n",
    "    if pd.isna(mu):\n",
    "        return \"\"\n",
    "    if pd.isna(sd):\n",
    "        sd = 0.0\n",
    "    return f\"{mu:.{d}f} ± {sd:.{d}f}\"\n",
    "\n",
    "def _fmt_median_q1q3(med, q1, q3, d=4):\n",
    "    \"\"\"median [Q1, Q3]\"\"\"\n",
    "    if pd.isna(med):\n",
    "        return \"\"\n",
    "    if pd.isna(q1) or pd.isna(q3):\n",
    "        return f\"{med:.{d}f}\"\n",
    "    return f\"{med:.{d}f} [{q1:.{d}f}, {q3:.{d}f}]\"\n",
    "\n",
    "def _q1(x): return x.quantile(0.25)\n",
    "def _q3(x): return x.quantile(0.75)\n",
    "def _iqr(x): return x.quantile(0.75) - x.quantile(0.25)\n",
    "\n",
    "# ==================== 75-row table: per log/backbone/Setting (mean±std over seeds) ====================\n",
    "group_cols = [\"log\", \"backbone\", \"Setting\"]\n",
    "\n",
    "agg_named = {}\n",
    "for m in METRICS:\n",
    "    agg_named[f\"{m}_mean\"] = (m, \"mean\")\n",
    "    agg_named[f\"{m}_std\"]  = (m, \"std\")\n",
    "\n",
    "if runtime_col is not None:\n",
    "    agg_named[\"runtime_s_mean\"] = (runtime_col, \"mean\")\n",
    "    agg_named[\"runtime_s_std\"]  = (runtime_col, \"std\")\n",
    "\n",
    "per_log = raw_selected.groupby(group_cols, dropna=False).agg(**agg_named).reset_index()\n",
    "\n",
    "# runtime hours\n",
    "if runtime_col is not None:\n",
    "    per_log[\"runtime_h_mean\"] = per_log[\"runtime_s_mean\"] / 3600.0\n",
    "    per_log[\"runtime_h_std\"]  = per_log[\"runtime_s_std\"] / 3600.0\n",
    "\n",
    "# pretty labels\n",
    "per_log[\"Backbone_pretty\"] = per_log[\"backbone\"].map(BACKBONE_MAP_LLM).fillna(per_log[\"backbone\"])\n",
    "per_log[\"Mode\"] = per_log[\"Setting\"].map(MODE_MAP).fillna(per_log[\"Setting\"])\n",
    "\n",
    "# formatted columns (mean±std over seeds)\n",
    "per_log[\"NA Acc.\"] = per_log.apply(\n",
    "    lambda r: _fmt_pm(r[\"test_next_activity_acc_mean\"], r[\"test_next_activity_acc_std\"], d=4),\n",
    "    axis=1\n",
    ")\n",
    "per_log[\"RT MSE\"] = per_log.apply(\n",
    "    lambda r: _fmt_pm(r[\"test_next_remaining_time_loss_mean\"], r[\"test_next_remaining_time_loss_std\"], d=4),\n",
    "    axis=1\n",
    ")\n",
    "per_log[\"NT MSE\"] = per_log.apply(\n",
    "    lambda r: _fmt_pm(r[\"test_next_time_to_next_event_loss_mean\"], r[\"test_next_time_to_next_event_loss_std\"], d=4),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "if runtime_col is not None:\n",
    "    per_log[\"Runtime (h)\"] = per_log.apply(\n",
    "        lambda r: _fmt_pm(r[\"runtime_h_mean\"], r[\"runtime_h_std\"], d=5),\n",
    "        axis=1\n",
    "    )\n",
    "else:\n",
    "    per_log[\"Runtime (h)\"] = \"\"\n",
    "\n",
    "# output 75-row CSV\n",
    "cols_75 = [\n",
    "    \"log\", \"Backbone_pretty\", \"Mode\",\n",
    "    \"NA Acc.\", \"RT MSE\", \"NT MSE\", \"Runtime (h)\",\n",
    "    \"backbone\", \"Setting\",\n",
    "    \"test_next_activity_acc_mean\", \"test_next_activity_acc_std\",\n",
    "    \"test_next_remaining_time_loss_mean\", \"test_next_remaining_time_loss_std\",\n",
    "    \"test_next_time_to_next_event_loss_mean\", \"test_next_time_to_next_event_loss_std\",\n",
    "]\n",
    "if runtime_col is not None:\n",
    "    cols_75 += [\"runtime_h_mean\", \"runtime_h_std\"]\n",
    "\n",
    "cols_75 = [c for c in cols_75 if c in per_log.columns]\n",
    "per_log_out = per_log[cols_75].copy()\n",
    "per_log_out = per_log_out.sort_values([\"log\", \"Backbone_pretty\", \"Mode\"]).reset_index(drop=True)\n",
    "\n",
    "per_log_out.to_csv(out_csv_75, index=False)\n",
    "print(\"Saved 75-row CSV to:\", out_csv_75)\n",
    "\n",
    "# ==================== 15-row pooled table: per backbone/Setting (MEDIAN + IQR across logs) ====================\n",
    "group_pooled = [\"backbone\", \"Setting\"]\n",
    "\n",
    "pooled15 = per_log.groupby(group_pooled, dropna=False).agg(\n",
    "    # center across logs (median of per-log means)\n",
    "    test_next_activity_acc_median=(\"test_next_activity_acc_mean\", \"median\"),\n",
    "    test_next_remaining_time_loss_median=(\"test_next_remaining_time_loss_mean\", \"median\"),\n",
    "    test_next_time_to_next_event_loss_median=(\"test_next_time_to_next_event_loss_mean\", \"median\"),\n",
    "\n",
    "    # dispersion across logs (Q1/Q3/IQR of per-log means)\n",
    "    test_next_activity_acc_q1_logs=(\"test_next_activity_acc_mean\", _q1),\n",
    "    test_next_activity_acc_q3_logs=(\"test_next_activity_acc_mean\", _q3),\n",
    "    test_next_activity_acc_iqr_logs=(\"test_next_activity_acc_mean\", _iqr),\n",
    "\n",
    "    test_next_remaining_time_loss_q1_logs=(\"test_next_remaining_time_loss_mean\", _q1),\n",
    "    test_next_remaining_time_loss_q3_logs=(\"test_next_remaining_time_loss_mean\", _q3),\n",
    "    test_next_remaining_time_loss_iqr_logs=(\"test_next_remaining_time_loss_mean\", _iqr),\n",
    "\n",
    "    test_next_time_to_next_event_loss_q1_logs=(\"test_next_time_to_next_event_loss_mean\", _q1),\n",
    "    test_next_time_to_next_event_loss_q3_logs=(\"test_next_time_to_next_event_loss_mean\", _q3),\n",
    "    test_next_time_to_next_event_loss_iqr_logs=(\"test_next_time_to_next_event_loss_mean\", _iqr),\n",
    ").reset_index()\n",
    "\n",
    "# runtime pooled (optional)\n",
    "if runtime_col is not None and \"runtime_h_mean\" in per_log.columns:\n",
    "    pooled_rt = per_log.groupby(group_pooled, dropna=False).agg(\n",
    "        runtime_h_median=(\"runtime_h_mean\", \"median\"),\n",
    "        runtime_h_q1_logs=(\"runtime_h_mean\", _q1),\n",
    "        runtime_h_q3_logs=(\"runtime_h_mean\", _q3),\n",
    "        runtime_h_iqr_logs=(\"runtime_h_mean\", _iqr),\n",
    "    ).reset_index()\n",
    "    pooled15 = pooled15.merge(pooled_rt, on=group_pooled, how=\"left\")\n",
    "\n",
    "# pretty labels\n",
    "pooled15[\"Backbone_pretty\"] = pooled15[\"backbone\"].map(BACKBONE_MAP_LLM).fillna(pooled15[\"backbone\"])\n",
    "pooled15[\"Mode\"] = pooled15[\"Setting\"].map(MODE_MAP).fillna(pooled15[\"Setting\"])\n",
    "\n",
    "# formatted columns (median [Q1, Q3] across logs)\n",
    "pooled15[\"NA Acc.\"] = pooled15.apply(\n",
    "    lambda r: _fmt_median_q1q3(r[\"test_next_activity_acc_median\"], r[\"test_next_activity_acc_q1_logs\"], r[\"test_next_activity_acc_q3_logs\"], d=4),\n",
    "    axis=1\n",
    ")\n",
    "pooled15[\"RT MSE\"] = pooled15.apply(\n",
    "    lambda r: _fmt_median_q1q3(r[\"test_next_remaining_time_loss_median\"], r[\"test_next_remaining_time_loss_q1_logs\"], r[\"test_next_remaining_time_loss_q3_logs\"], d=4),\n",
    "    axis=1\n",
    ")\n",
    "pooled15[\"NT MSE\"] = pooled15.apply(\n",
    "    lambda r: _fmt_median_q1q3(r[\"test_next_time_to_next_event_loss_median\"], r[\"test_next_time_to_next_event_loss_q1_logs\"], r[\"test_next_time_to_next_event_loss_q3_logs\"], d=4),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "if runtime_col is not None and \"runtime_h_median\" in pooled15.columns:\n",
    "    pooled15[\"Runtime (h)\"] = pooled15.apply(\n",
    "        lambda r: _fmt_median_q1q3(r[\"runtime_h_median\"], r[\"runtime_h_q1_logs\"], r[\"runtime_h_q3_logs\"], d=5),\n",
    "        axis=1\n",
    "    )\n",
    "else:\n",
    "    pooled15[\"Runtime (h)\"] = \"\"\n",
    "\n",
    "# output 15-row CSV\n",
    "cols_15 = [\n",
    "    \"Backbone_pretty\", \"Mode\",\n",
    "    \"NA Acc.\", \"RT MSE\", \"NT MSE\", \"Runtime (h)\",\n",
    "    \"backbone\", \"Setting\",\n",
    "\n",
    "    \"test_next_activity_acc_median\",\n",
    "    \"test_next_activity_acc_q1_logs\", \"test_next_activity_acc_q3_logs\", \"test_next_activity_acc_iqr_logs\",\n",
    "\n",
    "    \"test_next_remaining_time_loss_median\",\n",
    "    \"test_next_remaining_time_loss_q1_logs\", \"test_next_remaining_time_loss_q3_logs\", \"test_next_remaining_time_loss_iqr_logs\",\n",
    "\n",
    "    \"test_next_time_to_next_event_loss_median\",\n",
    "    \"test_next_time_to_next_event_loss_q1_logs\", \"test_next_time_to_next_event_loss_q3_logs\", \"test_next_time_to_next_event_loss_iqr_logs\",\n",
    "]\n",
    "if runtime_col is not None and \"runtime_h_median\" in pooled15.columns:\n",
    "    cols_15 += [\"runtime_h_median\", \"runtime_h_q1_logs\", \"runtime_h_q3_logs\", \"runtime_h_iqr_logs\"]\n",
    "\n",
    "cols_15 = [c for c in cols_15 if c in pooled15.columns]\n",
    "pooled15_out = pooled15[cols_15].copy()\n",
    "pooled15_out = pooled15_out.sort_values([\"Backbone_pretty\", \"Mode\"]).reset_index(drop=True)\n",
    "\n",
    "pooled15_out.to_csv(out_csv_15, index=False)\n",
    "print(\"Saved pooled 15-row CSV (median + IQR) to:\", out_csv_15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701bdf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "KEEP_SETTINGS = [\"ZeroShot\", \"LoRA\", \"FewShot-LoRA\"]\n",
    "\n",
    "BACKBONE_MAP_LLM = {\n",
    "    \"gpt2\":         \"GPT2\",\n",
    "    \"gptneo-1b3\":   \"GPT-Neo-1.3B\",\n",
    "    \"qwen25-05b\":   \"Qwen2.5-0.5B\",\n",
    "    \"llama32-1b\":   \"Llama3.2-1B\",\n",
    "    \"gemma-2-2b\":   \"Gemma-2-2B\",\n",
    "}\n",
    "\n",
    "PLOTS = [\n",
    "    (\"test_next_activity_acc_mean\",            \"NA Acc.\"),\n",
    "    (\"test_next_remaining_time_loss_mean\",     \"RT MSE\"),\n",
    "    (\"test_next_time_to_next_event_loss_mean\", \"NT MSE\"),\n",
    "]\n",
    "\n",
    "# input CSV from the code above\n",
    "in_csv_75 = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/rq3_llm_tradeoff_per_log_75_selected_hp.csv\"\n",
    "per_log = pd.read_csv(in_csv_75)\n",
    "\n",
    "# filter just in case\n",
    "per_log = per_log[\n",
    "    per_log[\"backbone\"].isin(LLM_BACKBONES) & per_log[\"Setting\"].isin(KEEP_SETTINGS)\n",
    "].copy()\n",
    "\n",
    "# ensure pretty names exist\n",
    "if \"Backbone_pretty\" not in per_log.columns:\n",
    "    per_log[\"Backbone_pretty\"] = per_log[\"backbone\"].map(BACKBONE_MAP_LLM).fillna(per_log[\"backbone\"])\n",
    "\n",
    "backbone_order = [BACKBONE_MAP_LLM[b] for b in LLM_BACKBONES if b in per_log[\"backbone\"].unique()]\n",
    "\n",
    "def annotate_medians_top(ax, data: pd.DataFrame, x_col: str, y_col: str, order: list[str], fmt: str = \"{:.4f}\"):\n",
    "    meds = data.groupby(x_col)[y_col].median()\n",
    "    for i, cat in enumerate(order):\n",
    "        if cat not in meds.index:\n",
    "            continue\n",
    "        val = meds.loc[cat]\n",
    "        if pd.isna(val):\n",
    "            continue\n",
    "        ax.text(\n",
    "            i, 1.02, fmt.format(float(val)),\n",
    "            transform=ax.get_xaxis_transform(),  # x=data, y=axes fraction\n",
    "            ha=\"center\", va=\"bottom\",\n",
    "            fontsize=9,\n",
    "            bbox=dict(facecolor=\"white\", edgecolor=\"none\", alpha=0.85, pad=1.5),\n",
    "            clip_on=False,\n",
    "        )\n",
    "\n",
    "plots_base_dir = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset\"\n",
    "out_dir = os.path.join(plots_base_dir, \"all_datasets\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "for setting in KEEP_SETTINGS:\n",
    "    df_s = per_log[per_log[\"Setting\"] == setting].copy()\n",
    "    if df_s.empty:\n",
    "        continue\n",
    "\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(8, 9), sharex=True)\n",
    "\n",
    "    for ax, (ycol, ylabel) in zip(axes, PLOTS):\n",
    "        if ycol not in df_s.columns:\n",
    "            raise ValueError(f\"Missing column '{ycol}' in {in_csv_75}. Did you generate the CSV first?\")\n",
    "\n",
    "        sns.boxplot(\n",
    "            data=df_s,\n",
    "            x=\"Backbone_pretty\",\n",
    "            y=ycol,\n",
    "            order=backbone_order,\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "        # median labels (these medians == pooled15 medians by construction)\n",
    "        annotate_medians_top(\n",
    "            ax=ax,\n",
    "            data=df_s,\n",
    "            x_col=\"Backbone_pretty\",\n",
    "            y_col=ycol,\n",
    "            order=backbone_order,\n",
    "            fmt=\"{:.4f}\",\n",
    "        )\n",
    "\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.set_xticklabels(backbone_order, rotation=45, ha=\"right\")\n",
    "\n",
    "    axes[-1].set_xlabel(\"LLM backbone\")\n",
    "    fig.suptitle(f\"All datasets – {setting}\", fontsize=12)\n",
    "    plt.tight_layout(rect=(0, 0, 1, 0.96))\n",
    "\n",
    "    out_path = os.path.join(out_dir, f\"llm_backbones_boxplot_ALL_{setting}_from_csv.png\")\n",
    "    save_png_and_pdf(fig, out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved pooled boxplot (from CSV table) for setting={setting} to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8141b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "\n",
    "multi_path = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/csv/multi_task_results_r256_a512.csv\"\n",
    "llm = pd.read_csv(multi_path)\n",
    "\n",
    "if \"Setting\" not in llm.columns:\n",
    "    raise ValueError(\"Column 'Setting' not found in multi_task_results_r256_a512.csv\")\n",
    "\n",
    "# keep only LLM backbones + selected settings\n",
    "llm = llm[llm[\"backbone\"].isin(LLM_BACKBONES)].copy()\n",
    "\n",
    "KEEP_SETTINGS = [\"ZeroShot\", \"LoRA\", \"FewShot-LoRA\"]\n",
    "llm = llm[llm[\"Setting\"].isin(KEEP_SETTINGS)].copy()\n",
    "\n",
    "llm[\"Setting_main\"] = llm[\"Setting\"]\n",
    "\n",
    "plots_base_dir = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset\"\n",
    "\n",
    "SETTING_ORDER_FULL = [\n",
    "    \"ZeroShot\",\n",
    "    \"FewShot-LoRA\",\n",
    "    \"LoRA\",\n",
    "]\n",
    "\n",
    "SETTING_ORDER_MAIN = [\n",
    "    \"ZeroShot\",\n",
    "    \"FewShot-LoRA\",\n",
    "    \"LoRA\",\n",
    "]\n",
    "\n",
    "PLOTS = [\n",
    "    (\"test_next_activity_acc\",            \"NA Acc.\"),\n",
    "    (\"test_next_remaining_time_loss\",     \"RT MSE\"),\n",
    "    (\"test_next_time_to_next_event_loss\", \"NT MSE\"),\n",
    "]\n",
    "\n",
    "def _get_mean_std(df_one: pd.DataFrame, metric: str) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Extract (mean, std) for a metric from an aggregated table row.\n",
    "    Supports either:\n",
    "      - columns: f\"{metric}_mean\", f\"{metric}_std\"\n",
    "      - or column: f\"{metric}_mean_std\" formatted like \"0.1234 ± 0.0056\"\n",
    "      - or (fallback) raw metric column (std=0)\n",
    "    \"\"\"\n",
    "    mean_col = f\"{metric}_mean\"\n",
    "    std_col = f\"{metric}_std\"\n",
    "    mean_std_col = f\"{metric}_mean_std\"\n",
    "\n",
    "    if mean_col in df_one.columns:\n",
    "        mean = float(df_one[mean_col].iloc[0])\n",
    "        std = float(df_one[std_col].iloc[0]) if std_col in df_one.columns and pd.notna(df_one[std_col].iloc[0]) else 0.0\n",
    "        return mean, std\n",
    "\n",
    "    if mean_std_col in df_one.columns:\n",
    "        s = str(df_one[mean_std_col].iloc[0])\n",
    "        if \"±\" in s:\n",
    "            left, right = s.split(\"±\", 1)\n",
    "            return float(left.strip()), float(right.strip())\n",
    "        # if it's just a number string\n",
    "        return float(s.strip()), 0.0\n",
    "\n",
    "    if metric in df_one.columns:\n",
    "        return float(df_one[metric].iloc[0]), 0.0\n",
    "\n",
    "    raise KeyError(\n",
    "        f\"Could not find '{mean_col}'/'{std_col}' or '{mean_std_col}' (or '{metric}') in the aggregated CSV.\"\n",
    "    )\n",
    "\n",
    "for log_name, df_log in llm.groupby(\"log\"):\n",
    "    log_dir = os.path.join(plots_base_dir, log_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    for backbone, df_b in df_log.groupby(\"backbone\"):\n",
    "\n",
    "        settings_full = [s for s in SETTING_ORDER_FULL if s in df_b[\"Setting\"].unique()]\n",
    "        if not settings_full:\n",
    "            continue\n",
    "\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(8, 9), sharex=True)\n",
    "\n",
    "        x_labels = settings_full\n",
    "        x = np.arange(len(x_labels))\n",
    "\n",
    "        for ax, (metric, ylabel) in zip(axes, PLOTS):\n",
    "            means = []\n",
    "            stds = []\n",
    "\n",
    "            for s in x_labels:\n",
    "                row = df_b[df_b[\"Setting\"] == s]\n",
    "                if row.empty:\n",
    "                    means.append(np.nan)\n",
    "                    stds.append(0.0)\n",
    "                else:\n",
    "                    m, sd = _get_mean_std(row, metric)\n",
    "                    means.append(m)\n",
    "                    stds.append(sd)\n",
    "\n",
    "            # drop missing entries (should rarely happen if your table is complete)\n",
    "            keep = [i for i, v in enumerate(means) if pd.notna(v)]\n",
    "            x_k = x[keep]\n",
    "            means_k = [means[i] for i in keep]\n",
    "            stds_k = [stds[i] for i in keep]\n",
    "            labels_k = [x_labels[i] for i in keep]\n",
    "\n",
    "            ax.bar(x_k, means_k, yerr=stds_k, capsize=4)\n",
    "            ax.set_ylabel(ylabel)\n",
    "            ax.set_xticks(x_k)\n",
    "            ax.set_xticklabels(labels_k, rotation=45, ha=\"right\")\n",
    "\n",
    "        axes[-1].set_xlabel(\"LLM adaptation setting\")\n",
    "        fig.suptitle(f\"{log_name} – {backbone}\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        out_path = os.path.join(log_dir, f\"llm_methods_boxplot_selected_{log_name}_{backbone}.png\")\n",
    "        save_png_and_pdf(fig, out_path, dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "        print(f\"Saved selected-methods boxplot for log={log_name}, backbone={backbone} to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e7ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "\n",
    "llm = df[df[\"backbone\"].isin(LLM_BACKBONES)].copy()\n",
    "llm[\"Setting\"] = llm.apply(map_setting, axis=1)\n",
    "\n",
    "# Keep only freezing variants\n",
    "llm_freezing = llm[\n",
    "    (llm[\"Setting\"] == \"Freezing\") | (llm[\"Setting\"].astype(str).str.startswith(\"Freezing-\"))\n",
    "].copy()\n",
    "llm_freezing = llm_freezing[llm_freezing[\"Setting\"] != \"FewShot-Freezing\"].copy()\n",
    "\n",
    "plots_base_dir = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset\"\n",
    "\n",
    "# Order as in your example plot (will be filtered to what exists per run)\n",
    "FREEZING_ORDER = [\n",
    "    \"Freezing\",\n",
    "    \"Freezing-[-1]\",\n",
    "    \"Freezing-[0]\",\n",
    "    \"Freezing-[0, 1]\",\n",
    "    \"Freezing-[-1, -2]\",\n",
    "]\n",
    "\n",
    "PLOTS = [\n",
    "    (\"test_next_activity_acc\",            \"NA Acc.\"),\n",
    "    (\"test_next_remaining_time_loss\",     \"RT MSE\"),\n",
    "    (\"test_next_time_to_next_event_loss\", \"NT MSE\"),\n",
    "]\n",
    "\n",
    "for log_name, df_log in llm_freezing.groupby(\"log\"):\n",
    "    log_dir = os.path.join(plots_base_dir, log_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    for backbone, df_b in df_log.groupby(\"backbone\"):\n",
    "        # keep only the freezing settings that are present for this (log, backbone)\n",
    "        settings = [s for s in FREEZING_ORDER if s in df_b[\"Setting\"].unique()]\n",
    "        # fallback: if something unexpected appears, include it (stable alphabetical after the known ones)\n",
    "        extra = sorted([s for s in df_b[\"Setting\"].unique() if s not in settings])\n",
    "        settings = settings + extra\n",
    "\n",
    "        if not settings:\n",
    "            continue\n",
    "\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(8, 9), sharex=True)\n",
    "\n",
    "        for ax, (metric, ylabel) in zip(axes, PLOTS):\n",
    "            sns.boxplot(\n",
    "                data=df_b,\n",
    "                x=\"Setting\",\n",
    "                y=metric,\n",
    "                order=settings,\n",
    "                ax=ax,\n",
    "            )\n",
    "            ax.set_ylabel(ylabel)\n",
    "            ax.set_xticks(range(len(settings)))\n",
    "            ax.set_xticklabels(settings, rotation=45, ha=\"right\")\n",
    "\n",
    "        axes[-1].set_xlabel(\"Freezing configuration\")\n",
    "\n",
    "        fig.suptitle(f\"{log_name} – {backbone} (Freezing variants)\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        out_path = os.path.join(\n",
    "            log_dir,\n",
    "            f\"llm_methods_boxplot_freezing_{log_name}_{backbone}.png\"\n",
    "        )\n",
    "        save_png_and_pdf(fig, out_path, dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "        print(f\"Saved Freezing-only boxplot for log={log_name}, backbone={backbone} to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bda212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_single(\n",
    "    wandb_id: str,\n",
    "    targets=[\"na\", \"rt\", \"nt\"],\n",
    "    project_name: str | None = None,\n",
    "    entity: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Holt Verlaufskurven (pro Epoch) für einen einzelnen W&B-Run.\n",
    "    Gibt (na_acc, na_loss, rt_loss, nt_loss) als Listen zurück.\n",
    "    Fehlende Targets -> entsprechende Liste = None.\n",
    "    \"\"\"\n",
    "    if isinstance(targets, str):\n",
    "        targets = [targets]\n",
    "\n",
    "    if project_name is None:\n",
    "        raise ValueError(\"fetch_single requires an explicit project_name.\")\n",
    "\n",
    "    if entity is None:\n",
    "        entity = os.environ.get(\"ENTITY\")\n",
    "        if entity is None:\n",
    "            raise ValueError(\"ENTITY not set and no entity passed to fetch_single().\")\n",
    "\n",
    "    api = wandb.Api()\n",
    "    run = api.run(f\"{entity}/{project_name}/{wandb_id}\")\n",
    "    history = list(run.scan_history())\n",
    "\n",
    "    na_acc, na_loss, rt_loss, nt_loss = None, None, None, None\n",
    "\n",
    "    if \"rt\" in targets:\n",
    "        rt_loss = [\n",
    "            row[\"test_next_remaining_time_loss\"]\n",
    "            for row in history\n",
    "            if \"test_next_remaining_time_loss\" in row\n",
    "        ]\n",
    "\n",
    "    if \"na\" in targets:\n",
    "        na_loss = [\n",
    "            row[\"test_next_activity_loss\"]\n",
    "            for row in history\n",
    "            if \"test_next_activity_loss\" in row\n",
    "        ]\n",
    "        na_acc = [\n",
    "            row[\"test_next_activity_acc\"]\n",
    "            for row in history\n",
    "            if \"test_next_activity_acc\" in row\n",
    "        ]\n",
    "\n",
    "    if \"nt\" in targets:\n",
    "        nt_loss = [\n",
    "            row[\"test_next_time_to_next_event_loss\"]\n",
    "            for row in history\n",
    "            if \"test_next_time_to_next_event_loss\" in row\n",
    "        ]\n",
    "\n",
    "    return na_acc, na_loss, rt_loss, nt_loss\n",
    "\n",
    "\n",
    "# Pfad für Loss-Curves-CSV\n",
    "loss_csv_path = os.path.join(output_dir_csv, \"loss_curves_multitask_lora_best.csv\")\n",
    "\n",
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "\n",
    "if os.path.exists(loss_csv_path):\n",
    "    losses = pd.read_csv(loss_csv_path)\n",
    "else:\n",
    "    # Nur LLM + LoRA\n",
    "    df_lora = df[\n",
    "        df[\"backbone\"].isin(LLM_BACKBONES)\n",
    "        & (df[\"fine_tuning\"] == \"lora\")\n",
    "    ].copy()\n",
    "\n",
    "    # Nur \"Full-LoRA\" (keine Few-Shot-LoRA)\n",
    "    if \"few_shot_k\" in df_lora.columns:\n",
    "        df_lora = df_lora[df_lora[\"few_shot_k\"].isna()].copy()\n",
    "\n",
    "    # Score-Spalte für beste Runs\n",
    "    score_col = \"mt_score\"\n",
    "    if score_col not in df_lora.columns:\n",
    "        score_col = \"test_next_activity_acc\"\n",
    "\n",
    "    # Bester LoRA-Run pro (log, backbone)\n",
    "    best_runs = (\n",
    "        df_lora\n",
    "        .sort_values(score_col, ascending=False)\n",
    "        .groupby([\"log\", \"backbone\"], as_index=False)\n",
    "        .head(1)\n",
    "    )\n",
    "\n",
    "    losses_list = []\n",
    "\n",
    "    for _, row in best_runs.iterrows():\n",
    "        na_acc, na_loss, rt_loss, nt_loss = fetch_single(\n",
    "            wandb_id=row[\"id\"],\n",
    "            project_name=row[\"project\"],\n",
    "            entity=entity,\n",
    "            targets=[\"na\", \"rt\", \"nt\"],\n",
    "        )\n",
    "\n",
    "        # falls etwas fehlt → überspringen\n",
    "        if na_loss is None or rt_loss is None or nt_loss is None:\n",
    "            continue\n",
    "\n",
    "        tmp = pd.DataFrame({\n",
    "            \"epoch\": range(len(na_loss)),\n",
    "            \"na_acc\": na_acc,\n",
    "            \"na_loss\": na_loss,\n",
    "            \"rt_loss\": rt_loss,\n",
    "            \"nt_loss\": nt_loss,\n",
    "        })\n",
    "        tmp[\"log\"] = row[\"log\"]\n",
    "        tmp[\"backbone\"] = row[\"backbone\"]\n",
    "        losses_list.append(tmp)\n",
    "\n",
    "    if not losses_list:\n",
    "        raise RuntimeError(\"Keine Loss-Curves für LoRA-Runs gefunden.\")\n",
    "\n",
    "    losses = pd.concat(losses_list, axis=0, ignore_index=True)\n",
    "    losses.to_csv(loss_csv_path, index=False)\n",
    "    print(\"Saved LoRA loss curves to:\", loss_csv_path)\n",
    "\n",
    "print(\"Loss curves shape (LoRA best runs):\", losses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd653b",
   "metadata": {
    "title": "Loss curve visualization (multi-task)"
   },
   "outputs": [],
   "source": [
    "LOGS_TO_PLOT = sorted(losses[\"log\"].unique())\n",
    "\n",
    "HUE_MAP = {\n",
    "    \"gpt2\":         \"GPT2\",\n",
    "    \"gptneo-1b3\":   \"GPT-Neo-1.3B\",\n",
    "    \"qwen25-05b\":   \"Qwen2.5-0.5B\",\n",
    "    \"llama32-1b\":   \"Llama3.2-1B\",\n",
    "    \"gemma-2-2b\":   \"Gemma-2-2B\",\n",
    "}\n",
    "\n",
    "HUE_ORDER = [\n",
    "    \"GPT2\",\n",
    "    \"GPT-Neo-1.3B\",\n",
    "    \"Qwen2.5-0.5B\",\n",
    "    \"Llama3.2-1B\",\n",
    "    \"Gemma-2-2B\",\n",
    "]\n",
    "\n",
    "# Long-Format\n",
    "l = losses.melt(\n",
    "    id_vars=[\"log\", \"backbone\", \"epoch\"],\n",
    "    value_vars=[\"na_loss\", \"rt_loss\", \"nt_loss\"],\n",
    "    var_name=\"Loss\",\n",
    "    value_name=\"Value\",\n",
    ").dropna(subset=[\"Value\"])\n",
    "\n",
    "l[\"Backbone\"] = l[\"backbone\"].map(HUE_MAP)\n",
    "l = l[l[\"Backbone\"].notna()]\n",
    "\n",
    "LOSS_LABELS = {\n",
    "    \"na_loss\": \"NA Loss\",\n",
    "    \"rt_loss\": \"RT Loss\",\n",
    "    \"nt_loss\": \"NT Loss\",\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    3, len(LOGS_TO_PLOT),\n",
    "    figsize=(4 * len(LOGS_TO_PLOT), 8),\n",
    "    sharex=True\n",
    ")\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "legend_handles, legend_labels = None, None  # globale Legende\n",
    "\n",
    "for loss_name in [\"na_loss\", \"rt_loss\", \"nt_loss\"]:\n",
    "    for log_name in LOGS_TO_PLOT:\n",
    "        ax = next(axes_iter)\n",
    "        tmp = l[(l[\"Loss\"] == loss_name) & (l[\"log\"] == log_name)]\n",
    "\n",
    "        sns.lineplot(\n",
    "            data=tmp,\n",
    "            x=\"epoch\",\n",
    "            y=\"Value\",\n",
    "            hue=\"Backbone\",\n",
    "            hue_order=[h for h in HUE_ORDER if h in tmp[\"Backbone\"].unique()],\n",
    "            ax=ax,\n",
    "            linewidth=2.0,\n",
    "        )\n",
    "\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(LOSS_LABELS[loss_name])\n",
    "        ax.set_title(log_name)\n",
    "\n",
    "        # Legend nur einmal abgreifen\n",
    "        leg = ax.get_legend()\n",
    "        if leg is not None:\n",
    "            handles, labels = leg.legend_handles, [t.get_text() for t in leg.get_texts()]\n",
    "            legend_handles, legend_labels = handles, labels\n",
    "            leg.remove()\n",
    "\n",
    "# globale Legende unter der Figure\n",
    "if legend_handles is not None:\n",
    "    fig.legend(\n",
    "        legend_handles,\n",
    "        legend_labels,\n",
    "        title=\"\",\n",
    "        loc=\"lower center\",\n",
    "        ncol=len(legend_labels),\n",
    "        bbox_to_anchor=(0.5, -0.02),\n",
    "    )\n",
    "\n",
    "plt.tight_layout(rect=(0, 0.05, 1, 1))  # unten Platz für Legende lassen\n",
    "\n",
    "plot_path = os.path.join(output_dir_plots, \"loss_curves_multitask_lora_best.png\")\n",
    "save_png_and_pdf(fig, out_path, dpi=300)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"Saved LoRA loss curve plot to:\", plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b476c2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# PARAMETER-SUMMARY \n",
    "\n",
    "multi_path = os.path.join(output_dir_csv, \"multi_task_benchmark_results.csv\")\n",
    "multi = pd.read_csv(multi_path)\n",
    "\n",
    "# Nur Zeilen mit Parameterinfos\n",
    "param_summary = (\n",
    "    multi[\n",
    "        [\n",
    "            \"log\",\n",
    "            \"backbone\",\n",
    "            \"Setting\",\n",
    "            \"total_params\",\n",
    "            \"trainable_params\",\n",
    "        ]\n",
    "    ]\n",
    "    .dropna(subset=[\"total_params\", \"trainable_params\"])\n",
    "    .drop_duplicates()\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "# Anteil trainierbarer Parameter in %\n",
    "param_summary[\"trainable_percent\"] = (\n",
    "    param_summary[\"trainable_params\"] / param_summary[\"total_params\"] * 100.0\n",
    ")\n",
    "\n",
    "param_summary[\"trainable_percent_fmt\"] = (\n",
    "    param_summary[\"trainable_percent\"].round(1).astype(str) + \"%\"\n",
    ")\n",
    "\n",
    "# total_params schön formatiert (wissenschaftliche Notation)\n",
    "param_summary[\"total_params_fmt\"] = param_summary[\"total_params\"].apply(\n",
    "    lambda x: np.format_float_scientific(x, precision=1)\n",
    ")\n",
    "\n",
    "param_summary[\"# params\\n(%trainable)\"] = (\n",
    "    param_summary[\"total_params_fmt\"]\n",
    "    + \" (\"\n",
    "    + param_summary[\"trainable_percent_fmt\"]\n",
    "    + \")\"\n",
    ")\n",
    "\n",
    "# falls vorhanden, Dataset & hübsche Namen mitnehmen\n",
    "if \"Dataset\" in multi.columns:\n",
    "    param_summary[\"Dataset\"] = multi.set_index(\n",
    "        [\"log\", \"backbone\", \"Setting\"]\n",
    "    ).loc[\n",
    "        param_summary.set_index([\"log\", \"backbone\", \"Setting\"]).index,\n",
    "        \"Dataset\"\n",
    "    ].values\n",
    "else:\n",
    "    param_summary[\"Dataset\"] = param_summary[\"log\"]\n",
    "\n",
    "if \"Backbone_pretty\" in multi.columns:\n",
    "    param_summary[\"Backbone_pretty\"] = multi.set_index(\n",
    "        [\"log\", \"backbone\", \"Setting\"]\n",
    "    ).loc[\n",
    "        param_summary.set_index([\"log\", \"backbone\", \"Setting\"]).index,\n",
    "        \"Backbone_pretty\"\n",
    "    ].values\n",
    "else:\n",
    "    param_summary[\"Backbone_pretty\"] = param_summary[\"backbone\"]\n",
    "\n",
    "# --- pro Datensatz (log) speichern ---\n",
    "for log_name, df_log in param_summary.groupby(\"log\"):\n",
    "    log_dir = os.path.join(output_dir_csv, \"per_dataset\", log_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    csv_path = os.path.join(log_dir, \"param_summary_multitask.csv\")\n",
    "    df_log.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved param summary for {log_name} to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f4bdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PARETO: MT-Score vs. trainierbare Parameter (LoRA, best per LLM-Backbone) ===\n",
    "\n",
    "multi_path = os.path.join(output_dir_csv, \"multi_task_benchmark_results.csv\")\n",
    "multi = pd.read_csv(multi_path)\n",
    "\n",
    "# Relevante LLM-Backbones + Pretty Names\n",
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "BACKBONE_MAP_LLM = {\n",
    "    \"gpt2\":         \"GPT2\",\n",
    "    \"gptneo-1b3\":   \"GPT-Neo-1.3B\",\n",
    "    \"qwen25-05b\":   \"Qwen2.5-0.5B\",\n",
    "    \"llama32-1b\":   \"Llama3.2-1B\",\n",
    "    \"gemma-2-2b\":   \"Gemma-2-2B\",\n",
    "}\n",
    "\n",
    "# Nur LLM + LoRA + benötigte Spalten\n",
    "pareto_source = multi[\n",
    "    (multi[\"backbone\"].isin(LLM_BACKBONES))\n",
    "    & (multi[\"Setting\"] == \"LoRA\")\n",
    "    & multi[\"trainable_params\"].notna()\n",
    "    & multi[\"mt_score_mean\"].notna()\n",
    "].copy()\n",
    "\n",
    "# Pretty-Namen ergänzen (falls noch nicht vorhanden)\n",
    "if \"Backbone_pretty\" not in pareto_source.columns:\n",
    "    pareto_source[\"Backbone_pretty\"] = (\n",
    "        pareto_source[\"backbone\"]\n",
    "        .map(BACKBONE_MAP_LLM)\n",
    "        .fillna(pareto_source[\"backbone\"])\n",
    "    )\n",
    "\n",
    "plots_base_dir = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset\"\n",
    "\n",
    "for log_name, df_log in pareto_source.groupby(\"log\"):\n",
    "    # pro Datensatz: bester LoRA-Run je Backbone\n",
    "    df_best = (\n",
    "        df_log\n",
    "        .sort_values(\n",
    "            [\"backbone\", \"mt_score_mean\", \"trainable_params\"],\n",
    "            ascending=[True, False, True],  # Score ↓, bei Tie weniger Params ↑\n",
    "        )\n",
    "        .drop_duplicates(subset=[\"backbone\"], keep=\"first\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    if df_best.empty:\n",
    "        continue\n",
    "\n",
    "    # etwas breiter, damit die Legend unten gut passt\n",
    "    fig, ax = plt.subplots(figsize=(9, 4.5))\n",
    "\n",
    "    # Scatter: ein Punkt pro Backbone\n",
    "    for bb, df_b in df_best.groupby(\"backbone\"):\n",
    "        label = df_b[\"Backbone_pretty\"].iloc[0]\n",
    "        ax.scatter(\n",
    "            df_b[\"trainable_params\"],\n",
    "            df_b[\"mt_score_mean\"],\n",
    "            label=label,\n",
    "            s=70,\n",
    "        )\n",
    "\n",
    "    # Pareto-Front hervorheben (min trainable_params, max mt_score_mean)\n",
    "    df_pf = df_best.sort_values(\"trainable_params\")\n",
    "    best_so_far = -np.inf\n",
    "    pareto_mask = []\n",
    "    for _, row in df_pf.iterrows():\n",
    "        if row[\"mt_score_mean\"] >= best_so_far - 1e-9:\n",
    "            pareto_mask.append(True)\n",
    "            best_so_far = row[\"mt_score_mean\"]\n",
    "        else:\n",
    "            pareto_mask.append(False)\n",
    "    df_pf_pareto = df_pf[pareto_mask]\n",
    "\n",
    "    if not df_pf_pareto.empty:\n",
    "        ax.scatter(\n",
    "            df_pf_pareto[\"trainable_params\"],\n",
    "            df_pf_pareto[\"mt_score_mean\"],\n",
    "            s=140,\n",
    "            facecolors=\"none\",\n",
    "            edgecolors=\"black\",\n",
    "            linewidths=1.5,\n",
    "        )\n",
    "\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(\"Model size (LoRA, log scale)\")\n",
    "    ax.set_ylabel(\"MT-Score (mean across seeds)\")\n",
    "    ax.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "    # Titel: wenn Dataset-Spalte existiert, nimm diese Bezeichnung\n",
    "    if \"Dataset\" in df_best.columns:\n",
    "        ds_label = df_best[\"Dataset\"].iloc[0]\n",
    "    else:\n",
    "        ds_label = log_name\n",
    "    ax.set_title(f\"{ds_label} – LLM LoRA Pareto (best per backbone)\")\n",
    "\n",
    "    # Legend unten über die ganze Breite\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(\n",
    "        handles,\n",
    "        labels,\n",
    "        title=\"Backbone\",\n",
    "        loc=\"lower center\",\n",
    "        ncol=len(labels),\n",
    "        frameon=False,\n",
    "        bbox_to_anchor=(0.5, -0.02),\n",
    "    )\n",
    "\n",
    "    # Platz für die Legend unten lassen\n",
    "    plt.tight_layout(rect=(0, 0.12, 1, 1))\n",
    "\n",
    "    log_dir = os.path.join(plots_base_dir, log_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    out_path = os.path.join(log_dir, f\"pareto_llm_lora_{log_name}.png\")\n",
    "\n",
    "    save_png_and_pdf(fig, out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved Pareto plot for log={log_name} to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb02c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_BACKBONES = [\"gpt2\", \"gptneo-1b3\", \"qwen25-05b\", \"llama32-1b\", \"gemma-2-2b\"]\n",
    "BACKBONE_MAP_LLM = {\n",
    "    \"gpt2\":         \"GPT2\",\n",
    "    \"gptneo-1b3\":   \"GPT-Neo-1.3B\",\n",
    "    \"qwen25-05b\":   \"Qwen2.5-0.5B\",\n",
    "    \"llama32-1b\":   \"Llama3.2-1B\",\n",
    "    \"gemma-2-2b\":   \"Gemma-2-2B\",\n",
    "}\n",
    "\n",
    "# Datensatz-Kürzel (wie bisher)\n",
    "DATASET_MAP = {\n",
    "    \"BPI12\": \"BPI12\",\n",
    "    \"BPI17\": \"BPI17\",\n",
    "    \"BPI20PrepaidTravelCosts\": \"BPI20PTC\",\n",
    "    \"BPI20RequestForPayment\": \"BPI20RfP\",\n",
    "    \"BPI20TravelPermitData\": \"BPI20TPD\",\n",
    "}\n",
    "\n",
    "# 1) Alle LLM-LoRA-Runs aus den Roh-Runs df ziehen\n",
    "lora_all = df[\n",
    "    df[\"backbone\"].isin(LLM_BACKBONES)\n",
    "    & (df[\"fine_tuning\"] == \"lora\")\n",
    "].copy()\n",
    "\n",
    "# Voll-LoRA (ohne Few-Shot-LoRA)\n",
    "if \"few_shot_k\" in lora_all.columns:\n",
    "    lora_all = lora_all[lora_all[\"few_shot_k\"].isna()].copy()\n",
    "\n",
    "# nur Zeilen mit Parametern & MT-Score\n",
    "if \"mt_score\" not in lora_all.columns:\n",
    "    raise ValueError(\"Spalte 'mt_score' fehlt in df – bitte sicherstellen, dass sie vorher berechnet wird.\")\n",
    "\n",
    "lora_all = lora_all[\n",
    "    lora_all[\"trainable_params\"].notna()\n",
    "    & lora_all[\"mt_score\"].notna()\n",
    "].copy()\n",
    "\n",
    "if lora_all.empty:\n",
    "    print(\"Keine LoRA-Sweeps mit trainable_params + mt_score gefunden – Pareto-Front wird übersprungen.\")\n",
    "else:\n",
    "    # 2) HParam-Kombi definieren (deine Sweep-Parameter)\n",
    "    HP_SWEEP_COLS = [\n",
    "        \"lr\",\n",
    "        \"batch_size\",\n",
    "        \"epochs\",\n",
    "        \"r\",\n",
    "        \"lora_alpha\",\n",
    "        \"embedding_size\",\n",
    "        \"hidden_size\",\n",
    "        \"strategy\",\n",
    "    ]\n",
    "    HP_SWEEP_COLS = [c for c in HP_SWEEP_COLS if c in lora_all.columns]\n",
    "\n",
    "    group_cols_sweep = [\"log\", \"backbone\"] + HP_SWEEP_COLS\n",
    "\n",
    "    # 3) Über Seeds mitteln: MT-Score + trainable_params\n",
    "    lora_sweeps_grouped = (\n",
    "        lora_all\n",
    "        .groupby(group_cols_sweep, dropna=False)\n",
    "        .agg(\n",
    "            mt_score_mean=(\"mt_score\", \"mean\"),\n",
    "            mt_score_std=(\"mt_score\", \"std\"),\n",
    "            trainable_params=(\"trainable_params\", \"mean\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Backbone-Label\n",
    "    lora_sweeps_grouped[\"Backbone_pretty\"] = (\n",
    "        lora_sweeps_grouped[\"backbone\"]\n",
    "        .map(BACKBONE_MAP_LLM)\n",
    "        .fillna(lora_sweeps_grouped[\"backbone\"])\n",
    "    )\n",
    "\n",
    "    plots_base_dir = \"/ceph/lfertig/Thesis/notebook/llm-peft-ppm/results/plots/per_dataset\"\n",
    "\n",
    "    # 4) Pro Datensatz: All-Sweeps + echte Pareto-Front\n",
    "    for log_name, df_log in lora_sweeps_grouped.groupby(\"log\"):\n",
    "        if df_log.empty:\n",
    "            continue\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "        # --- alle Sweeps: Linien je Backbone (wie zuvor) ---\n",
    "        for bb, df_b in df_log.groupby(\"backbone\"):\n",
    "            df_b = df_b.sort_values(\"trainable_params\")\n",
    "            label = df_b[\"Backbone_pretty\"].iloc[0]\n",
    "\n",
    "            ax.plot(\n",
    "                df_b[\"trainable_params\"],\n",
    "                df_b[\"mt_score_mean\"],\n",
    "                marker=\"o\",\n",
    "                linestyle=\"-\",\n",
    "                linewidth=1.0,\n",
    "                markersize=6,\n",
    "                label=label,\n",
    "                alpha=0.9,\n",
    "            )\n",
    "\n",
    "        # --- echte Pareto-Front über ALLE Sweeps dieses Datensatzes ---\n",
    "        # Ziele: min trainable_params, max mt_score_mean\n",
    "        df_sorted = df_log.sort_values(\"trainable_params\")\n",
    "        best_score = -np.inf\n",
    "        pareto_rows = []\n",
    "\n",
    "        for _, row in df_sorted.iterrows():\n",
    "            score = row[\"mt_score_mean\"]\n",
    "            if score >= best_score - 1e-9:\n",
    "                pareto_rows.append(row)\n",
    "                best_score = score\n",
    "\n",
    "        pareto_df = pd.DataFrame(pareto_rows)\n",
    "\n",
    "        if not pareto_df.empty:\n",
    "            pareto_df = pareto_df.sort_values(\"trainable_params\")\n",
    "            ax.plot(\n",
    "                pareto_df[\"trainable_params\"],\n",
    "                pareto_df[\"mt_score_mean\"],\n",
    "                color=\"black\",\n",
    "                linewidth=1.3,\n",
    "                marker=\"o\",\n",
    "                markersize=4,\n",
    "                label=\"Pareto front\",\n",
    "            )\n",
    "\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.set_xlabel(\"Model size (LoRA, log scale)\")\n",
    "        ax.set_ylabel(\"MT-Score (mean across seeds)\")\n",
    "\n",
    "        ds_label = DATASET_MAP.get(log_name, log_name)\n",
    "        ax.set_title(f\"{ds_label} – LLM LoRA sweeps (Pareto front)\")\n",
    "\n",
    "        ax.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "        # Legend unten zentriert wie beim anderen Plot\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        fig.legend(\n",
    "            handles,\n",
    "            labels,\n",
    "            title=\"Backbone / front\",\n",
    "            loc=\"lower center\",\n",
    "            ncol=len(labels),\n",
    "            bbox_to_anchor=(0.5, -0.02),\n",
    "        )\n",
    "\n",
    "        # Platz für Legende lassen\n",
    "        plt.tight_layout(rect=(0, 0.10, 1, 1))\n",
    "\n",
    "        log_dir = os.path.join(plots_base_dir, log_name)\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        out_path = os.path.join(log_dir, f\"pareto_llm_lora_sweeps_true_{log_name}.png\")\n",
    "\n",
    "        save_png_and_pdf(fig, out_path, dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "        print(f\"Saved TRUE Pareto-front LoRA-sweeps plot for log={log_name} to: {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "llm-peft-ppm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
